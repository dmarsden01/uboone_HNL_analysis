{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab81f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, string, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from platform import python_version\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import math\n",
    "import csv\n",
    "# from matplotlib.patches import Rectangle\n",
    "\n",
    "import Utilities.Plotter as PT\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Variables_list as Variables\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print('Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0afb01",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook is used for calculating POT normalization factors. <br>\n",
    "The normalization factors derived from this notebook are already saved in the Utilities/Constants.py file for Run1 and Run3 samples. So this does not need to be run if you are only using those samples. <br>\n",
    "\n",
    "Later in the notebook there are cells for making a 10 event limit on theta squared and comparing events per POT for different samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f013d62-7799-46a8-9bf7-afd8620ad5fb",
   "metadata": {},
   "source": [
    "## Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb49e7-e430-407b-8abf-032d585b8de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Single_file_POT_events(location): #Returns the cumulative POT of the sample at the \"location\" and number of events\n",
    "    \"\"\"\n",
    "    Given a .root file location with 'nuselection/NeutrinoSelectionFilter' and 'nuselection/SubRun' Trees, \n",
    "    returns the cumulative POT and the number of events in the file. \n",
    "    \"\"\"\n",
    "    file = uproot3.open(location)['nuselection/SubRun']\n",
    "    file_evs = uproot3.open(location)['nuselection/NeutrinoSelectionFilter']\n",
    "    events = file_evs.numentries\n",
    "    if b\"pot\" in file.keys(): #POT exists\n",
    "        POT = Functions.POT_counter(file)\n",
    "    else: POT = None #POT doesn't exist\n",
    "\n",
    "    return POT, events\n",
    "\n",
    "def Single_file_POT_scaling(POT, Run, file_type): #Do not use for EXT\n",
    "    \"\"\"\n",
    "    Given the POT, Run and file_type (\"signal\", \"overlay\", \"dirt\") will return the POT normalisation scaling factor required.\n",
    "    If the file is EXT (\"beamoff\"), the scale factor will just be taken from the Constants.py file. \n",
    "    \"\"\"\n",
    "    if POT == None:\n",
    "        print(\"No POT for this file. Assuming EXT file.\")\n",
    "        if Run == \"run1\":SF = Constants.SF_EXT_run1\n",
    "        elif Run == \"run3\":SF = Constants.SF_EXT_run3\n",
    "        else: \n",
    "            print(\"No scaling factor saved for beamoff that isn't Run1 or Run3. Needs to be calculated. Returning one.\")\n",
    "            return 1\n",
    "\n",
    "        return SF\n",
    "    if Run == \"run1\": #Not EXT file\n",
    "        Data_POT = Constants.Run1_POT\n",
    "        further_scaling = {\"signal\":Constants.NuMI_KDAR_scaling_run1,\n",
    "                           \"overlay\":1.0,\n",
    "                           \"dirt\":Constants.DIRT_run1_scaling}\n",
    "    if Run == \"run3\": #Not EXT file\n",
    "        Data_POT = Constants.Run3_POT\n",
    "        further_scaling = {\"signal\":Constants.NuMI_KDAR_scaling_run3,\n",
    "                           \"overlay\":1.0,\n",
    "                           \"dirt\":Constants.DIRT_run3_scaling}\n",
    "        \n",
    "    if Run == \"run2a\": #Not EXT file\n",
    "        Data_POT = Constants.Run2a_POT\n",
    "        further_scaling = {\"signal\":Constants.NuMI_KDAR_scaling_run1,\n",
    "                           \"overlay\":1.0,\n",
    "                           \"dirt\":Constants.DIRT_run1_scaling}\n",
    "    if Run == \"run2b\": #Not EXT file\n",
    "        Data_POT = Constants.Run2b_POT\n",
    "        further_scaling = {\"signal\":Constants.NuMI_KDAR_scaling_run1,\n",
    "                           \"overlay\":1.0,\n",
    "                           \"dirt\":Constants.DIRT_run1_scaling}\n",
    "    \n",
    "    SF = (Data_POT/POT)*further_scaling[file_type]\n",
    "\n",
    "    return SF\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c2618c-c267-4eb6-b256-a31d16a6ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = '../NuMI_signal/KDAR_dump/sfnues/sfnues_KDAR_dump_10_Umu4_majorana_numi_RHC.root'\n",
    "locs = {'10_ee_dirac':'../NuMI_signal/KDAR_dump/sfnues/sfnues_KDAR_dump_10_ee_Umu4_dirac_FHC.root',\n",
    "        '100_ee_dirac':'../NuMI_signal/KDAR_dump/sfnues/sfnues_KDAR_dump_100_ee_Umu4_dirac_FHC.root',\n",
    "        '150_ee_dirac':'../NuMI_signal/KDAR_dump/sfnues/sfnues_KDAR_dump_150_ee_Umu4_dirac_FHC.root',\n",
    "        '150_pi0_dirac':'../NuMI_signal/KDAR_dump/sfnues/pi0/sfnues_KDAR_dump_150_pi0_Umu4_dirac_FHC.root',\n",
    "        '200_pi0_dirac':'../NuMI_signal/KDAR_dump/sfnues/pi0/sfnues_KDAR_dump_200_pi0_Umu4_dirac_FHC.root',\n",
    "        '245_pi0_dirac':'../NuMI_signal/KDAR_dump/sfnues/pi0/sfnues_KDAR_dump_245_pi0_Umu4_dirac_FHC.root'}\n",
    "\n",
    "\n",
    "file_type = \"signal\" #\"signal\", \"overlay\" or \"dirt\"\n",
    "Run=\"run1\"\n",
    "\n",
    "SF_dict = {}\n",
    "\n",
    "print(f\"Printing POT, events and SF for {Run} {file_type} files.\")\n",
    "\n",
    "for mass in locs:\n",
    "    POT, events = Single_file_POT_events(locs[mass])\n",
    "    print(f\"{mass} file: {POT} POT, {events} events\")\n",
    "    SF=Single_file_POT_scaling(POT, Run, file_type)\n",
    "    SF_dict[mass] = SF\n",
    "    print(f\"POT norm factor: {SF}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5caf9-75ad-4752-86ed-f2c466e9ef24",
   "metadata": {},
   "source": [
    "## Run2 calculations (not finished, need to check event list is correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18548577-7098-4739-875c-0225b7bd7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OnBeam_EA9CNT_run2a = 8370956\n",
    "OnBeam_EA9CNT_run2b = 3167451\n",
    "\n",
    "OffBeam_EXT_NUMIwin_FEMBeamTriggerAlgo_run2a = 18621110.2\n",
    "OffBeam_EXT_NUMIwin_FEMBeamTriggerAlgo_run2b = 12223866.55\n",
    "\n",
    "print(\"------beamoff_scaling-------\")\n",
    "print(\"run2a: \" + str(OnBeam_EA9CNT_run2a/OffBeam_EXT_NUMIwin_FEMBeamTriggerAlgo_run2a))\n",
    "print(\"run2b: \" + str(OnBeam_EA9CNT_run2b/OffBeam_EXT_NUMIwin_FEMBeamTriggerAlgo_run2b))\n",
    "print(\"----------------------------\")\n",
    "\n",
    "locs = {\"run2a_overlay\": '../NuMI_MC/SLIMMED_neutrinoselection_filt_run2a_overlay.root',\n",
    "        \"run2b_overlay\": '../NuMI_MC/SLIMMED_neutrinoselection_filt_run2b_overlay.root',\n",
    "        \"run2a_beamon\": '../NuMI_data/neutrinoselection_filt_run2a_beamon.root',\n",
    "        \"run2b_beamon\": '../NuMI_data/neutrinoselection_filt_run2b_beamon.root',\n",
    "        \"run2a_beamoff\": '../NuMI_data/neutrinoselection_filt_run2a_beamoff.root',\n",
    "        \"run2b_beamoff\": '../NuMI_data/neutrinoselection_filt_run2b_beamoff.root'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9600511-bd19-4872-af7b-17bf1eedae3d",
   "metadata": {},
   "source": [
    "## Quick event counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4603f14c-82a4-4f50-b626-2ee12742c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_dict = {\"overlay\":'../NuMI_MC/SLIMMED_neutrinoselection_filt_run1_overlay.root',\n",
    "            \"dirt\":'../NuMI_MC/neutrinoselection_filt_run1_dirt_overlay.root',\n",
    "            \"ext\":'../NuMI_data/neutrinoselection_filt_run1_beamoff.root'}\n",
    "\n",
    "events_dict = {}\n",
    "\n",
    "for file in loc_dict:\n",
    "\n",
    "    file_type = file #\"signal\", \"overlay\" or \"dirt\"\n",
    "    Run=\"run1\"\n",
    "    POT, events = Single_file_POT_events(loc_dict[file])\n",
    "    events_dict[file] = events\n",
    "    SF = Single_file_POT_scaling(POT, Run, file_type)\n",
    "    scaled_events = events*SF\n",
    "    \n",
    "    print(f\"{Run} {file} has {round(scaled_events)} scaled events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75058c7b-1076-4b2f-94f8-6bbf158ee957",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_dict = {\"overlay\":'../NuMI_MC/SLIMMED_neutrinoselection_filt_run3_overlay.root',\n",
    "            \"dirt\":'../NuMI_MC/neutrinoselection_filt_run3_dirt_overlay.root',\n",
    "            \"ext\":'../NuMI_data/neutrinoselection_filt_run3_beamoff.root'}\n",
    "\n",
    "events_dict = {}\n",
    "\n",
    "for file in loc_dict:\n",
    "\n",
    "    file_type = file #\"signal\", \"overlay\" or \"dirt\"\n",
    "    Run=\"run3\"\n",
    "    POT, events = Single_file_POT_events(loc_dict[file])\n",
    "    events_dict[file] = events\n",
    "    SF = Single_file_POT_scaling(POT, Run, file_type)\n",
    "    scaled_events = events*SF\n",
    "    \n",
    "    print(f\"{Run} {file} has {round(scaled_events)} scaled events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d7a74-b3df-4f21-83a5-3046449c4081",
   "metadata": {},
   "source": [
    "## Making ratio of events for two sets of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f406fa4-4f9b-48e2-9520-6bb339b8c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to compare the events per POT for different samples (e.g. the generator has been updated) you can use this.\n",
    "SF_old_to_new = {}\n",
    "SF_list = []\n",
    "HNL_masses = []\n",
    "for HNL_mass in SF_dict:\n",
    "    Event_ratio = events_dict[HNL_mass]/events_dict_2[HNL_mass]\n",
    "\n",
    "    POT_ratio = POT_dict[HNL_mass]/POT_dict_2[HNL_mass]\n",
    "\n",
    "    ev_per_POT_1 = events_dict[HNL_mass]/POT_dict[HNL_mass]\n",
    "    ev_per_POT_2 = events_dict_2[HNL_mass]/POT_dict_2[HNL_mass]\n",
    "\n",
    "    ratio_evs_POT = ev_per_POT_1/ev_per_POT_2\n",
    "    SF_old_to_new[HNL_mass] = ratio_evs_POT\n",
    "    SF_list.append((1-ratio_evs_POT)*100)\n",
    "    HNL_masses.append(HNL_mass)\n",
    "\n",
    "    print(\"Event ratio is \" + str(Event_ratio))\n",
    "    print(\"POT ratio is \" + str(POT_ratio))\n",
    "    print(\"Scale factor should be \" + str(ratio_evs_POT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f063208-7ff9-4a2b-886e-2872bf87a91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(HNL_masses, SF_list, label=\"New generator version run1\", lw=3)\n",
    "\n",
    "for count, HNL_mass in enumerate(HNL_masses):\n",
    "    plt.plot(HNL_mass, SF_list[count],marker=\"o\",markersize=10, color=\"black\")\n",
    "\n",
    "plt.xlabel('HNL mass [MeV]',fontsize=25)\n",
    "plt.ylabel(r'Extra events (%)',fontsize=25)\n",
    "plt.legend()\n",
    "plt.ylim(0,25)\n",
    "#plt.xlim(0,0.45)$\n",
    "plt.xlim(10,210)\n",
    "plt.grid(ls='--',color='C7',alpha=0.1)\n",
    "# plt.yscale('log')\n",
    "plt.tick_params(axis='x', labelsize=20)\n",
    "plt.tick_params(axis='y', labelsize=20)\n",
    "# plt.legend(prop={'size': 16}, loc=\"lower left\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('plots/Generator/New_generator_version_events_fraction.pdf',bbox_inches='tight', pad_inches=0.3)\n",
    "# plt.savefig('plots/Generator/New_generator_version_events_fraction.png',bbox_inches='tight', pad_inches=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0b1fb7-1575-4d5d-832b-0e8169c947cf",
   "metadata": {},
   "source": [
    "## 10 event limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60583d9-7a1e-45b6-8c2a-de0056cbada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting with only run3, adapt as required\n",
    "thetas = Constants.theta_mu_4_dict\n",
    "run1_POT_scaling_dict = Constants.run1_POT_scaling_dict\n",
    "run3_POT_scaling_dict = Constants.run3_POT_scaling_dict\n",
    "HNL_masses = []\n",
    "thetas_for_10ev = []\n",
    "\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    NumEvs = events_dict_2[HNL_mass]*run3_POT_scaling_dict[HNL_mass] #Need to fill events_dict_2 somewhere\n",
    "    theta_squared = thetas[HNL_mass]**2\n",
    "    theta_for_10ev = np.sqrt(10/NumEvs)*theta_squared #For HNLs number of events scales with theta^4\n",
    "    HNL_masses.append(HNL_mass)\n",
    "    thetas_for_10ev.append(theta_for_10ev)\n",
    "    \n",
    "r = zip(HNL_masses, thetas_for_10ev)\n",
    "with open(f'limit_files/10_event_expected_mu_run3.csv', \"w\") as s:\n",
    "    w = csv.writer(s)\n",
    "    for row in r:\n",
    "        w.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a142a-755c-4d8f-bbd3-2625b065d556",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(HNL_masses, thetas_for_10ev, label=\"10 event limit (Run3)\", lw=3)\n",
    "plt.xlabel('HNL mass [MeV]',fontsize=20)\n",
    "plt.ylabel(r'$|U_{\\mu 4}|^2$ limit at 90% C.L.',fontsize=20)\n",
    "plt.legend()\n",
    "plt.ylim(1e-9,1e-3)\n",
    "#plt.xlim(0,0.45)$\n",
    "plt.xlim(0,250)\n",
    "plt.grid(ls='--',color='C7',alpha=0.1)\n",
    "plt.yscale('log')\n",
    "plt.tick_params(axis='x', labelsize=20)\n",
    "plt.tick_params(axis='y', labelsize=20)\n",
    "# plt.legend(prop={'size': 16}, loc=\"lower left\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a3504-392d-4b23-a225-0db439fcb0f5",
   "metadata": {},
   "source": [
    "## Reading in all files, giving all SF (manual, deprecated code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d368690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This reads in the MC overlay, MC dirt overlay, EXT, data and signal samples for NuMI run1 or run3\n",
    "#HNL_masses = Constants.HNL_mass_samples #in MeV\n",
    "\n",
    "Run = \"run1\" #so far either \"run1\" or \"run3\"\n",
    "\n",
    "Load_pi0_samples = True\n",
    "\n",
    "FLATTEN = False\n",
    "\n",
    "root_dir = 'nuselection'\n",
    "POT_tree = 'SubRun' #Branch for POT\n",
    "MC_samples_dir = '../NuMI_MC/'\n",
    "data_samples_dir = '../NuMI_data/'\n",
    "signal_samples_dir = '../NuMI_signal/KDAR_dump/sfnues/'\n",
    "\n",
    "loc_overlay_run1 = MC_samples_dir+'SLIMMED_neutrinoselection_filt_run1_overlay.root'#NuMI Run1 MC WITHOUT systematics weights\n",
    "loc_dirt_run1 = MC_samples_dir+'neutrinoselection_filt_run1_dirt_overlay.root'\n",
    "loc_EXT_run1 = data_samples_dir+'neutrinoselection_filt_run1_beamoff.root'\n",
    "loc_beamgood_run1 = data_samples_dir+'neutrinoselection_filt_run1_beamon_beamgood.root'\n",
    "\n",
    "loc_overlay_run3 = MC_samples_dir+'SLIMMED_neutrinoselection_filt_run3_overlay.root' #NuMI Run3 MC WITHOUT systematics weights\n",
    "loc_dirt_run3 = MC_samples_dir+'neutrinoselection_filt_run3_dirt_overlay.root'\n",
    "loc_EXT_run3 = data_samples_dir+'neutrinoselection_filt_run3_beamoff.root'\n",
    "loc_beamgood_run3 = data_samples_dir+'neutrinoselection_filt_run3_beamon_beamgood.root'\n",
    "\n",
    "print(\"Opening Run1 samples with uproot\")\n",
    "NuMI_MC_overlay_run1 = uproot3.open(loc_overlay_run1)[root_dir+'/'+POT_tree]\n",
    "NuMI_MC_dirt_run1 = uproot3.open(loc_dirt_run1)[root_dir+'/'+POT_tree]\n",
    "NuMI_EXT_run1 = uproot3.open(loc_EXT_run1)[root_dir+'/'+POT_tree]\n",
    "NuMI_beamgood_run1 = uproot3.open(loc_beamgood_run1)[root_dir+'/'+POT_tree]\n",
    "\n",
    "signal_samples_dict_run1 = {}\n",
    "if Load_pi0_samples == True:\n",
    "    for HNL_mass in Constants.HNL_mass_pi0_samples:\n",
    "        Signal_run1  = uproot3.open(signal_samples_dir+f'pi0/sfnues_KDAR_dump_{HNL_mass}_pi0_Umu4_majorana_RHC.root')[root_dir+'/'+POT_tree]\n",
    "        signal_samples_dict_run1[HNL_mass] = Signal_run1\n",
    "else:\n",
    "    for HNL_mass in Constants.HNL_mass_samples:\n",
    "        Signal_run1  = uproot3.open(signal_samples_dir+f'sfnues_KDAR_dump_{HNL_mass}_ee_Umu4_majorana_RHC.root')[root_dir+'/'+POT_tree]\n",
    "        signal_samples_dict_run1[HNL_mass] = Signal_run1\n",
    "    \n",
    "print(\"Opening Run3 samples with uproot\")\n",
    "NuMI_MC_overlay_run3 = uproot3.open(loc_overlay_run3)[root_dir+'/'+POT_tree]\n",
    "NuMI_MC_dirt_run3 = uproot3.open(loc_dirt_run3)[root_dir+'/'+POT_tree]\n",
    "NuMI_EXT_run3 = uproot3.open(loc_EXT_run3)[root_dir+'/'+POT_tree]\n",
    "NuMI_beamgood_run3 = uproot3.open(loc_beamgood_run3)[root_dir+'/'+POT_tree]\n",
    "\n",
    "signal_samples_dict_run3 = {}\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    Signal_run3  = uproot3.open(signal_samples_dir+f'sfnues_KDAR_dump_{HNL_mass}_ee_Umu4_majorana_RHC.root')[root_dir+'/'+POT_tree]\n",
    "    signal_samples_dict_run3[HNL_mass] = Signal_run3\n",
    "\n",
    "print(\"Opened files\" + \"\\n\")\n",
    "print(\"----RUN1----\"+ \"\\n\")\n",
    "print(\"----MC OVERLAY BACKGROUND----\")\n",
    "print(\"Number of branches is \" + str(len(NuMI_MC_overlay_run1.keys()))) \n",
    "print(\"Number of subruns is \" + str(NuMI_MC_overlay_run1.numentries))\n",
    "print(\"----MC DIRT BACKGROUND----\")\n",
    "print(\"Number of branches is \" + str(len(NuMI_MC_dirt_run1.keys()))) \n",
    "print(\"Number of subruns is \" + str(NuMI_MC_dirt_run1.numentries))\n",
    "print(\"----EXT BACKGROUND----\")\n",
    "print(\"Number of branches is \" + str(len(NuMI_EXT_run1.keys()))) \n",
    "print(\"Number of subruns is \" + str(NuMI_EXT_run1.numentries))\n",
    "print(\"----DATA----\")\n",
    "print(\"Number of branches is \" + str(len(NuMI_beamgood_run1.keys()))) \n",
    "print(\"Number of subruns is \" + str(NuMI_beamgood_run1.numentries))\n",
    "print(\"----SIGNAL----\")\n",
    "if Load_pi0_samples == True:\n",
    "    for HNL_mass in Constants.HNL_mass_pi0_samples:\n",
    "        print(f\"Number of branches in pi0 {HNL_mass}MeV is \" + str(len(signal_samples_dict_run1[HNL_mass].keys()))) \n",
    "        print(f\"Number of subruns in pi0 {HNL_mass}MeV is \" + str(signal_samples_dict_run1[HNL_mass].numentries))\n",
    "else:\n",
    "    for HNL_mass in Constants.HNL_mass_samples:\n",
    "        print(f\"Number of branches in {HNL_mass}MeV is \" + str(len(signal_samples_dict_run1[HNL_mass].keys()))) \n",
    "        print(f\"Number of subruns in {HNL_mass}MeV is \" + str(signal_samples_dict_run1[HNL_mass].numentries))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"----RUN3----\"+ \"\\n\")\n",
    "print(\"----MC OVERLAY BACKGROUND----\")\n",
    "print(\"Number of branches is \" + str(len(NuMI_MC_overlay_run3.keys()))) \n",
    "print(\"Number of subruns is \" + str(NuMI_MC_overlay_run3.numentries))\n",
    "print(\"----MC DIRT BACKGROUND----\")\n",
    "print(\"Number of branches is \" + str(len(NuMI_MC_dirt_run3.keys()))) \n",
    "print(\"Number of subruns is \" + str(NuMI_MC_dirt_run3.numentries))\n",
    "print(\"----EXT BACKGROUND----\")\n",
    "print(\"Number of branches is \" + str(len(NuMI_EXT_run3.keys()))) \n",
    "print(\"Number of subruns is \" + str(NuMI_EXT_run3.numentries))\n",
    "print(\"----DATA----\")\n",
    "print(\"Number of branches is \" + str(len(NuMI_beamgood_run3.keys()))) \n",
    "print(\"Number of subruns is \" + str(NuMI_beamgood_run3.numentries))\n",
    "print(\"----SIGNAL----\")\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    print(f\"Number of branches in {HNL_mass}MeV is \" + str(len(signal_samples_dict_run3[HNL_mass].keys()))) \n",
    "    print(f\"Number of subruns in {HNL_mass}MeV is \" + str(signal_samples_dict_run3[HNL_mass].numentries))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dc945e-2615-4e65-b3e0-645545a137fb",
   "metadata": {},
   "source": [
    "# Checking POT normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781434a-6801-4b7b-b18c-8edab389c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def POT_counter_old(df): #This takes dataframe, which I probably don't need to load anymore\n",
    "    Total_POT = 0\n",
    "    for i in range(len(df['pot'])):\n",
    "        Total_POT += df['pot'][i]\n",
    "    return Total_POT\n",
    "\n",
    "def POT_counter(file): #Takes uproot file\n",
    "    Total_POT = file[\"pot\"].array().sum()\n",
    "    return Total_POT\n",
    "\n",
    "signal_POT_dict_run1 = {}\n",
    "\n",
    "#-----Run1-----#\n",
    "if Load_pi0_samples == True:\n",
    "    for HNL_mass in Constants.HNL_mass_pi0_samples:\n",
    "        signal_POT_run1 = POT_counter(signal_samples_dict_run1[HNL_mass])\n",
    "        signal_POT_dict_run1[HNL_mass] = signal_POT_run1\n",
    "else:\n",
    "    for HNL_mass in Constants.HNL_mass_samples:\n",
    "        signal_POT_run1 = POT_counter(signal_samples_dict_run1[HNL_mass])\n",
    "        signal_POT_dict_run1[HNL_mass] = signal_POT_run1\n",
    "\n",
    "overlay_POT_run1 = POT_counter(NuMI_MC_overlay_run1)\n",
    "dirt_POT_run1 = POT_counter(NuMI_MC_dirt_run1)\n",
    "#beamgood_POT = POT_counter(df_beamgood_run1) #There is no 'pot' branch for beamgood\n",
    "OffBeam_EXT_NUMIwin_FEMBeamTriggerAlgo_run1 = 9199232.74 #Taken from the NuMI samples page\n",
    "\n",
    "#-----Run3-----#\n",
    "signal_POT_dict_run3 = {}\n",
    "\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    signal_POT_run3 = POT_counter(signal_samples_dict_run3[HNL_mass])\n",
    "    signal_POT_dict_run3[HNL_mass] = signal_POT_run3\n",
    "    \n",
    "\n",
    "overlay_POT_run3 = POT_counter(NuMI_MC_overlay_run3)\n",
    "dirt_POT_run3 = POT_counter(NuMI_MC_dirt_run3)\n",
    "#beamgood_POT = POT_counter(df_beamgood_run1) #There is no 'pot' branch for beamgood\n",
    "OffBeam_EXT_NUMIwin_FEMBeamTriggerAlgo_run3 = 32878305.25\n",
    "\n",
    "#Signal\n",
    "print(\"----RUN1----\"+ \"\\n\")\n",
    "print('Total signal POTs are: ')\n",
    "if Load_pi0_samples == True:\n",
    "    for HNL_mass in Constants.HNL_mass_pi0_samples:\n",
    "        print(f\"{HNL_mass}MeV : \"  + str(signal_POT_dict_run1[HNL_mass]))\n",
    "else:\n",
    "    for HNL_mass in Constants.HNL_mass_samples:\n",
    "        print(f\"{HNL_mass}MeV : \"  + str(signal_POT_dict_run1[HNL_mass]))\n",
    "print('-------------------')\n",
    "#Dirt\n",
    "print('Total dirt POT is ' + str(dirt_POT_run1))\n",
    "print('-------------------')\n",
    "#Overlay\n",
    "print('Total overlay POT is ' + str(overlay_POT_run1))\n",
    "print('-------------------'+ \"\\n\")\n",
    "print(\"----RUN3----\"+ \"\\n\")\n",
    "print('Total signal POTs are: ')\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    print(f\"{HNL_mass}MeV : \"  + str(signal_POT_dict_run3[HNL_mass]))\n",
    "print('-------------------')\n",
    "#Dirt\n",
    "print('Total dirt POT is ' + str(dirt_POT_run3))\n",
    "print('-------------------')\n",
    "#Overlay\n",
    "print('Total overlay POT is ' + str(overlay_POT_run3))\n",
    "print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc443821-23ab-4ffa-9c1a-b5233896ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run1_POT = 2e20 #NEEDS TO BE CHECKED, just taken from the NuMI samples page\n",
    "Run3_POT = 5.0e20 #NEEDS TO BE CHECKED, just taken from the NuMI samples page\n",
    "OnBeam_EA9CNT_wcut_run1 = 5268051.0 #\"Triggers\" taken from the NuMI samples page\n",
    "OnBeam_EA9CNT_wcut_run3 = 10363728.0 #\"Triggers\" taken from the NuMI samples page\n",
    "BeamOff_scaling_for_nus = 0.98 #This Factor described by Owen: \n",
    "# An additional scaling\n",
    "# factor of 0.98 is applied to the Beam-off sample to take\n",
    "# into account that 2% of all NuMI Beam-on events are\n",
    "# expected to contain a neutrino interaction\n",
    "DIRT_run1_scaling = 0.75 #NOT SURE where this comes from, apparently it is standard procedure for NuMI DIRT\n",
    "DIRT_run3_scaling = 0.35 #NOT SURE where this comes from, apparently it is standard procedure for NuMI DIRT\n",
    "NuMI_KDAR_scaling_run1 = 8.0 #This comes from the discrepancy between numu flux from KDAR dump between Geant4 and MiniBooNE measurement. Taken from Owen's thesis\n",
    "NuMI_KDAR_scaling_run3 = 8.6\n",
    "\n",
    "run1_POT_scaling_dict = {}\n",
    "run3_POT_scaling_dict = {}\n",
    "\n",
    "#Calculation of POT scaling factors\n",
    "if Load_pi0_samples == True:\n",
    "    for HNL_mass in Constants.HNL_mass_pi0_samples:\n",
    "        SF_signal_run1 = (Run1_POT/signal_POT_dict_run1[HNL_mass])*NuMI_KDAR_scaling_run1\n",
    "        run1_POT_scaling_dict[HNL_mass] = SF_signal_run1\n",
    "else:\n",
    "    for HNL_mass in Constants.HNL_mass_samples:\n",
    "        SF_signal_run1 = (Run1_POT/signal_POT_dict_run1[HNL_mass])*NuMI_KDAR_scaling_run1\n",
    "        run1_POT_scaling_dict[HNL_mass] = SF_signal_run1\n",
    "        \n",
    "SF_overlay_run1 = Run1_POT/overlay_POT_run1\n",
    "SF_dirt_run1 = (Run1_POT/dirt_POT_run1)*DIRT_run1_scaling\n",
    "SF_EXT_run1 = (OnBeam_EA9CNT_wcut_run1/OffBeam_EXT_NUMIwin_FEMBeamTriggerAlgo_run1)*BeamOff_scaling_for_nus\n",
    "    \n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    SF_signal_run3 = (Run3_POT/signal_POT_dict_run3[HNL_mass])*NuMI_KDAR_scaling_run3\n",
    "    run3_POT_scaling_dict[HNL_mass] = SF_signal_run3\n",
    "    \n",
    "SF_overlay_run3 = Run3_POT/overlay_POT_run3\n",
    "SF_dirt_run3 = (Run3_POT/dirt_POT_run3)*DIRT_run3_scaling\n",
    "SF_EXT_run3 = (OnBeam_EA9CNT_wcut_run3/OffBeam_EXT_NUMIwin_FEMBeamTriggerAlgo_run3)*BeamOff_scaling_for_nus\n",
    "\n",
    "\n",
    "print(\"The following factors can be applied to the full samples, i.e they are not event-dependent\")\n",
    "print()\n",
    "print('For Run1 the scale factors are: ')\n",
    "print('Overlay: ' + str(SF_overlay_run1))\n",
    "print('Dirt: ' + str(SF_dirt_run1))\n",
    "print('EXT: ' + str(SF_EXT_run1))\n",
    "print('Signal: ')\n",
    "if Load_pi0_samples == True:\n",
    "    for HNL_mass in Constants.HNL_mass_pi0_samples:\n",
    "        print(f\"{HNL_mass}MeV \" + str(run1_POT_scaling_dict[HNL_mass]))\n",
    "else:\n",
    "    for HNL_mass in Constants.HNL_mass_samples:\n",
    "        print(f\"{HNL_mass}MeV \" + str(run1_POT_scaling_dict[HNL_mass]))\n",
    "\n",
    "print()\n",
    "print('For Run3 the scale factors are: ')\n",
    "print('Overlay: ' + str(SF_overlay_run3))\n",
    "print('Dirt: ' + str(SF_dirt_run3))\n",
    "print('EXT: ' + str(SF_EXT_run3))\n",
    "print('Signal: ')\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    print(f\"{HNL_mass}MeV \" + str(run3_POT_scaling_dict[HNL_mass]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ace01-8e59-4c36-9755-f796ec0d4d96",
   "metadata": {},
   "source": [
    "## Background Overlay Detector Variation POTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa3296-5ac9-4a85-a3a4-489cf781bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run1_DetVar_POT_dict = {}\n",
    "run3_DetVar_POT_dict = {}\n",
    "\n",
    "for DetVar in Constants.Detector_variations:\n",
    "    NuMI_MC_overlay_run1 = uproot3.open(f\"../NuMI_MC/DetVars/neutrinoselection_filt_run1_overlay_{DetVar}.root\")[root_dir+'/'+POT_tree]\n",
    "    NuMI_MC_overlay_run3 = uproot3.open(f\"../NuMI_MC/DetVars/neutrinoselection_filt_run3_overlay_{DetVar}.root\")[root_dir+'/'+POT_tree]\n",
    "    \n",
    "    run1_POT = POT_counter(NuMI_MC_overlay_run1)\n",
    "    run3_POT = POT_counter(NuMI_MC_overlay_run3)\n",
    "    \n",
    "    run1_DetVar_POT_dict[DetVar] = run1_POT\n",
    "    run3_DetVar_POT_dict[DetVar] = run3_POT\n",
    "\n",
    "print(\"-----Run1-----\")\n",
    "for DetVar in Constants.Detector_variations:\n",
    "    print(f\"{DetVar} POT: \" + str(run1_DetVar_POT_dict[DetVar]))\n",
    "    \n",
    "print(\"-----Run3-----\")\n",
    "for DetVar in Constants.Detector_variations:\n",
    "    print(f\"{DetVar} POT: \" + str(run3_DetVar_POT_dict[DetVar]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e0d24d-ad0e-427f-b5ff-4e98799b09c1",
   "metadata": {},
   "source": [
    "## Events numbers Detector variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6761acf-9504-4aad-8ff6-07f97675fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_tree = 'NeutrinoSelectionFilter'\n",
    "\n",
    "run1_DetVar_events_dict = {}\n",
    "run3_DetVar_events_dict = {}\n",
    "\n",
    "for DetVar in Constants.Detector_variations:\n",
    "    NuMI_MC_overlay_run1 = uproot3.open(f\"../NuMI_MC/DetVars/neutrinoselection_filt_run1_overlay_{DetVar}.root\")[root_dir+'/'+main_tree]\n",
    "    NuMI_MC_overlay_run3 = uproot3.open(f\"../NuMI_MC/DetVars/neutrinoselection_filt_run3_overlay_{DetVar}.root\")[root_dir+'/'+main_tree]\n",
    "    \n",
    "    run1_events = len(NuMI_MC_overlay_run1)\n",
    "    run3_events = len(NuMI_MC_overlay_run3)\n",
    "    \n",
    "    run1_DetVar_events_dict[DetVar] = run1_events\n",
    "    run3_DetVar_events_dict[DetVar] = run3_events\n",
    "\n",
    "print(\"-----Run1-----\")\n",
    "for DetVar in Constants.Detector_variations:\n",
    "    print(f\"{DetVar} events: \" + str(run1_DetVar_events_dict[DetVar]))\n",
    "    \n",
    "print(\"-----Run3-----\")\n",
    "for DetVar in Constants.Detector_variations:\n",
    "    print(f\"{DetVar} events: \" + str(run3_DetVar_events_dict[DetVar]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c8719e-994b-42d9-a91d-6aff9042e36a",
   "metadata": {},
   "source": [
    "# Finished code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
