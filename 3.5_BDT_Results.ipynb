{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab81f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "import os, sys, string, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from platform import python_version\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import math\n",
    "from matplotlib.patches import Rectangle\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from importlib import reload\n",
    "import csv\n",
    "\n",
    "import Utilities.Plotter as PT\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Variables_list as Variables\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print ('Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a3504-392d-4b23-a225-0db439fcb0f5",
   "metadata": {},
   "source": [
    "## Reading in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de87920a-3a93-4217-93ab-1963f9066403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading these run1 samples: \n",
      "['overlay', 'dirtoverlay', 'beamoff', '2_ee', '10_ee', '20_ee', '50_ee', '100_ee', '150_ee', 'beamgood']\n"
     ]
    }
   ],
   "source": [
    "Params = {\"Run\":\"run1\", #The run number, so far either \"run1\" or \"run3\"\n",
    "          \"Load_single_file\":False, #This will override everything else, put the desired file in the \"single_file\" line\n",
    "          \"single_file\":245,\n",
    "          \"Load_standard\":True, #bkgs\n",
    "          \"Load_lepton_signal\":True,\n",
    "          \"Load_lepton_dirac\":False,\n",
    "          \"Load_pi0_signal\":False,\n",
    "          \"Load_pi0_dirac\":False,\n",
    "          \"Load_DetVars\":False, #This is for overlay\n",
    "          \"Only_keep_common_DetVar_evs\":True,\n",
    "          \"Load_Signal_DetVars\":False, #Don't do here, but in seperate script\n",
    "          'Load_pi0_signal_DetVars':False, #Don't do here, but in seperate script\n",
    "          \"Load_data\":True,\n",
    "          \"FLATTEN\":True, #Have one row per reconstructed object in the analysis dataframe\n",
    "          \"only_presel\":False, #Create small files containing only variables necessary for pre-selection, for making pre-selection plots\n",
    "          \"EXT_in_training\":False,\n",
    "          \"Use_logit\":True,\n",
    "          \"nbins\":5} \n",
    "\n",
    "feature_names = Variables.First_pass_vars_for_BDT #All variables\n",
    "feature_names_MC = feature_names + [\"weight\"]\n",
    "loc_pkls = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/my_vars/\"\n",
    "\n",
    "samples = Functions.create_test_samples_list(Params)\n",
    "\n",
    "if Params[\"Load_pi0_signal\"] == True:\n",
    "    pi0_sample_strings = [] #Unfortunately need to make, to discriminate lepton final states from pi0 final states for signal\n",
    "    for pi0_point in Constants.HNL_mass_pi0_samples:\n",
    "        pi0_sample_strings += [str(pi0_point)+\"_pi0\"]\n",
    "        \n",
    "# end_string = \"_FINAL\"\n",
    "end_string = \"_Finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2627116c-1b50-4017-a0dd-4c3b380f7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_preselected_pkls(samples, Params, loc_pkls, filename):\n",
    "    \"\"\"\n",
    "    For loading in preselected pkl files, including BDT test pkls. \n",
    "    \"\"\"\n",
    "    sample_test_dict = {}\n",
    "    sig_names_list = Constants.HNL_ee_samples_names+Constants.HNL_mass_pi0_samples_names+Constants.HNL_ee_dirac_names+Constants.HNL_pi0_dirac_names\n",
    "    if Params[\"Load_DetVars\"] == True: loc_pkls += \"DetVars/\"\n",
    "    elif Params[\"Load_Signal_DetVars\"] == True: loc_pkls += \"Signal_DetVars/\"\n",
    "    elif Params['Load_pi0_signal_DetVars'] == True: loc_pkls += \"Signal_DetVars/pi0/\"\n",
    "    for sample in samples:\n",
    "        if (sample == \"overlay\") or (sample in sig_names_list): start_str = loc_pkls + \"BDT_Test_dfs/Test_\"\n",
    "        # elif (sample == \"beamoff\") and (Params[\"EXT_in_training\"] == True: start_str = loc_pkls + \"BDT_Test_dfs/Test_overlay_\"\n",
    "        else: start_str = loc_pkls + \"Preselected_\"\n",
    "        # sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"Preselected_{sample}_\"+Params[\"Run\"]+f\"_flattened{filename}.pkl\")\n",
    "        sample_test_dict[sample] = pd.read_pickle(start_str+f\"{sample}_\"+Params[\"Run\"]+f\"_flattened{filename}.pkl\")\n",
    "    \n",
    "    return sample_test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f117d8-87e9-4d73-9649-d2b278fcb712",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pkl_files/run1/current_files/my_vars/BDT_Test_dfs/Test_overlay_run1_flattened_Finished.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample_test_dict \u001b[38;5;241m=\u001b[39m \u001b[43mLoad_preselected_pkls\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mParams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc_pkls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m, in \u001b[0;36mLoad_preselected_pkls\u001b[0;34m(samples, Params, loc_pkls, filename)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: start_str \u001b[38;5;241m=\u001b[39m loc_pkls \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreselected_\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"Preselected_{sample}_\"+Params[\"Run\"]+f\"_flattened{filename}.pkl\")\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     sample_test_dict[sample] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_str\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msample\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_flattened\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfilename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample_test_dict\n",
      "File \u001b[0;32m~/Virtual_envs/HNL_ana/lib/python3.11/site-packages/pandas/io/pickle.py:179\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Virtual_envs/HNL_ana/lib/python3.11/site-packages/pandas/io/common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    869\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pkl_files/run1/current_files/my_vars/BDT_Test_dfs/Test_overlay_run1_flattened_Finished.pkl'"
     ]
    }
   ],
   "source": [
    "sample_test_dict = Load_preselected_pkls(samples, Params, loc_pkls, end_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb5e4d-8063-408a-b273-0db95562cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_dict = {}\n",
    "for sample in samples:\n",
    "    if sample in Constants.Detector_variations: #Reading in an overlay DetVar sample\n",
    "        loc=loc_pkls+\"DetVars/\"+\"Preselected_overlay_\"+Params[\"Run\"]+\"_my_vars\"+f\"_{sample}_flattened_reduced_evs.pkl\"\n",
    "        sample_test_dict[sample] = pd.read_pickle(loc)\n",
    "    elif Params[\"Load_Signal_DetVars\"] == True:\n",
    "        loc=loc_pkls+\"Signal_DetVars/\"+\"Preselected_\"+Params[\"Run\"]+f\"_{sample}_reduced_evs_final.pkl\"\n",
    "        sample_test_dict[sample] = pd.read_pickle(loc)\n",
    "    elif Params[\"Load_pi0_signal\"] == True:\n",
    "        if sample == 'overlay':\n",
    "            # sample_test_dict[sample] = pd.read_pickle(loc_pkls+\"BDT_Test_dfs/pi0_selection/Test_overlay_\"+Params[\"Run\"]+\".pkl\")\n",
    "            sample_test_dict[sample] = pd.read_pickle(loc_pkls+\"BDT_Test_dfs/Test_overlay_\"+Params[\"Run\"]+\"_FINAL.pkl\")\n",
    "        elif sample in pi0_sample_strings:\n",
    "            # sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"BDT_Test_dfs/pi0_selection/Test_signal_{sample}_\"+Params[\"Run\"]+\"_FIXED.pkl\")\n",
    "            sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"BDT_Test_dfs/pi0_selection/Test_signal_{sample}_\"+Params[\"Run\"]+\".pkl\")\n",
    "        elif (sample == 'beamoff') and (Params[\"EXT_in_training\"] == True): #EXT only if extra EXT has been added\n",
    "            sample_test_dict[sample] = pd.read_pickle(loc_pkls+\"BDT_Test_dfs/pi0_selection/Test_beamoff_\"+Params[\"Run\"]+\".pkl\")\n",
    "        else: \n",
    "            sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"Preselected_{sample}_\"+Params[\"Run\"]+\"_my_vars_flattened_FINAL.pkl\")\n",
    "    else: #Standard sample types\n",
    "        if sample == 'overlay':\n",
    "            sample_test_dict[sample] = pd.read_pickle(loc_pkls+\"BDT_Test_dfs/Test_overlay_\"+Params[\"Run\"]+\"_FINAL.pkl\")\n",
    "        elif (Params[\"Load_single_file\"] == True) and (isinstance(sample,int)):\n",
    "            sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"BDT_Test_dfs/Test_signal_{sample}_\"+Params[\"Run\"]+\"_FINAL.pkl\")\n",
    "        elif sample in Constants.HNL_mass_samples:\n",
    "            sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"BDT_Test_dfs/Test_signal_{sample}_\"+Params[\"Run\"]+\"_FINAL.pkl\")\n",
    "        elif (sample == 'beamoff') and (Params[\"EXT_in_training\"] == True): #EXT only if extra EXT has been added\n",
    "            sample_test_dict[sample] = pd.read_pickle(loc_pkls+\"BDT_Test_dfs/Test_beamoff_\"+Params[\"Run\"]+\"_FINAL.pkl\")\n",
    "        else: \n",
    "            # sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"Preselected_{sample}_\"+Params[\"Run\"]+\"_my_vars_flattened_FINAL.pkl\")\n",
    "            sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"Preselected_{sample}_\"+Params[\"Run\"]+\"_flattened_FINAL.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30854c-0b46-43eb-806f-367c42e9a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_BDT_models(sample_dict, Params):\n",
    "    print(\"write this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492c66c-a88d-4a48-89af-94e21859745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Params[\"Run\"] == \"run1\": BDT_name = \"ultimate_full_reduced_benchmark\"\n",
    "# if Params[\"Run\"] == \"run3\": BDT_name = \"ultimate_benchmark_plus_flashmatch\"\n",
    "if (Params[\"Load_lepton_signal\"] == True) or (Params[\"Load_lepton_dirac\"] == True): \n",
    "    BDT_name = \"ee_FINAL_2\"\n",
    "    with open(f\"bdts/input_vars/{BDT_name}_\"+Params[\"Run\"], \"rb\") as fp:   # Unpickling\n",
    "        saved_variables = pickle.load(fp)\n",
    "if (Params[\"Load_pi0_signal\"] == True) or (Params[\"Load_pi0_dirac\"] == True): \n",
    "    BDT_name = \"pi0_FINAL_2\"\n",
    "    with open(f\"bdts/pi0_selection/input_vars/{BDT_name}_\"+Params[\"Run\"], \"rb\") as fp:   # Unpickling\n",
    "        saved_variables = pickle.load(fp)\n",
    "\n",
    "# if Params[\"Load_pi0_signal\"] == False:\n",
    "#     with open(f\"bdts/input_vars/{BDT_name}_\"+Params[\"Run\"], \"rb\") as fp:   # Unpickling\n",
    "#         saved_variables = pickle.load(fp)\n",
    "# elif Params[\"Load_pi0_signal\"] == True:\n",
    "#     with open(f\"bdts/pi0_selection/input_vars/{BDT_name}_\"+Params[\"Run\"], \"rb\") as fp:   # Unpickling\n",
    "#         saved_variables = pickle.load(fp)\n",
    "\n",
    "bdt_vars = saved_variables\n",
    "xgb_test_dict = {}\n",
    "\n",
    "for sample in sample_test_dict:\n",
    "    xgb_test_dict[sample] = xgboost.DMatrix(sample_test_dict[sample][bdt_vars])\n",
    "    # print(\"Done \" + str(sample))\n",
    "    \n",
    "if Params[\"Load_single_file\"] == True:\n",
    "    HNL_mass = Params[\"single_file\"]\n",
    "    bdt = xgboost.Booster()\n",
    "    bdt.load_model(f\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "    for sample in xgb_test_dict:\n",
    "        results = bdt.predict(xgb_test_dict[sample])\n",
    "        sample_test_dict[sample][f\"BDT_output_{HNL_mass}MeV\"] = results\n",
    "    \n",
    "elif Params[\"Load_lepton_signal\"] == True:\n",
    "    for HNL_mass in Constants.HNL_mass_samples:\n",
    "        bdt = xgboost.Booster()\n",
    "        bdt.load_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "        # bdt.load_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}_MeV_{BDT_name}.json\")\n",
    "        for sample in xgb_test_dict:\n",
    "            results = bdt.predict(xgb_test_dict[sample])\n",
    "            sample_test_dict[sample][f\"BDT_output_{HNL_mass}MeV\"] = results\n",
    "            \n",
    "elif Params[\"Load_pi0_signal\"] == True:\n",
    "    for HNL_mass in Constants.HNL_mass_pi0_samples:\n",
    "        bdt = xgboost.Booster()\n",
    "        # bdt.load_model(f\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}_MeV_pi0_FIXED.json\")\n",
    "        bdt.load_model(f\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "        for sample in xgb_test_dict:\n",
    "            results = bdt.predict(xgb_test_dict[sample])\n",
    "            sample_test_dict[sample][f\"BDT_output_{HNL_mass}MeV\"] = results\n",
    "            \n",
    "elif Params[\"Load_lepton_dirac\"] == True:\n",
    "    for HNL_mass in Constants.HNL_ee_dirac_mass_samples:\n",
    "        bdt = xgboost.Booster()\n",
    "        # bdt.load_model(f\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}_MeV_pi0_FIXED.json\")\n",
    "        bdt.load_model(f\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "        for sample in xgb_test_dict:\n",
    "            results = bdt.predict(xgb_test_dict[sample])\n",
    "            sample_test_dict[sample][f\"BDT_output_{HNL_mass}MeV\"] = results\n",
    "            \n",
    "elif Params[\"Load_pi0_dirac\"] == True:\n",
    "    for HNL_mass in Constants.HNL_pi0_dirac_mass_samples:\n",
    "        bdt = xgboost.Booster()\n",
    "        # bdt.load_model(f\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}_MeV_pi0_FIXED.json\")\n",
    "        bdt.load_model(f\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "        for sample in xgb_test_dict:\n",
    "            results = bdt.predict(xgb_test_dict[sample])\n",
    "            sample_test_dict[sample][f\"BDT_output_{HNL_mass}MeV\"] = results\n",
    "\n",
    "print(len(bdt_vars))\n",
    "print(bdt_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b92a01-f1c2-4143-b39e-4196eca1e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_test_dict['150_ee_dirac'].keys()\n",
    "sample_test_dict['150_pi0_dirac'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f25910-cafc-4541-aaac-b67c0558c08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c40481-4ea8-4c77-879d-6024bdee383f",
   "metadata": {},
   "source": [
    "## Checking max BDT score (for logit transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd70bcba-723a-481c-896b-b37562c25864",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Constants)\n",
    "if Params[\"Load_lepton_signal\"] == True: \n",
    "    HNL_masses = Constants.HNL_mass_samples\n",
    "    sample_names = Constants.HNL_ee_samples_names\n",
    "if Params[\"Load_pi0_signal\"] == True: \n",
    "    HNL_masses = Constants.HNL_mass_pi0_samples\n",
    "    sample_names = Constants.HNL_mass_pi0_samples_names\n",
    "if Params[\"Load_lepton_dirac\"] == True: \n",
    "    HNL_masses = Constants.HNL_ee_dirac_mass_samples\n",
    "    sample_names = Constants.HNL_ee_dirac_names\n",
    "if Params[\"Load_pi0_dirac\"] == True: \n",
    "    HNL_masses = Constants.HNL_pi0_dirac_mass_samples\n",
    "    sample_names = Constants.HNL_pi0_dirac_names\n",
    "elif Params[\"Load_single_file\"] == True: HNL_masses = [Params[\"single_file\"]]\n",
    "\n",
    "max_scores = []\n",
    "for HNL_mass in HNL_masses:\n",
    "    if Params[\"Load_lepton_signal\"] == True:\n",
    "        max_scores.append(max(Functions.logit(sample_test_dict[HNL_mass][f\"BDT_output_{HNL_mass}MeV\"])))\n",
    "    if Params[\"Load_pi0_signal\"] == True:\n",
    "        max_scores.append(max(Functions.logit(sample_test_dict[str(HNL_mass)+\"_pi0\"][f\"BDT_output_{HNL_mass}MeV\"])))\n",
    "    if Params[\"Load_lepton_dirac\"] == True:\n",
    "        max_scores.append(max(Functions.logit(sample_test_dict[str(HNL_mass)+\"_ee_dirac\"][f\"BDT_output_{HNL_mass}MeV\"])))\n",
    "    if Params[\"Load_pi0_dirac\"] == True:\n",
    "        max_scores.append(max(Functions.logit(sample_test_dict[str(HNL_mass)+\"_pi0_dirac\"][f\"BDT_output_{HNL_mass}MeV\"])))\n",
    "    \n",
    "max_all_scores = max(max_scores)\n",
    "print(\"Maximum score for all signal samples is \" + str(max_all_scores))\n",
    "\n",
    "max_score_int = np.ceil(max_all_scores)\n",
    "print(\"Maximum integer for bins is \" + str(max_score_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596562e-2410-4e48-ac46-0a19839a5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Params[\"Load_Signal_DetVars\"] == True: max_score_int = 7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c952252-1786-400e-aecd-7b94517223af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(HNL_masses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f93b37-bb23-4c7c-a7a9-11b5ff5eb6e3",
   "metadata": {},
   "source": [
    "## Merging bins with zero bkg prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a3394-63e1-4a4e-9d60-247777d0429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking which bins to merge Need to properly write this\n",
    "merged_bins_dict = {}\n",
    "min_score_cut = -5.0\n",
    "xlims = [min_score_cut,max_score_int]\n",
    "nbins = int(xlims[1]-xlims[0])\n",
    "min_overlay = 3 #The threshold of minimum overlay events required in a bin, used to be 1\n",
    "print(\"Initial nbins is \" + str(nbins))\n",
    "print(\"With range of \" + str(xlims))\n",
    "print(f\"Requiring more than {min_overlay} overlay events in each bin\")\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    bins_list = np.histogram(sample_test_dict['overlay'][f'BDT_output_{HNL_mass}MeV'],bins=nbins,range=xlims)[1]\n",
    "    if Params[\"Use_logit\"] == True:\n",
    "        bkg_scores=[Functions.logit(sample_test_dict['overlay'][f'BDT_output_{HNL_mass}MeV']),\n",
    "                    Functions.logit(sample_test_dict['dirtoverlay'][f'BDT_output_{HNL_mass}MeV']),\n",
    "                    Functions.logit(sample_test_dict['beamoff'][f'BDT_output_{HNL_mass}MeV'])]\n",
    "    totbkg=np.histogram(bkg_scores[0],bins=nbins,range=xlims)[0]+np.histogram(bkg_scores[1],bins=nbins,range=xlims)[0]+np.histogram(bkg_scores[2],bins=nbins,range=xlims)[0]\n",
    "    offbkg=np.histogram(bkg_scores[2],bins=nbins,range=xlims)[0]\n",
    "    overlaybkg=np.histogram(bkg_scores[0],bins=nbins,range=xlims)[0]\n",
    "    dirtbkg=np.histogram(bkg_scores[1],bins=nbins,range=xlims)[0]\n",
    "    bins_new=[]\n",
    "    for i,bin_bkg in enumerate(totbkg):\n",
    "        if(overlaybkg[i]>min_overlay): #Checking if unweighted overlay bkg has at least one event in the bin\n",
    "            bins_new.append(bins_list[i])\n",
    "\n",
    "    bins_new.append(bins_list[-1])\n",
    "    merged_bins_dict[HNL_mass] = bins_new\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306655e8-a2bf-47ce-ab6c-6044b43baa4d",
   "metadata": {},
   "source": [
    "## Plotting BDT outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896b786-2bd2-4eb3-91bf-7e28f82bc70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to add dirac samples into plot dict here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912923d0-818e-4422-b043-a1fe0644e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needs to be rewritten, perhaps made into a function\n",
    "train_vs_test_fraction = 0.7 #This was set in 3_BDT_training, need to change if I have changed this there.\n",
    "SF_test = 1.0/(1-train_vs_test_fraction)\n",
    "if (Params[\"Load_lepton_dirac\"]) or (Params[\"Load_pi0_dirac\"]): SF_test=1.0\n",
    "    \n",
    "if Params[\"Run\"] == \"run1\":\n",
    "    overlay_scale = Constants.SF_overlay_run1*SF_test\n",
    "    EXT_scale_list = np.ones(len(sample_test_dict['beamoff'][f'BDT_output_{HNL_mass}MeV']))*Constants.SF_EXT_run1\n",
    "    dirt_scale = Constants.SF_dirt_run1\n",
    "        \n",
    "elif Params[\"Run\"] == \"run3\":\n",
    "    overlay_scale = Constants.SF_overlay_run3*SF_test\n",
    "    EXT_scale_list = np.ones(len(sample_test_dict['beamoff'][f'BDT_output_{HNL_mass}MeV']))*Constants.SF_EXT_run3\n",
    "    dirt_scale = Constants.SF_dirt_run3\n",
    "\n",
    "print(\"Creating the sample and normalisation dictionaries\")    \n",
    "samples_plot={'overlay_test':sample_test_dict['overlay'],\n",
    "         'dirtoverlay':sample_test_dict['dirtoverlay'],\n",
    "         'beamoff':sample_test_dict['beamoff']}\n",
    "\n",
    "sample_norms={'overlay_test':np.array(sample_test_dict['overlay'][\"weight\"]*overlay_scale),\n",
    "         'dirtoverlay':np.array(sample_test_dict['dirtoverlay'][\"weight\"]*dirt_scale),\n",
    "         'beamoff':EXT_scale_list}\n",
    "\n",
    "print(\"Adding signal samples to sample plot dictionary\")\n",
    "if Params[\"Load_single_file\"] == True:\n",
    "    HNL_mass = Params[\"single_file\"]\n",
    "    signal_scale_list = np.ones(len(sample_test_dict[HNL_mass][f'BDT_output_{HNL_mass}MeV']))*SF_test\n",
    "    samples_plot[HNL_mass]=sample_test_dict[HNL_mass]\n",
    "    sample_norms[HNL_mass]=signal_scale_list\n",
    "    samples_plot[\"beamgood\"]=sample_test_dict[\"beamgood\"]\n",
    "    sample_norms[\"beamgood\"]=np.ones(len(sample_test_dict[\"beamgood\"]))\n",
    "    \n",
    "elif Params[\"Load_lepton_signal\"] == True:\n",
    "    for HNL_mass in Constants.HNL_mass_samples:\n",
    "        signal_scale_list = np.ones(len(sample_test_dict[HNL_mass][f'BDT_output_{HNL_mass}MeV']))*SF_test\n",
    "        samples_plot[HNL_mass]=sample_test_dict[HNL_mass]\n",
    "        sample_norms[HNL_mass]=signal_scale_list\n",
    "if Params[\"Load_pi0_signal\"] == True:\n",
    "    for HNL_mass in Constants.HNL_mass_pi0_samples:\n",
    "        signal_scale_list = np.ones(len(sample_test_dict[str(HNL_mass)+\"_pi0\"][f'BDT_output_{HNL_mass}MeV']))*SF_test\n",
    "        samples_plot[HNL_mass]=sample_test_dict[str(HNL_mass)+\"_pi0\"]\n",
    "        sample_norms[HNL_mass]=signal_scale_list\n",
    "    \n",
    "else: \n",
    "    for i, HNL_mass in enumerate(HNL_masses):\n",
    "        sig_name = sample_names[i]\n",
    "        signal_scale_list = np.ones(len(sample_test_dict[sig_name][f'BDT_output_{HNL_mass}MeV']))*SF_test\n",
    "        samples_plot[sig_name]=sample_test_dict[sig_name]\n",
    "        sample_norms[sig_name]=signal_scale_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a9e59-e952-4a06-b5ed-7b03a430a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_plot.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b235c8-35d8-486b-a77f-94c0be052bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(PT)\n",
    "\n",
    "# if Params[\"Load_single_file\"] == True:\n",
    "#     HNL_masses = [Params[\"single_file\"]]\n",
    "# elif Params[\"Load_lepton_signal\"] == True:\n",
    "#     HNL_masses = Constants.HNL_mass_samples\n",
    "# if Params[\"Load_pi0_signal\"] == True:\n",
    "#     HNL_masses = Constants.HNL_mass_pi0_samples\n",
    "    \n",
    "xlims=[0,max_score_int]    \n",
    "BINS = int(xlims[1]-xlims[0])\n",
    "    \n",
    "PT.Plot_BDT_output(HNL_masses=HNL_masses, signal_names=sample_names, samples=samples_plot, sample_norms=sample_norms, colours={}, xlims=xlims,\n",
    "                   bins=BINS,figsize=[12,8], MergeBins=True, density=False, legloc=\"best\",logy=False, savefig=False, \n",
    "                   save_str = BDT_name, Run=Params[\"Run\"], logit = Params[\"Use_logit\"], HNL_scale=0.001)\n",
    "\n",
    "#If plotting data too: Write this\n",
    "# PT.Plot_BDT_output_data(HNL_masses=HNL_masses, samples=samples_plot, sample_norms=sample_norms, colours={}, xlims=xlims,\n",
    "#                    bins=BINS,figsize=[12,8], MergeBins=True, density=False, legloc=\"best\",logy=False, savefig=True, \n",
    "#                    save_str = BDT_name, Run=Params[\"Run\"], logit = Params[\"Use_logit\"], HNL_scale=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70862c0a-eb3f-49db-bf68-d323d2ae6190",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving BDT output to .root files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d31dc-fc06-4287-81ce-719371104775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_one_hist(hist,name,nbins,xlims):\n",
    "    tData = ROOT.TH1F(name,name,len(nbins)-1,array(\"d\",nbins))\n",
    "    for i in range(len(nbins)-1):\n",
    "        tData.SetBinContent(i+1,hist['hist'][i])\n",
    "        tData.SetBinError(i+1,hist['err'][i])\n",
    "    return tData\n",
    "    \n",
    "def SaveToRoot_new(nbins,xlims,hist_samples,theta,fileName='test.root'): \n",
    "    rFile = ROOT.TFile(f'{fileName}','RECREATE')\n",
    "    tData = ROOT.TH1F(\"theta\",\"theta\",1,array(\"d\",[0,1]))\n",
    "    tData.SetBinContent(1,theta)\n",
    "    rFile.Write()\n",
    "    for i, name in enumerate(hist_samples):\n",
    "        tData = Save_one_hist(hist_samples[name],name,nbins,xlims)\n",
    "        rFile.Write()\n",
    "    rFile.Close()\n",
    "\n",
    "def make_stat_err(hist, SF): #This doesn't take the weights into account, assumes they average to 1.0\n",
    "    stat_err = []\n",
    "    for i in range(0,len(hist[0])):\n",
    "        error = np.sqrt(hist[0][i])*np.sqrt(SF)\n",
    "        stat_err.append(error)\n",
    "    return stat_err\n",
    "\n",
    "def make_stat_err_true(var, bins, weights_times_SF): #Saved in Functions.py\n",
    "    hist_unweighted = np.histogram(var,bins=bins)[0]\n",
    "    hist_weighted = np.histogram(var,bins=bins,weights=weights_times_SF)[0]\n",
    "    Total_SF = np.nan_to_num(hist_weighted/hist_unweighted)\n",
    "    stat_err = np.sqrt(hist_unweighted)*Total_SF\n",
    "    return stat_err\n",
    "\n",
    "def make_stat_err_new(var, bins, weights_times_SF): \n",
    "    hist_squared = np.histogram(var,bins=bins,weights=weights_times_SF**2)[0]\n",
    "    stat_err=np.sqrt(hist_squared)\n",
    "    return stat_err\n",
    "\n",
    "def SF_times_weights(sample_dict, sample, Run, SF_test, SF_signal):\n",
    "    if Run == \"run1\": POT_norm = Constants.run1_POT_scaling_dict\n",
    "    if Run == \"run3\": POT_norm = Constants.run3_POT_scaling_dict\n",
    "    \n",
    "    if sample == 'overlay': SF = sample_dict[sample]['weight']*POT_norm[sample]*SF_test\n",
    "    elif sample == 'dirtoverlay': SF = sample_dict[sample]['weight']*POT_norm[sample]\n",
    "    elif (sample == 'beamoff') or (sample =='beamgood'): SF = np.ones(len(sample_dict[sample][f'run']))*POT_norm[sample]\n",
    "    else: \n",
    "        SF = np.ones(len(sample_dict[sample][f'BDT_output_{sample}MeV']))*(POT_norm[sample]*SF_test*SF_signal)\n",
    "    return SF\n",
    "\n",
    "def SF_times_weights_pi0(sample_dict, sample, Run, SF_test, SF_signal):\n",
    "    if Run == \"run1\": POT_norm = Constants.run1_POT_scaling_dict\n",
    "    if Run == \"run3\": POT_norm = Constants.run3_POT_scaling_dict\n",
    "    \n",
    "    if sample == 'overlay': SF = sample_dict[sample]['weight']*POT_norm[sample]*SF_test\n",
    "    elif sample == 'dirtoverlay': SF = sample_dict[sample]['weight']*POT_norm[sample]\n",
    "    elif (sample == 'beamoff') or (sample =='beamgood'): SF = np.ones(len(sample_dict[sample][f'run']))*POT_norm[sample]\n",
    "    else: \n",
    "        SF = np.ones(len(sample_dict[str(sample)+\"_pi0\"][f'BDT_output_{sample}MeV']))*(POT_norm[str(sample)+\"_pi0\"]*SF_test*SF_signal)\n",
    "    return SF\n",
    "\n",
    "def SF_times_weights_sample_names(sample_dict, sample, HNL_mass, Run, SF_test, SF_signal):\n",
    "    if Run == \"run1\": POT_norm = Constants.run1_POT_scaling_dict\n",
    "    if Run == \"run3\": POT_norm = Constants.run3_POT_scaling_dict\n",
    "    \n",
    "    if sample == 'overlay': SF = sample_dict[sample]['weight']*POT_norm[sample]*SF_test\n",
    "    elif sample == 'dirtoverlay': SF = sample_dict[sample]['weight']*POT_norm[sample]\n",
    "    elif (sample == 'beamoff') or (sample =='beamgood'): SF = np.ones(len(sample_dict[sample][f'run']))*POT_norm[sample]\n",
    "    else: \n",
    "        SF = np.ones(len(sample_dict[sample][f'BDT_output_{HNL_mass}MeV']))*(POT_norm[sample]*SF_test*SF_signal)\n",
    "    return SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797887d5-1b1f-4c0f-a297-b5146ac5d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE_UP_dict = {2:5,10:5,20:200,50:50,100:10,150:5,180:4,200:4,220:4,240:4,245:4} #Doing this because the scale factor used in pyhf is bounded\n",
    "SCALE_UP_dict = {2:1.0,10:1.0,20:1.0,50:1.0,100:1.0,150:1.0}\n",
    "\n",
    "new_theta_dict, dict_for_root = {}, {}\n",
    "\n",
    "if Params[\"Load_pi0_signal\"] == True: exit() #This cell is for lepton samples\n",
    "\n",
    "bkg_samples = []\n",
    "for sample in samples:\n",
    "    if isinstance(sample,str): bkg_samples.append(sample)\n",
    "\n",
    "train_vs_test_fraction = 0.7 #This was set in 3_BDT_training, need to change if I have changed this there.\n",
    "SF_test = 1.0/(1-train_vs_test_fraction)\n",
    "\n",
    "print(\"Saving minimum BDT score of \" + str(merged_bins_dict[150][0]))\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    original_theta = Constants.theta_mu_4_dict[HNL_mass]\n",
    "    new_theta = original_theta*SCALE_UP_dict[HNL_mass]\n",
    "    new_theta_dict[HNL_mass] = original_theta*SCALE_UP_dict[HNL_mass]\n",
    "    SCALE_EVENTS = SCALE_UP_dict[HNL_mass]**4 #The Number of events is proportional to theta^4\n",
    "    \n",
    "    bins = merged_bins_dict[HNL_mass]\n",
    "    sample_list = bkg_samples + [HNL_mass]\n",
    "    # if Params[\"Load_data\"] == True: sample_list += [\"beamgood\"]\n",
    "    for sample in sample_list:\n",
    "        score = Functions.logit(sample_test_dict[sample][f'BDT_output_{HNL_mass}MeV'])\n",
    "        SF_list = SF_times_weights(sample_test_dict, sample, Params[\"Run\"], SF_test, SCALE_EVENTS)\n",
    "        stat_err = make_stat_err_true(score, bins, SF_list)\n",
    "        hist = np.histogram(score, bins=bins, weights=SF_list)\n",
    "        dict_for_root[sample] = {'bins': np.array(hist[1]), 'hist': np.array(hist[0]), 'err': np.array(stat_err)}\n",
    "        \n",
    "    hist_samples = {\"bkg_overlay\":dict_for_root['overlay'],\"bkg_dirt\":dict_for_root['dirtoverlay'],\n",
    "                    \"bkg_EXT\":dict_for_root['beamoff'],\"signal\":dict_for_root[HNL_mass]}\n",
    "    if 'beamgood' in bkg_samples:\n",
    "        hist_samples.update({\"data\":dict_for_root['beamgood']})\n",
    "        \n",
    "    save_name = Params[\"Run\"]+f\"_{HNL_mass}MeV_FINAL_Dirac.root\"\n",
    "    \n",
    "    SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"bdt_output/\"+save_name)\n",
    "    SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"Uncertainties/\"+save_name)\n",
    "print(\"Saved all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750fff37-4cbe-441f-9052-8f078c9c7659",
   "metadata": {},
   "source": [
    "## Using sample_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59975b-1c4a-4c48-9a91-d616c3906a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_bins_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c0409-eb43-4c0a-b71d-9202946dffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE_UP_dict = {2:5,10:5,20:200,50:50,100:10,150:5,180:4,200:4,220:4,240:4,245:4} #Doing this because the scale factor used in pyhf is bounded\n",
    "SCALE_UP_dict = {2:1.0,10:1.0,20:1.0,50:1.0,100:1.0,150:1.0, 180:1.0, 200:1.0, 220:1.0, 240:1.0, 245:1.0}\n",
    "\n",
    "new_theta_dict, dict_for_root = {}, {}\n",
    "\n",
    "if Params[\"Load_pi0_signal\"] == True: exit() #This cell is for lepton samples\n",
    "\n",
    "bkg_samples = []\n",
    "for sample in samples:\n",
    "    if isinstance(sample,str): bkg_samples.append(sample)\n",
    "\n",
    "train_vs_test_fraction = 0.7 #This was set in 3_BDT_training, need to change if I have changed this there.\n",
    "SF_test = 1.0/(1-train_vs_test_fraction)\n",
    "if (Params[\"Load_lepton_dirac\"]) or (Params[\"Load_pi0_dirac\"]): SF_test=1.0 #Didn't train on these samples so don't scale up again. \n",
    "\n",
    "print(\"Saving minimum BDT score of \" + str(merged_bins_dict[150][0]))\n",
    "\n",
    "for i, HNL_mass in enumerate(HNL_masses):\n",
    "    sig_name = sample_names[i]\n",
    "    original_theta = Constants.theta_mu_4_dict[HNL_mass]\n",
    "    new_theta = original_theta*SCALE_UP_dict[HNL_mass]\n",
    "    new_theta_dict[HNL_mass] = original_theta*SCALE_UP_dict[HNL_mass]\n",
    "    SCALE_EVENTS = SCALE_UP_dict[HNL_mass]**4 #The Number of events is proportional to theta^4\n",
    "    \n",
    "    bins = merged_bins_dict[HNL_mass]\n",
    "    sample_list = bkg_samples + [sig_name]\n",
    "    # if Params[\"Load_data\"] == True: sample_list += [\"beamgood\"]\n",
    "    for sample in sample_list:\n",
    "        score = Functions.logit(sample_test_dict[sample][f'BDT_output_{HNL_mass}MeV'])\n",
    "        SF_list = SF_times_weights_sample_names(sample_test_dict, sample, HNL_mass, Params[\"Run\"], SF_test, SCALE_EVENTS)\n",
    "        stat_err = make_stat_err_true(score, bins, SF_list)\n",
    "        hist = np.histogram(score, bins=bins, weights=SF_list)\n",
    "        dict_for_root[sample] = {'bins': np.array(hist[1]), 'hist': np.array(hist[0]), 'err': np.array(stat_err)}\n",
    "        \n",
    "    hist_samples = {\"bkg_overlay\":dict_for_root['overlay'],\"bkg_dirt\":dict_for_root['dirtoverlay'],\n",
    "                    \"bkg_EXT\":dict_for_root['beamoff'],\"signal\":dict_for_root[sample]}\n",
    "    if 'beamgood' in bkg_samples:\n",
    "        hist_samples.update({\"data\":dict_for_root['beamgood']})\n",
    "        \n",
    "    save_name = Params[\"Run\"]+f\"_{HNL_mass}MeV_FINAL_Dirac.root\"\n",
    "    \n",
    "    if Params[\"Load_pi0_dirac\"]==True:\n",
    "        SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"bdt_output/pi0/\"+save_name)\n",
    "        SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"Uncertainties/pi0/\"+save_name)\n",
    "    else:\n",
    "        SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"bdt_output/\"+save_name)\n",
    "        SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"Uncertainties/\"+save_name)\n",
    "print(\"Saved all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed859c-acc2-47ef-ad96-41fe77bfd9bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving pi0 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1d878-d6af-4735-b08a-a0362a02f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Constants.run1_POT_scaling_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0cfd0c-34f7-48e5-9711-2177b095f0c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCALE_UP_pi0_dict = {150:1.0, 180:1.0, 200:1.0, 220:1.0, 240:1.0, 245:1.0} \n",
    "\n",
    "new_theta_dict, dict_for_root = {}, {}\n",
    "\n",
    "if Params[\"Load_pi0_signal\"] == False: exit()\n",
    "\n",
    "# bkg_samples = []\n",
    "# for sample in samples:\n",
    "#     if isinstance(sample,str): bkg_samples.append(sample)\n",
    "bkg_samples = ['overlay', 'dirtoverlay', 'beamoff']\n",
    "if Params[\"Load_data\"] == True: bkg_samples = bkg_samples + ['beamgood']\n",
    "\n",
    "train_vs_test_fraction = 0.7 #This was set in 3_BDT_training, need to change if I have changed this there.\n",
    "SF_test = 1.0/(1-train_vs_test_fraction)\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    original_theta = 1e-4 #Same for all samples\n",
    "    new_theta = original_theta*SCALE_UP_pi0_dict[HNL_mass]\n",
    "    new_theta_dict[HNL_mass] = original_theta*SCALE_UP_pi0_dict[HNL_mass]\n",
    "    SCALE_EVENTS = SCALE_UP_pi0_dict[HNL_mass]**4 #The Number of events is proportional to theta^4\n",
    "    \n",
    "    bins = merged_bins_dict[HNL_mass]\n",
    "    sample_list = bkg_samples + [HNL_mass]\n",
    "    # if Params[\"Load_data\"] == True: sample_list += [\"beamgood\"]\n",
    "    # print(sample_list)\n",
    "    for sample in sample_list:\n",
    "        sample_test = sample\n",
    "        if isinstance(sample,int): sample_test = str(sample) + \"_pi0\"\n",
    "        score = Functions.logit(sample_test_dict[sample_test][f'BDT_output_{HNL_mass}MeV'])\n",
    "        SF_list = SF_times_weights_pi0(sample_test_dict, sample, Params[\"Run\"], SF_test, SCALE_EVENTS)\n",
    "        stat_err = make_stat_err_true(score, bins, SF_list)\n",
    "        hist = np.histogram(score, bins=bins, weights=SF_list)\n",
    "        dict_for_root[sample] = {'bins': np.array(hist[1]), 'hist': np.array(hist[0]), 'err': np.array(stat_err)}\n",
    "        \n",
    "    hist_samples = {\"bkg_overlay\":dict_for_root['overlay'],\"bkg_dirt\":dict_for_root['dirtoverlay'],\n",
    "                    \"bkg_EXT\":dict_for_root['beamoff'],\"signal\":dict_for_root[HNL_mass]}\n",
    "    if 'beamgood' in bkg_samples:\n",
    "        hist_samples.update({\"data\":dict_for_root['beamgood']})\n",
    "        \n",
    "    save_name = Params[\"Run\"]+f\"_{HNL_mass}MeV_FINAL_3.root\"\n",
    "    \n",
    "    SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"bdt_output/pi0/\"+save_name)\n",
    "    SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"Uncertainties/pi0/\"+save_name)\n",
    "print(\"Saved all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9a478-704c-4b02-b0e3-a06763631a98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Calculating efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44822b-e324-4216-b3d7-e4967c8c4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_efficiency(df, file_name, Run, BDT_cutoff, HNL_mass, debug=False):\n",
    "    if Run == \"run1\": ev_num_dict = Constants.run1_event_numbers\n",
    "    if Run == \"run3\": ev_num_dict = Constants.run3_event_numbers\n",
    "    \n",
    "    if file_name == \"beamoff\" or file_name == \"dirtoverlay\":\n",
    "        norm = 1.0\n",
    "    else:\n",
    "        train_vs_test_fraction = 0.7 #Set in script 3.0\n",
    "        norm = 1.0/(1-train_vs_test_fraction) #Need to scale up to account for events lost\n",
    "    \n",
    "    total_ev_num = len(df)\n",
    "    ev_num_initial = ev_num_dict[file_name]\n",
    "    cut_df = df.query(f\"BDT_output_{HNL_mass}MeV > {BDT_cutoff}\")\n",
    "    new_ev_num = len(cut_df)\n",
    "    \n",
    "    efficiency = norm*(new_ev_num/ev_num_initial)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Total events is {total_ev_num}\")\n",
    "        print(f\"Events after cut is {new_ev_num}\")\n",
    "    \n",
    "    return efficiency\n",
    "\n",
    "def sum_total_weights(df): #This is to check that the weighting doesn't affect the overall efficiency significantly (i.e av weight is approx 1)\n",
    "    total_evs = len(df)\n",
    "    sum_weights = df[\"weight\"].sum()\n",
    "    \n",
    "    weight_per_ev = sum_weights/total_evs\n",
    "    \n",
    "    print(f\"Sum of weights is {sum_weights}\")\n",
    "    print(f\"Average weight is {weight_per_ev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3134f-a034-452a-83bf-09f98a074793",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_test_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59979a5c-4225-479a-8a81-3a762dd82a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_BDT_cut = -20\n",
    "# logit_BDT_cut = 2.0\n",
    "standard_BDT_cut = Functions.invlogit(logit_BDT_cut)\n",
    "print(f\"Using a logit BDT cut of {logit_BDT_cut}, which corresponds to {standard_BDT_cut} in [0,1] BDT score.\")\n",
    "\n",
    "efficiency_dict = {}\n",
    "\n",
    "for file in Constants.HNL_mass_samples: #Just signal samples\n",
    "# for file in sample_test_dict:\n",
    "\n",
    "    efficiency_dict[file] = calculate_efficiency(sample_test_dict[file], file, Params[\"Run\"], standard_BDT_cut, file, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebfe02-3497-4c7e-beec-b8419bd06b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in ['overlay', 'dirtoverlay', 'beamoff']: #Calculating efficiency for 100MeV BDT model score\n",
    "\n",
    "    efficiency_dict[file] = calculate_efficiency(sample_test_dict[file], file, Params[\"Run\"], standard_BDT_cut, 100, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d484621-3f26-4510-9c8b-37a7dc0d643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Params[\"Run\"] + \" pre-selectoin efficiencies are\")\n",
    "for file in sample_test_dict:\n",
    "    # print(f\"{file} efficiency is \" + str(efficiency_dict[file]))\n",
    "    print(f\"{file} efficiency is \" + str(efficiency_dict[file]*100) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46e5f7-ea37-4d7a-9ab0-818012d23026",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## \"Closure test\" i.e testing adjacent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b637f-ce6a-4900-b20e-c6a82035e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass_point = 50\n",
    "mass_point = 245\n",
    "\n",
    "savefig = False\n",
    "\n",
    "test_models = [150, 180]\n",
    "\n",
    "test_results, test_labels = [], []\n",
    "\n",
    "plt.figure(figsize=[12,8],facecolor='white')\n",
    "\n",
    "for mass_model in test_models:\n",
    "    test_results.append(Functions.logit(samples_plot[mass_point][f'BDT_output_{mass_model}MeV']))\n",
    "    test_labels.append(f\"{mass_model} MeV model\")\n",
    "\n",
    "plot=plt.hist(Functions.logit(samples_plot[mass_point][f'BDT_output_{mass_point}MeV']),label=f\"Correct {mass_point} MeV model\",\n",
    "                         range=xlims,bins=merged_bins_dict[mass_point],histtype=\"step\",\n",
    "                         stacked=False,density=False,linewidth=4)\n",
    "plot=plt.hist(test_results,label=test_labels,range=xlims,bins=merged_bins_dict[mass_point],histtype=\"step\",\n",
    "              stacked=False,density=False,linewidth=2)\n",
    "\n",
    "plt.xlabel(f\"BDT score for {mass_point} MeV sample\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "if savefig == True:\n",
    "    plt.savefig(\"plots/BDT_output/Adjacent_models_tests/\" + Params[\"Run\"] + \"_\" + str(mass_point) + \"MeV_adjcent_models.pdf\")\n",
    "    plt.savefig(\"plots/BDT_output/Adjacent_models_tests/\" + Params[\"Run\"] + \"_\" + str(mass_point) + \"MeV_adjcent_models.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfb395-b3dc-4fcc-83ad-60a025d0e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the output in a .root file\n",
    "bkg_list = ['overlay', 'dirtoverlay', 'beamoff']\n",
    "new_theta = original_theta*SCALE_UP_dict[mass_point]\n",
    "bkg_hist_dict = {}\n",
    "# test_models += [mass_point]\n",
    "# res = [*set(test_models)]\n",
    "# print(res)\n",
    "# print(test_models)\n",
    "for mass_model in test_models+[mass_point]:\n",
    "    # for bkg in bkg_list:\n",
    "    #     bkg_hist_dict[bkg] = Functions.logit(sample_test_dict[bkg][f'BDT_output_{mass_model}MeV'])\n",
    "    SF_signal_list = np.ones(len(sample_test_dict[mass_point][f'BDT_output_{mass_model}MeV']))*Constants.run1_POT_scaling_dict[HNL_mass]*SF_test*SCALE_EVENTS\n",
    "    SF_overlay_list = sample_test_dict['overlay']['weight']*Constants.SF_overlay_run1*SF_test\n",
    "    SF_dirt_list = sample_test_dict['dirtoverlay']['weight']*Constants.SF_dirt_run1\n",
    "    SF_EXT_list = np.ones(len(sample_test_dict['beamoff'][f'BDT_output_{mass_model}MeV']))*Constants.SF_EXT_run1\n",
    "    \n",
    "    bins=merged_bins_dict[mass_point]\n",
    "    signal_bdt_hist = np.histogram(Functions.logit(sample_test_dict[mass_point][f'BDT_output_{mass_model}MeV']), \n",
    "                               bins=bins, weights=SF_signal_list, range = xlims_logit)\n",
    "    overlay_bdt_hist = np.histogram(Functions.logit(sample_test_dict['overlay'][f'BDT_output_{mass_model}MeV']), \n",
    "                                bins=bins, weights=SF_overlay_list, range = xlims_logit)\n",
    "    dirt_bdt_hist = np.histogram(Functions.logit(sample_test_dict['dirtoverlay'][f'BDT_output_{mass_model}MeV']), \n",
    "                             bins=bins, weights=SF_dirt_list, range = xlims_logit)\n",
    "    EXT_bdt_hist = np.histogram(Functions.logit(sample_test_dict['beamoff'][f'BDT_output_{mass_model}MeV']), \n",
    "                            bins=bins, weights=SF_EXT_list, range = xlims_logit)\n",
    "\n",
    "    signal_bdt_err = make_stat_err(signal_bdt_hist, Constants.run1_POT_scaling_dict[HNL_mass]*SF_test*SCALE_EVENTS)\n",
    "    overlay_bdt_err = make_stat_err(overlay_bdt_hist, Constants.SF_overlay_run1*SF_test)\n",
    "    dirt_bdt_err = make_stat_err(dirt_bdt_hist, Constants.SF_dirt_run1)\n",
    "    EXT_bdt_err = make_stat_err(EXT_bdt_hist, Constants.SF_EXT_run1)\n",
    "\n",
    "    data_h = overlay_bdt_hist[0]+dirt_bdt_hist[0]+EXT_bdt_hist[0]\n",
    "\n",
    "    bkg_overlay = {'bins': np.array(overlay_bdt_hist[1]), 'hist': np.array(overlay_bdt_hist[0]), 'err': np.array(overlay_bdt_err)}\n",
    "    bkg_dirt = {'bins': np.array(dirt_bdt_hist[1]), 'hist': np.array(dirt_bdt_hist[0]), 'err': np.array(dirt_bdt_err)}\n",
    "    bkg_EXT = {'bins': np.array(EXT_bdt_hist[1]), 'hist': np.array(EXT_bdt_hist[0]), 'err': np.array(EXT_bdt_err)}\n",
    "    sig = {'bins': np.array(signal_bdt_hist[1]), 'hist': np.array(signal_bdt_hist[0]), 'err': np.array(signal_bdt_err)}\n",
    "    data = {'bins': np.array(overlay_bdt_hist[1]), 'hist': np.array(data_h), 'err': np.array(overlay_bdt_err)}\n",
    "\n",
    "    hist_samples = {\"bkg_overlay\":bkg_overlay,\"bkg_dirt\":bkg_dirt,\"bkg_EXT\":bkg_EXT,\"signal\":sig,\"data\":data}\n",
    "\n",
    "    save_name = f\"bdt_output/adjacent_models/\"+ Params[\"Run\"] + f\"_{mass_point}MeV_{mass_model}MeV_model.root\"\n",
    "    SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=save_name)\n",
    "    print(\"Saved \" + save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ebca0-be3e-4a28-97c9-86fb959899cd",
   "metadata": {},
   "source": [
    "## Saving BDT output for reweighting systematics plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee823212-6e51-4aaf-bc9d-dcb8b03a66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_results_loc = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/Results/\"\n",
    "if Params[\"Load_pi0_signal\"] == True: pkl_results_loc += \"pi0/\"\n",
    "\n",
    "# print(sample_test_dict['overlay'].keys())\n",
    "\n",
    "print_vals = input(\"Do you want to save the new BDT scores to .pkl files? y/n \")\n",
    "if print_vals == \"y\":\n",
    "    \n",
    "    columns = Variables.event_vars + Variables.weight_related + [\"weight\"]\n",
    "    for HNL_mass in merged_bins_dict:\n",
    "        columns.append(f'BDT_output_{HNL_mass}MeV')\n",
    "    print(columns)\n",
    "    \n",
    "    overlay_to_save = sample_test_dict['overlay'][columns].copy()\n",
    "    #df2 = overlay_merged_to_save.loc[:,~overlay_merged_to_save.columns.duplicated()] #Getting rid of duplicate \"weight\" column\n",
    "    #df2.to_pickle(pkl_results_loc + \"overlay_results.pkl\")\n",
    "    overlay_to_save.to_pickle(pkl_results_loc + \"overlay_results_FINAL_3.pkl\")\n",
    "    \n",
    "    print(overlay_to_save.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9fbe06-875c-4ef8-8d36-638646360096",
   "metadata": {},
   "source": [
    "## Saving only high BDT Score events for variable study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695497be-be6d-45c3-9125-351686a4dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.95\n",
    "signal_like = {}\n",
    "frac_retained = {}\n",
    "pkl_variable_tests_loc = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/Variable_tests/\"\n",
    "\n",
    "Number_signal = 1000\n",
    "top_signal = {}\n",
    "\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    signal_like[HNL_mass] = sample_test_dict[HNL_mass].query(f\"BDT_output_{HNL_mass}MeV > {cutoff}\")\n",
    "    frac_retained[HNL_mass] = len(signal_like[HNL_mass])/len(sample_test_dict[HNL_mass])\n",
    "    \n",
    "    signal_like[HNL_mass].to_pickle(pkl_variable_tests_loc + f\"signal_like_{HNL_mass}MeV.pkl\")\n",
    "    \n",
    "    top_signal[HNL_mass] = sample_test_dict[HNL_mass].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(Number_signal)\n",
    "    \n",
    "print(frac_retained)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a07a5-acf6-4e6c-9ca0-ff5c930fabd1",
   "metadata": {},
   "source": [
    "## Looking at variables for signal-like events vs. full sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1346efb-8389-4995-a70a-d6fadc614ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load signal-like pkls\n",
    "pkl_variable_tests_loc = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/Variable_tests/\"\n",
    "\n",
    "signal_like[HNL_mass] = {}\n",
    "\n",
    "for HNL_mass in [20,50,100]:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b417309-8164-4f36-97b9-d1f6c6da1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,8],facecolor='white')\n",
    "\n",
    "HNL_mass = 100\n",
    "\n",
    "variable = 'shr_theta_v'\n",
    "#'shr_theta_v', 'shr_phi_v', 'trk_theta_v', 'trk_phi_v'\n",
    "\n",
    "bins = 40\n",
    "xlims = [0,3.2]\n",
    "\n",
    "plt.hist(sample_test_dict[HNL_mass][variable],\n",
    "#               label=[f\"HNL ({mass} MeV) \\n $|U_{{\\mu4}}|^2=\"+sci_notation(sample_info[\"300\"][\"theta_u2\"]) +f\" (x{HNLplotscale})\"],\n",
    "         label=[f\"All preselected {HNL_mass} MeV HNL\"],\n",
    "         range=xlims,bins=bins,\n",
    "         stacked=False,density=True,\n",
    "         histtype=\"step\",lw=3)\n",
    "\n",
    "plt.hist(top_signal[HNL_mass][variable],\n",
    "#               label=[f\"HNL ({mass} MeV) \\n $|U_{{\\mu4}}|^2=\"+sci_notation(sample_info[\"300\"][\"theta_u2\"]) +f\" (x{HNLplotscale})\"],\n",
    "         label=[f\"Signal-like {HNL_mass} MeV HNL\"],\n",
    "         range=xlims,bins=bins,\n",
    "         stacked=False,density=True,\n",
    "         histtype=\"step\",lw=3)\n",
    "plt.xlabel(variable)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25909893-a728-4a26-b351-cb43c089e251",
   "metadata": {},
   "source": [
    "## Make list of run subrun event which are most signal-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65122c06-640e-450a-b49c-2f0ef6966a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_CV_100 = sample_test_dict['100_CV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622ac58-ed31-41dd-a030-746a64eb8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 50\n",
    "csv_location = \"bdt_output/signal_like_rse_lists/CV_samples/\"\n",
    "HNL_mass = 100\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "signal_sorted = sample_CV_100.sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "signal_run_sub_event = signal_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "csv_name = csv_location + Params[\"Run\"] + f\"_signal_CV_{HNL_mass}.csv\"\n",
    "signal_run_sub_event.to_csv(csv_name, sep=\" \", header=False, index=False)\n",
    "\n",
    "txt_file = csv_location + Params[\"Run\"] + f\"_signal_CV_{HNL_mass}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_name, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a7a88-8e83-4b3f-87d8-97f5cc9c8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 50\n",
    "csv_location = \"bdt_output/signal_like_rse_lists/\"\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    signal_sorted = sample_test_dict[HNL_mass].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "    overlay_sorted = sample_test_dict['overlay'].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "    dirt_sorted = sample_test_dict['dirtoverlay'].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "    beamoff_sorted = sample_test_dict['beamoff'].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "    signal_run_sub_event = signal_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "    overlay_run_sub_event = overlay_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "    dirt_run_sub_event = dirt_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "    beamoff_run_sub_event = beamoff_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "    # print(df_run_sub_event.keys())\n",
    "    csv_name = csv_location + Params[\"Run\"] + f\"_signal_{HNL_mass}.csv\"\n",
    "    csv_overlay_name = csv_location + Params[\"Run\"] + f\"_overlay_{HNL_mass}.csv\"\n",
    "    csv_dirt_name = csv_location + Params[\"Run\"] + f\"_dirtoverlay_{HNL_mass}.csv\"\n",
    "    csv_beamoff_name = csv_location + Params[\"Run\"] + f\"_beamoff_{HNL_mass}.csv\"\n",
    "\n",
    "    signal_run_sub_event.to_csv(csv_name, sep=\" \", header=False, index=False) #i.e separating by a space, removing column names\n",
    "    overlay_run_sub_event.to_csv(csv_overlay_name, sep=\" \", header=False, index=False)\n",
    "    dirt_run_sub_event.to_csv(csv_dirt_name, sep=\" \", header=False, index=False)\n",
    "    beamoff_run_sub_event.to_csv(csv_beamoff_name, sep=\" \", header=False, index=False)\n",
    "    \n",
    "    #signal\n",
    "    csv_file = csv_name\n",
    "    txt_file = csv_location + Params[\"Run\"] + f\"_signal_{HNL_mass}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "    with open(txt_file, \"w\") as my_output_file:\n",
    "        with open(csv_file, \"r\") as my_input_file:\n",
    "            [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "        my_output_file.close()\n",
    "        \n",
    "    #overlay\n",
    "    csv_file = csv_overlay_name\n",
    "    txt_file = csv_location + Params[\"Run\"] + f\"_overlay_{HNL_mass}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "    with open(txt_file, \"w\") as my_output_file:\n",
    "        with open(csv_file, \"r\") as my_input_file:\n",
    "            [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "        my_output_file.close()\n",
    "        \n",
    "    #dirt\n",
    "    csv_file = csv_dirt_name\n",
    "    txt_file = csv_location + Params[\"Run\"] + f\"_dirtoverlay_{HNL_mass}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "    with open(txt_file, \"w\") as my_output_file:\n",
    "        with open(csv_file, \"r\") as my_input_file:\n",
    "            [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "        my_output_file.close()\n",
    "        \n",
    "    #beamoff\n",
    "    csv_file = csv_beamoff_name\n",
    "    txt_file = csv_location + Params[\"Run\"] + f\"_beamoff_{HNL_mass}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "    with open(txt_file, \"w\") as my_output_file:\n",
    "        with open(csv_file, \"r\") as my_input_file:\n",
    "            [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "        my_output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275803f-0b98-43c4-9871-b8f16419b88f",
   "metadata": {},
   "source": [
    "# End of code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
