{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab81f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, string, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from platform import python_version\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import math\n",
    "from matplotlib.patches import Rectangle\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from importlib import reload\n",
    "import csv\n",
    "import ROOT\n",
    "from array import array\n",
    "\n",
    "import Utilities.Plotter as PT\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Variables_list as Variables\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print ('Success')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb601be",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook loads the BDT models trained in script 3 and saves the BDT output scores of the various samples. <br>\n",
    "There is also code that merges the high BDT score bins if there are too few background events. Therefore the binning of the distributions is determined at this stage of the analysis. <br>\n",
    "The notebook also contains code that applies BDT models to the adjacent mass points to the one they were trained for. This is for the purposes of a sort of \"closure test\" where we look at limit acquired if the adjacent BDT models is applied, just to justify the interpolation we apply between limit points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a3504-392d-4b23-a225-0db439fcb0f5",
   "metadata": {},
   "source": [
    "## Reading in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de87920a-3a93-4217-93ab-1963f9066403",
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = {\"Run\":\"run3\", #The run number, so far either \"run1\" or \"run3\"\n",
    "          \"Load_single_file\":False, #This will override everything else, put the desired file in the \"single_file\" line\n",
    "          \"single_file\":245,\n",
    "          \"Load_standard\":True, #bkgs\n",
    "          \"Load_lepton_signal\":False,\n",
    "          \"Load_lepton_dirac\":False,\n",
    "          \"Load_pi0_signal\":True,\n",
    "          \"Load_pi0_dirac\":False,\n",
    "          \"Load_DetVars\":False, #This is for overlay\n",
    "          \"Only_keep_common_DetVar_evs\":True,\n",
    "          \"Load_Signal_DetVars\":False, #Don't do here, but in seperate script\n",
    "          'Load_pi0_signal_DetVars':False, #Don't do here, but in seperate script\n",
    "          \"Load_data\":True,\n",
    "          \"FLATTEN\":True, #Have one row per reconstructed object in the analysis dataframe\n",
    "          \"only_presel\":False, #Create small files containing only variables necessary for pre-selection, for making pre-selection plots\n",
    "          \"EXT_in_training\":True,\n",
    "          \"dirt_in_training\":True,\n",
    "          \"Use_logit\":True,\n",
    "          \"nbins\":5} \n",
    "\n",
    "loc_pkls = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/my_vars/\"\n",
    "\n",
    "samples = Functions.create_test_samples_list(Params)\n",
    "\n",
    "if Params[\"Load_pi0_signal\"] == True:\n",
    "    pi0_sample_strings = [] #Unfortunately need to make, to discriminate lepton final states from pi0 final states for signal\n",
    "    for pi0_point in Constants.HNL_mass_pi0_samples:\n",
    "        pi0_sample_strings += [str(pi0_point)+\"_pi0\"]\n",
    "        \n",
    "if (Params[\"Load_lepton_signal\"] == True) or (Params[\"Load_lepton_dirac\"] == True): loc = \"bdts/\"\n",
    "else: loc = \"bdts/pi0_selection/\"\n",
    "        \n",
    "# end_string = \"_FINAL\"\n",
    "end_string = \"_full_Finished_10\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627116c-1b50-4017-a0dd-4c3b380f7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_preselected_pkls(samples, Params, loc_pkls, filename, Test_filename):\n",
    "    \"\"\"\n",
    "    For loading in preselected pkl files, including BDT test pkls. \n",
    "    \"\"\"\n",
    "    sample_test_dict = {}\n",
    "    sig_names_list = Constants.HNL_ee_samples_names+Constants.HNL_mass_pi0_samples_names\n",
    "    if Params[\"Load_DetVars\"] == True: loc_pkls += \"DetVars/\"\n",
    "    elif Params[\"Load_Signal_DetVars\"] == True: loc_pkls += \"Signal_DetVars/\"\n",
    "    elif Params['Load_pi0_signal_DetVars'] == True: loc_pkls += \"Signal_DetVars/pi0/\"\n",
    "    \n",
    "    for sample in samples:\n",
    "        if (sample == \"overlay\") or (sample in Constants.HNL_ee_samples_names): start_str = loc_pkls + \"BDT_Test_dfs/Test_\"\n",
    "        elif (sample == \"beamoff\") and (Params[\"EXT_in_training\"] == True): start_str = loc_pkls + \"BDT_Test_dfs/Test_\"\n",
    "        elif (sample == \"dirtoverlay\") and (Params[\"dirt_in_training\"] == True): start_str = loc_pkls + \"BDT_Test_dfs/Test_\"\n",
    "        elif sample in Constants.HNL_mass_pi0_samples_names: start_str = loc_pkls + \"BDT_Test_dfs/pi0_selection/Test_\"\n",
    "        else: start_str = loc_pkls + \"Preselected_\"\n",
    "        \n",
    "        if (sample == \"beamoff\") and Params[\"EXT_in_training\"]==True:\n",
    "            sample_test_dict[sample] = pd.read_pickle(start_str+f\"{sample}_\"+Params[\"Run\"]+f\"_flattened{Test_filename}.pkl\")\n",
    "        elif (sample == \"dirtoverlay\") and Params[\"dirt_in_training\"]==True:\n",
    "            sample_test_dict[sample] = pd.read_pickle(start_str+f\"{sample}_\"+Params[\"Run\"]+f\"_flattened{Test_filename}.pkl\")\n",
    "        elif ((sample == \"overlay\") or (sample in sig_names_list)):\n",
    "            sample_test_dict[sample] = pd.read_pickle(start_str+f\"{sample}_\"+Params[\"Run\"]+f\"_flattened{Test_filename}.pkl\")\n",
    "        else:\n",
    "            sample_test_dict[sample] = pd.read_pickle(start_str+f\"{sample}_\"+Params[\"Run\"]+f\"_flattened{filename}.pkl\")\n",
    "    \n",
    "    return sample_test_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f117d8-87e9-4d73-9649-d2b278fcb712",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_dict = Load_preselected_pkls(samples, Params, loc_pkls, \"_full_Finished\", end_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30854c-0b46-43eb-806f-367c42e9a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_BDT_models(sample_dict, Params, BDT_name):\n",
    "    \"\"\"\n",
    "    Load variable list then loads BDT models for samples.\n",
    "    \"\"\"\n",
    "    if (Params[\"Load_lepton_signal\"] == True) or (Params[\"Load_lepton_dirac\"] == True): loc = \"bdts/\"\n",
    "    else: loc = \"bdts/pi0_selection/\"\n",
    "    \n",
    "    with open(loc+f\"input_vars/{BDT_name}_\"+Params[\"Run\"], \"rb\") as fp:   # Unpickling\n",
    "        saved_variables = pickle.load(fp)\n",
    "    \n",
    "    bdt_vars = saved_variables\n",
    "    xgb_test_dict = {}\n",
    "    \n",
    "    for sample in sample_dict:\n",
    "        xgb_test_dict[sample] = xgboost.DMatrix(sample_dict[sample][bdt_vars])\n",
    "    \n",
    "    if Params[\"Load_lepton_signal\"] == True: signal_names = Constants.HNL_ee_samples_names\n",
    "    elif Params[\"Load_pi0_signal\"] == True: signal_names = Constants.HNL_mass_pi0_samples_names\n",
    "    # elif Params[\"Load_lepton_dirac\"] == True: signal_names = Constants.HNL_ee_dirac_names\n",
    "    elif Params[\"Load_lepton_dirac\"] == True: signal_names = [\"10_ee\", \"100_ee\", \"150_ee\"] #Using Majorana models currently\n",
    "    # elif Params[\"Load_pi0_dirac\"] == True: signal_names = Constants.HNL_pi0_dirac_names\n",
    "    elif Params[\"Load_pi0_dirac\"] == True: signal_names = [\"150_pi0\", \"200_pi0\", \"245_pi0\"]\n",
    "    elif Params[\"Load_single_file\"] == True: signal_names = Params[\"single_file\"]\n",
    "    \n",
    "    for HNL_mass in signal_names:\n",
    "        # bdt = xgboost.Booster()\n",
    "        filename = loc+Params[\"Run\"]+f\"_{HNL_mass}{BDT_name}.pkl\"\n",
    "        bdt = pickle.load(open(filename, \"rb\"))\n",
    "        mass_val = int(HNL_mass.split(\"_\")[0])\n",
    "        for sample in xgb_test_dict:\n",
    "            results = bdt.predict(xgb_test_dict[sample])\n",
    "            sample_dict[sample][f\"BDT_output_{mass_val}MeV\"] = results\n",
    "\n",
    "    return sample_dict\n",
    "\n",
    "def OLD_Load_BDT_models(sample_dict, Params, BDT_name):\n",
    "    \"\"\"\n",
    "    Load variable list then loads BDT models for samples, using the older .json way of saving.\n",
    "    \"\"\"\n",
    "    if (Params[\"Load_lepton_signal\"] == True) or (Params[\"Load_lepton_dirac\"] == True): loc = \"bdts/\"\n",
    "    else: loc = \"bdts/pi0_selection/\"\n",
    "    \n",
    "    with open(loc+f\"input_vars/{BDT_name}_\"+Params[\"Run\"], \"rb\") as fp:   # Unpickling\n",
    "        saved_variables = pickle.load(fp)\n",
    "    \n",
    "    bdt_vars = saved_variables\n",
    "    xgb_test_dict = {}\n",
    "    \n",
    "    for sample in sample_dict:\n",
    "        xgb_test_dict[sample] = xgboost.DMatrix(sample_dict[sample][bdt_vars])\n",
    "    \n",
    "    if Params[\"Load_lepton_signal\"] == True: signal_names = Constants.HNL_ee_samples_names\n",
    "    elif Params[\"Load_pi0_signal\"] == True: signal_names = Constants.HNL_mass_pi0_samples_names\n",
    "    # elif Params[\"Load_lepton_dirac\"] == True: signal_names = Constants.HNL_ee_dirac_names\n",
    "    elif Params[\"Load_lepton_dirac\"] == True: signal_names = [\"10_ee\", \"100_ee\", \"150_ee\"] #Using Majorana models currently\n",
    "    # elif Params[\"Load_pi0_dirac\"] == True: signal_names = Constants.HNL_pi0_dirac_names\n",
    "    elif Params[\"Load_pi0_dirac\"] == True: signal_names = [\"150_pi0\", \"200_pi0\", \"245_pi0\"]\n",
    "    elif Params[\"Load_single_file\"] == True: signal_names = Params[\"single_file\"]\n",
    "    \n",
    "    for HNL_mass in signal_names:\n",
    "        bdt = xgboost.Booster()\n",
    "        bdt.load_model(loc+Params[\"Run\"]+f\"_{HNL_mass}{BDT_name}.json\")\n",
    "        mass_val = int(HNL_mass.split(\"_\")[0])\n",
    "        for sample in xgb_test_dict:\n",
    "            results = bdt.predict(xgb_test_dict[sample])\n",
    "            sample_dict[sample][f\"BDT_output_{mass_val}MeV\"] = results\n",
    "\n",
    "    return sample_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a1430-45be-4df0-a1e5-53713c21b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_dict = Load_BDT_models(sample_test_dict, Params, end_string)\n",
    "\n",
    "print(sample_test_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c40481-4ea8-4c77-879d-6024bdee383f",
   "metadata": {},
   "source": [
    "## Checking max BDT score (after logit transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd70bcba-723a-481c-896b-b37562c25864",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Constants)\n",
    "if Params[\"Load_lepton_signal\"] == True: \n",
    "    HNL_masses = Constants.HNL_mass_samples\n",
    "    sample_names = Constants.HNL_ee_samples_names\n",
    "    name_ends = \"_ee\"\n",
    "if Params[\"Load_pi0_signal\"] == True: \n",
    "    HNL_masses = Constants.HNL_mass_pi0_samples\n",
    "    sample_names = Constants.HNL_mass_pi0_samples_names\n",
    "    name_ends = \"_pi0\"\n",
    "if Params[\"Load_lepton_dirac\"] == True: \n",
    "    HNL_masses = Constants.HNL_ee_dirac_mass_samples\n",
    "    sample_names = Constants.HNL_ee_dirac_names\n",
    "    name_ends = \"_ee_dirac\"\n",
    "if Params[\"Load_pi0_dirac\"] == True: \n",
    "    HNL_masses = Constants.HNL_pi0_dirac_mass_samples\n",
    "    sample_names = Constants.HNL_pi0_dirac_names\n",
    "    name_ends = \"_pi0_dirac\"\n",
    "elif Params[\"Load_single_file\"] == True: HNL_masses = [Params[\"single_file\"]]\n",
    "\n",
    "max_scores, min_scores = [], []\n",
    "for HNL_mass in HNL_masses:\n",
    "    max_scores.append(max(Functions.logit(sample_test_dict[str(HNL_mass)+name_ends][f\"BDT_output_{HNL_mass}MeV\"])))\n",
    "    min_scores.append(min(Functions.logit(sample_test_dict[\"overlay\"][f\"BDT_output_{HNL_mass}MeV\"])))\n",
    "    \n",
    "max_all_scores = max(max_scores)\n",
    "min_all_scores = min(min_scores)\n",
    "print(\"Maximum score for all signal samples is \" + str(max_all_scores))\n",
    "print(\"Minimum score for all signal samples is \" + str(min_all_scores))                     \n",
    "\n",
    "max_score_int = np.ceil(max_all_scores)\n",
    "min_score_int = np.floor(min_all_scores)\n",
    "print(\"Maximum integer for bins is \" + str(max_score_int))\n",
    "print(\"Minimum integer for bins is \" + str(min_score_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d45934-f31a-4345-bfa9-3e02c47e8c47",
   "metadata": {},
   "source": [
    "## Looking at correlations of BDT scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8818101-e377-40cd-94c7-45628e06bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_test_dict['beamgood']\n",
    "\n",
    "ee_output_list = [\"BDT_output_10MeV\", \"BDT_output_20MeV\", \"BDT_output_50MeV\", \"BDT_output_100MeV\", \"BDT_output_150MeV\"]\n",
    "pi0_output_list = [\"BDT_output_150MeV\", \"BDT_output_180MeV\", \"BDT_output_200MeV\", \"BDT_output_220MeV\", \"BDT_output_240MeV\", \"BDT_output_245MeV\"]\n",
    "\n",
    "if Params[\"Load_lepton_signal\"]==True: score_list = ee_output_list\n",
    "if Params[\"Load_pi0_signal\"]==True: score_list = pi0_output_list\n",
    "\n",
    "#select only top events, e.g score >5 in one model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b512e400-34fa-4929-9ab4-c4f6c2a5b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "method = 'kendall'\n",
    "correlations = sample_test_dict['beamgood'][score_list].astype(np.float64).corr(method=method)\n",
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(correlations,vmin=-1,annot=False,square=True,cbar_kws={'label':method+' correlation'},cmap = 'RdBu_r')\n",
    "plt.title('Input Variable Correlations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca53cf2-ce20-4c12-93d9-25d94d364507",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc808a1-b889-4851-9cba-57d414cc983c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Testing Maj vs Dirac scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27235ba6-15c1-4307-a2aa-766406b9d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking which bins to merge Need to properly write this\n",
    "merged_bins_dict, bin_cents = {}, {}\n",
    "# min_score_cut = -5.0\n",
    "min_score_cut = min_score_int\n",
    "xlims = [min_score_cut,max_score_int]\n",
    "nbins = int(xlims[1]-xlims[0])\n",
    "min_overlay = 3 #The threshold of minimum overlay events required in a bin, used to be 1\n",
    "print(\"Initial nbins is \" + str(nbins))\n",
    "print(\"With range of \" + str(xlims))\n",
    "print(f\"Requiring more than {min_overlay} overlay events in each bin\")\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    bins_list = np.histogram(sample_test_dict['overlay'][f'BDT_output_{HNL_mass}MeV'],bins=nbins,range=xlims)[1]\n",
    "    if Params[\"Use_logit\"] == True:\n",
    "        bkg_scores=[Functions.logit(sample_test_dict['overlay'][f'BDT_output_{HNL_mass}MeV']),\n",
    "                    Functions.logit(sample_test_dict['dirtoverlay'][f'BDT_output_{HNL_mass}MeV']),\n",
    "                    Functions.logit(sample_test_dict['beamoff'][f'BDT_output_{HNL_mass}MeV'])]\n",
    "    totbkg=np.histogram(bkg_scores[0],bins=nbins,range=xlims)[0]+np.histogram(bkg_scores[1],bins=nbins,range=xlims)[0]+np.histogram(bkg_scores[2],bins=nbins,range=xlims)[0]\n",
    "    offbkg=np.histogram(bkg_scores[2],bins=nbins,range=xlims)[0]\n",
    "    overlaybkg=np.histogram(bkg_scores[0],bins=nbins,range=xlims)[0]\n",
    "    dirtbkg=np.histogram(bkg_scores[1],bins=nbins,range=xlims)[0]\n",
    "    bins_new=[]\n",
    "    for i,bin_bkg in enumerate(totbkg):\n",
    "        if(overlaybkg[i]>min_overlay): #Checking if unweighted overlay bkg has at least one event in the bin\n",
    "            bins_new.append(bins_list[i])\n",
    "\n",
    "    bins_new.append(bins_list[-1])\n",
    "    merged_bins_dict[HNL_mass] = bins_new\n",
    "    bin_cents[HNL_mass] = (np.array(bins_new[:-1])+np.array(bins_new[1:]))/2\n",
    "\n",
    "def make_overflow_bin(bins_dict, bins_cents_dict):\n",
    "    \"\"\"\n",
    "    For making the final \"overflow\" bin the same size as the previous bins, i.e one integer in width.\n",
    "    \"\"\"\n",
    "    bins_overflow, bins_cent_overflow = {}, {}\n",
    "    for HNL_mass in bins_dict:\n",
    "        overflow_bin = bins_cents_dict[HNL_mass][-2]+1 #Just adding one to the penultimate bin centre val. \n",
    "        bins_cent_overflow[HNL_mass] = bins_cents_dict[HNL_mass].copy()\n",
    "        bins_cent_overflow[HNL_mass][-1] = overflow_bin\n",
    "        bins_overflow[HNL_mass] = bins_dict[HNL_mass].copy()\n",
    "        bins_overflow[HNL_mass][-1] = bins_dict[HNL_mass][-2]+1 #Just adding one to the penultimate bin end val. \n",
    "    return bins_overflow, bins_cent_overflow\n",
    "\n",
    "bins_overflow, bins_cents_overflow = make_overflow_bin(merged_bins_dict, bin_cents)\n",
    "\n",
    "def make_xlims_dict(bins_dict, spacing, lower = None):\n",
    "    \"\"\"\n",
    "    Making a dict of xlims for plotting several mass points at once.\n",
    "    Also returns a dict of xticks for the purpose of indicating the overflow.\n",
    "    \"\"\"\n",
    "    xlims_adjusted, xticks_adjusted = {}, {}\n",
    "    vals_dict={}\n",
    "    for HNL_mass in bins_dict:\n",
    "        if isinstance(lower,(int, float)): lower_val = lower\n",
    "        else: lower_val = bins_dict[HNL_mass][0]\n",
    "        xlims_adjusted[HNL_mass] = [lower_val,bins_dict[HNL_mass][-1]]\n",
    "        ticks = np.arange(bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1], spacing)\n",
    "        if ticks[-1] != bins_dict[HNL_mass][-2]: ticks = np.append(ticks, bins_dict[HNL_mass][-1]-1)\n",
    "        ticks_strings = []\n",
    "        vals = []\n",
    "        for val in ticks:\n",
    "            ticks_strings.append(str(int(val)))\n",
    "            vals.append(val)\n",
    "        ticks_strings[-1] = str(ticks_strings[-1])+\"+\"\n",
    "        xticks_adjusted[HNL_mass] = ticks_strings\n",
    "        vals_dict[HNL_mass] = vals\n",
    "        \n",
    "    return xlims_adjusted, xticks_adjusted, vals_dict\n",
    "\n",
    "xlims_dict, xticks_dict, vals_dict = make_xlims_dict(bins_overflow, 2)\n",
    "\n",
    "xlims=[min_score_int,max_score_int]    \n",
    "BINS = int(xlims[1]-xlims[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92987a3-3142-4910-9a8d-89eb27be551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 150\n",
    "adjacent_mass = 245\n",
    "score_test = Functions.logit(sample_test_dict[str(HNL_mass)+name_ends][f\"BDT_output_{HNL_mass}MeV\"])\n",
    "score_test_adjacent = Functions.logit(sample_test_dict[str(HNL_mass)+name_ends][f\"BDT_output_{adjacent_mass}MeV\"])\n",
    "\n",
    "# bins_test = np.arange(min_score_int, max_score_int)\n",
    "bins_test = merged_bins_dict[HNL_mass]\n",
    "\n",
    "dir_hist = np.histogram(score_test, bins=bins_test)[0]\n",
    "# print(len(dir_hist))\n",
    "# print(len(bins_cents_overflow))\n",
    "\n",
    "plt.hist(bins_cents_overflow[HNL_mass], bins=bins_overflow[HNL_mass], weights = dir_hist, histtype=\"step\", lw=2, label=f\"{HNL_mass} Dirac {HNL_mass} model\")\n",
    "\n",
    "# plt.hist(score_test, bins=bins_test, histtype=\"step\", lw=2, label=f\"{HNL_mass} Dirac {HNL_mass} model\")\n",
    "# plt.hist(score_test_adjacent, bins=bins_test, histtype=\"step\", lw=2, label=f\"{HNL_mass} Dirac {adjacent_mass} model\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9929be5-37d7-4a47-a960-a51126f65bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Maj_Params = {\"Run\":\"run1\", #The run number, so far either \"run1\" or \"run3\"\n",
    "          \"Load_single_file\":False, #This will override everything else, put the desired file in the \"single_file\" line\n",
    "          \"single_file\":245,\n",
    "          \"Load_standard\":False, #bkgs\n",
    "          \"Load_lepton_signal\":False,\n",
    "          \"Load_lepton_dirac\":False,\n",
    "          \"Load_pi0_signal\":True,\n",
    "          \"Load_pi0_dirac\":False,\n",
    "          \"Load_DetVars\":False, #This is for overlay\n",
    "          \"Only_keep_common_DetVar_evs\":True,\n",
    "          \"Load_Signal_DetVars\":False, #Don't do here, but in seperate script\n",
    "          'Load_pi0_signal_DetVars':False, #Don't do here, but in seperate script\n",
    "          \"Load_data\":False,\n",
    "          \"FLATTEN\":True, #Have one row per reconstructed object in the analysis dataframe\n",
    "          \"only_presel\":False, #Create small files containing only variables necessary for pre-selection, for making pre-selection plots\n",
    "          \"EXT_in_training\":True,\n",
    "          \"dirt_in_training\":True,\n",
    "          \"Use_logit\":True,\n",
    "          \"nbins\":5} \n",
    "\n",
    "Maj_samples = Functions.create_test_samples_list(Maj_Params)\n",
    "\n",
    "Majorana_test_dict = Load_preselected_pkls(Maj_samples, Maj_Params, loc_pkls, \"_full_Finished\", end_string)\n",
    "\n",
    "Majorana_test_dict.keys()\n",
    "\n",
    "# Majorana_test_dict = Load_BDT_models(Majorana_test_dict, Maj_Params, end_string)\n",
    "Majorana_test_dict = OLD_Load_BDT_models(Majorana_test_dict, Maj_Params, \"_Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce710489-06ec-4120-a271-4e2a425c7723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Maj_ends = \"_pi0\"\n",
    "score_test = Functions.logit(Majorana_test_dict[str(HNL_mass)+Maj_ends][f\"BDT_output_{HNL_mass}MeV\"])\n",
    "score_test_adjacent = Functions.logit(Majorana_test_dict[str(HNL_mass)+Maj_ends][f\"BDT_output_{adjacent_mass}MeV\"])\n",
    "\n",
    "bins_test = np.arange(min_score_int, max_score_int)\n",
    "\n",
    "plt.hist(score_test, bins=bins_test, histtype=\"step\", lw=2, label=f\"{HNL_mass} Majorana {HNL_mass} model\")\n",
    "plt.hist(score_test_adjacent, bins=bins_test, histtype=\"step\", lw=2, label=f\"{HNL_mass} Majorana {adjacent_mass} model\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926018f8-0179-43af-87de-79f41d8942ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "BDT_name = end_string\n",
    "\n",
    "path_training_fracs = loc+f\"Training_fractions/{BDT_name}_\"+Params[\"Run\"]\n",
    "\n",
    "if(os.path.exists(path_training_fracs)):\n",
    "    with open(path_training_fracs, \"rb\") as fp:   # Unpickling\n",
    "        saved_train_fracs = pickle.load(fp)\n",
    "print(saved_train_fracs)\n",
    "\n",
    "SF_Maj = 1/(1-saved_train_fracs['signal'])\n",
    "\n",
    "HNL_mass = 150\n",
    "\n",
    "POT_ratio = Constants.run1_POT_scaling_dict[f\"{HNL_mass}_pi0\"]/Constants.run1_POT_scaling_dict[f\"{HNL_mass}_pi0_dirac\"]\n",
    "print(POT_ratio)\n",
    "Maj_scaling = POT_ratio/2.0\n",
    "\n",
    "Maj_weights = np.ones(len(Majorana_test_dict[str(HNL_mass)+Maj_ends][f\"BDT_output_{HNL_mass}MeV\"]))\n",
    "Maj_weights = Maj_weights*SF_Maj*Maj_scaling\n",
    "\n",
    "score_test_Dir = Functions.logit(sample_test_dict[str(HNL_mass)+name_ends][f\"BDT_output_{HNL_mass}MeV\"])\n",
    "score_test_adjacent_Dir = Functions.logit(sample_test_dict[str(HNL_mass)+name_ends][f\"BDT_output_{adjacent_mass}MeV\"])\n",
    "\n",
    "score_test_Maj = Functions.logit(Majorana_test_dict[str(HNL_mass)+Maj_ends][f\"BDT_output_{HNL_mass}MeV\"])\n",
    "score_test_adjacent_Maj = Functions.logit(Majorana_test_dict[str(HNL_mass)+Maj_ends][f\"BDT_output_{adjacent_mass}MeV\"])\n",
    "\n",
    "maj_hist = np.histogram(score_test_Maj, bins=merged_bins_dict[HNL_mass], weights=Maj_weights)[0]\n",
    "\n",
    "print(merged_bins_dict[HNL_mass])\n",
    "print(dir_hist)\n",
    "\n",
    "plt.figure(figsize=[10,8])\n",
    "\n",
    "plt.hist(bins_cents_overflow[HNL_mass], bins=bins_overflow[HNL_mass], weights = dir_hist, histtype=\"step\", lw=2, label=f\"{HNL_mass} Dirac {HNL_mass} model\")\n",
    "plt.hist(bins_cents_overflow[HNL_mass], bins=bins_overflow[HNL_mass], weights = maj_hist, histtype=\"step\", lw=2, label=f\"{HNL_mass} Majorana {HNL_mass} model\")\n",
    "\n",
    "# bins_test = np.arange(min_score_int, max_score_int)\n",
    "\n",
    "# plt.hist(score_test_Dir, bins=bins_test, histtype=\"step\", lw=2, label=f\"{HNL_mass} Dirac {HNL_mass} model\")\n",
    "# plt.hist(score_test_Maj, bins=bins_test, weights = Maj_weights, histtype=\"step\", lw=2, label=f\"{HNL_mass} Majorana {HNL_mass} model\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f93b37-bb23-4c7c-a7a9-11b5ff5eb6e3",
   "metadata": {},
   "source": [
    "## Merging bins with low stat bkg prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a3394-63e1-4a4e-9d60-247777d0429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking which bins to merge Need to properly write this\n",
    "merged_bins_dict, bin_cents = {}, {}\n",
    "# min_score_cut = -5.0\n",
    "min_score_cut = min_score_int\n",
    "xlims = [min_score_cut,max_score_int]\n",
    "nbins = int(xlims[1]-xlims[0])\n",
    "min_overlay = 3 #The threshold of minimum overlay events required in a bin, used to be 1\n",
    "print(\"Initial nbins is \" + str(nbins))\n",
    "print(\"With range of \" + str(xlims))\n",
    "print(f\"Requiring more than {min_overlay} overlay events in each bin\")\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    bins_list = np.histogram(sample_test_dict['overlay'][f'BDT_output_{HNL_mass}MeV'],bins=nbins,range=xlims)[1]\n",
    "    if Params[\"Use_logit\"] == True:\n",
    "        bkg_scores=[Functions.logit(sample_test_dict['overlay'][f'BDT_output_{HNL_mass}MeV']),\n",
    "                    Functions.logit(sample_test_dict['dirtoverlay'][f'BDT_output_{HNL_mass}MeV']),\n",
    "                    Functions.logit(sample_test_dict['beamoff'][f'BDT_output_{HNL_mass}MeV'])]\n",
    "    totbkg=np.histogram(bkg_scores[0],bins=nbins,range=xlims)[0]+np.histogram(bkg_scores[1],bins=nbins,range=xlims)[0]+np.histogram(bkg_scores[2],bins=nbins,range=xlims)[0]\n",
    "    offbkg=np.histogram(bkg_scores[2],bins=nbins,range=xlims)[0]\n",
    "    overlaybkg=np.histogram(bkg_scores[0],bins=nbins,range=xlims)[0]\n",
    "    dirtbkg=np.histogram(bkg_scores[1],bins=nbins,range=xlims)[0]\n",
    "    bins_new=[]\n",
    "    for i,bin_bkg in enumerate(totbkg):\n",
    "        if(overlaybkg[i]>min_overlay): #Checking if unweighted overlay bkg has at least one event in the bin\n",
    "            bins_new.append(bins_list[i])\n",
    "\n",
    "    bins_new.append(bins_list[-1])\n",
    "    merged_bins_dict[HNL_mass] = bins_new\n",
    "    bin_cents[HNL_mass] = (np.array(bins_new[:-1])+np.array(bins_new[1:]))/2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306655e8-a2bf-47ce-ab6c-6044b43baa4d",
   "metadata": {},
   "source": [
    "## Plotting BDT scores (no data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896b786-2bd2-4eb3-91bf-7e28f82bc70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to add dirac samples into plot dict here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642e40a-33f8-408d-8309-99cfd6ca868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BDT_name = end_string\n",
    "\n",
    "path_training_fracs = loc+f\"Training_fractions/{BDT_name}_\"+Params[\"Run\"]\n",
    "\n",
    "if(os.path.exists(path_training_fracs)):\n",
    "    with open(path_training_fracs, \"rb\") as fp:   # Unpickling\n",
    "        saved_train_fracs = pickle.load(fp)\n",
    "else:\n",
    "    print(\"No training fractions saved for this BDT model.\\nUsing defaults of overlay=0.7, signal=0.7, EXT=0.3 and dirt=0.0\")\n",
    "    saved_train_fracs = {\"overlay\":0.7, \"signal\":0.7, \"beamoff\":0.3, \"dirtoverlay\":0.0}\n",
    "    \n",
    "print(saved_train_fracs)\n",
    "    \n",
    "sample_norms = Functions.Get_weighted_sample_norms(Params, sample_test_dict, sample_names, saved_train_fracs[\"signal\"], saved_train_fracs[\"overlay\"], \n",
    "                                                   saved_train_fracs[\"beamoff\"], saved_train_fracs[\"dirtoverlay\"])\n",
    "\n",
    "if (Params[\"Load_lepton_dirac\"] == True) or (Params[\"Load_pi0_dirac\"]):\n",
    "    for sample in sample_names:\n",
    "        signal_scale_list = np.ones(len(sample_test_dict[sample]['n_pfps']))*1.0 #No test sample for Dirac, so no scale factor needed\n",
    "        sample_norms[sample]=signal_scale_list\n",
    "\n",
    "samples_plot={'overlay_test':sample_test_dict['overlay'],\n",
    "              'dirtoverlay':sample_test_dict['dirtoverlay'],\n",
    "              'beamoff':sample_test_dict['beamoff']}\n",
    "\n",
    "#Update with signal dfs too.\n",
    "for signal_name in sample_names:\n",
    "    samples_plot.update({signal_name:sample_test_dict[signal_name]})\n",
    "    \n",
    "print(samples_plot.keys())\n",
    "print(sample_norms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0292c75-a882-43de-b6a2-c476a4119582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overflow_bin(bins_dict, bins_cents_dict):\n",
    "    \"\"\"\n",
    "    For making the final \"overflow\" bin the same size as the previous bins, i.e one integer in width.\n",
    "    \"\"\"\n",
    "    bins_overflow, bins_cent_overflow = {}, {}\n",
    "    for HNL_mass in bins_dict:\n",
    "        overflow_bin = bins_cents_dict[HNL_mass][-2]+1 #Just adding one to the penultimate bin centre val. \n",
    "        bins_cent_overflow[HNL_mass] = bins_cents_dict[HNL_mass].copy()\n",
    "        bins_cent_overflow[HNL_mass][-1] = overflow_bin\n",
    "        bins_overflow[HNL_mass] = bins_dict[HNL_mass].copy()\n",
    "        bins_overflow[HNL_mass][-1] = bins_dict[HNL_mass][-2]+1 #Just adding one to the penultimate bin end val. \n",
    "    return bins_overflow, bins_cent_overflow\n",
    "\n",
    "bins_overflow, bins_cents_overflow = make_overflow_bin(merged_bins_dict, bin_cents)\n",
    "\n",
    "def make_xlims_dict(bins_dict, spacing, lower = None):\n",
    "    \"\"\"\n",
    "    Making a dict of xlims for plotting several mass points at once.\n",
    "    Also returns a dict of xticks for the purpose of indicating the overflow.\n",
    "    \"\"\"\n",
    "    xlims_adjusted, xticks_adjusted = {}, {}\n",
    "    vals_dict={}\n",
    "    for HNL_mass in bins_dict:\n",
    "        if isinstance(lower,(int, float)): lower_val = lower\n",
    "        else: lower_val = bins_dict[HNL_mass][0]\n",
    "        xlims_adjusted[HNL_mass] = [lower_val,bins_dict[HNL_mass][-1]]\n",
    "        ticks = np.arange(bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1], spacing)\n",
    "        if ticks[-1] != bins_dict[HNL_mass][-2]: ticks = np.append(ticks, bins_dict[HNL_mass][-1]-1)\n",
    "        ticks_strings = []\n",
    "        vals = []\n",
    "        for val in ticks:\n",
    "            ticks_strings.append(str(int(val)))\n",
    "            vals.append(val)\n",
    "        ticks_strings[-1] = str(ticks_strings[-1])+\"+\"\n",
    "        xticks_adjusted[HNL_mass] = ticks_strings\n",
    "        vals_dict[HNL_mass] = vals\n",
    "        \n",
    "    return xlims_adjusted, xticks_adjusted, vals_dict\n",
    "\n",
    "xlims_dict, xticks_dict, vals_dict = make_xlims_dict(bins_overflow, 2)\n",
    "\n",
    "xlims=[min_score_int,max_score_int]    \n",
    "BINS = int(xlims[1]-xlims[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4492b47-da87-4c5b-94de-fa853c3d39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_scale = 0.1\n",
    "\n",
    "for sig_name in sample_names:\n",
    "    HNL_mass = int(sig_name.split(\"_\")[0])\n",
    "    theta_original = Constants.theta_mu_4_dict[HNL_mass]\n",
    "    theta_squared = theta_original**2\n",
    "    POT_scaling = Constants.run1_POT_scaling_dict[sig_name]\n",
    "    plotted_scaling = HNL_scale\n",
    "    \n",
    "    theta_scaling_4 = POT_scaling/plotted_scaling \n",
    "    theta_scaling_2 = np.sqrt(theta_scaling_4)\n",
    "    \n",
    "    approx_theta_squared = theta_squared/theta_scaling_2\n",
    "    \n",
    "    print(sig_name)\n",
    "    print(approx_theta_squared)\n",
    "    \n",
    "xlims_dict = {}\n",
    "for HNL_mass in bins_overflow:\n",
    "    xlims_dict[HNL_mass] = [bins_overflow[HNL_mass][0]-1,bins_overflow[HNL_mass][-1]+1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94074ce9-38f0-4141-b6c1-0b9519ea508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(PT)   \n",
    "# HNL_scale = 0.1\n",
    "\n",
    "PT.Plot_BDT_output(HNL_masses=HNL_masses, signal_names=sample_names, samples=samples_plot, sample_norms=sample_norms, colours={}, xlims=xlims_dict,\n",
    "                   bins_cent_dict=bins_cents_overflow, bins_dict=bins_overflow, bins_original=merged_bins_dict,\n",
    "                   xticks=xticks_dict, xticks_vals=vals_dict,figsize=[12,8], MergeBins=True, density=False, legloc=\"best\",logy=False, savefig=False, \n",
    "                   save_str = \"_full_Finished_10\", Run=Params[\"Run\"], logit = Params[\"Use_logit\"],legsize=20, HNL_scale=HNL_scale)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70862c0a-eb3f-49db-bf68-d323d2ae6190",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving BDT output to .root files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d31dc-fc06-4287-81ce-719371104775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_one_hist(hist,name,nbins,xlims):\n",
    "    tData = ROOT.TH1F(name,name,len(nbins)-1,array(\"d\",nbins))\n",
    "    for i in range(len(nbins)-1):\n",
    "        tData.SetBinContent(i+1,hist['hist'][i])\n",
    "        tData.SetBinError(i+1,hist['err'][i])\n",
    "    return tData\n",
    "    \n",
    "def SaveToRoot_new(nbins,xlims,hist_samples,theta,fileName='test.root'): \n",
    "    rFile = ROOT.TFile(f'{fileName}','RECREATE')\n",
    "    tData = ROOT.TH1F(\"theta\",\"theta\",1,array(\"d\",[0,1]))\n",
    "    tData.SetBinContent(1,theta)\n",
    "    rFile.Write()\n",
    "    for i, name in enumerate(hist_samples):\n",
    "        tData = Save_one_hist(hist_samples[name],name,nbins,xlims)\n",
    "        rFile.Write()\n",
    "    rFile.Close()\n",
    "\n",
    "def make_stat_err_true(var, bins, weights_times_SF): #Saved in Functions.py\n",
    "    hist_unweighted = np.histogram(var,bins=bins)[0]\n",
    "    hist_weighted = np.histogram(var,bins=bins,weights=weights_times_SF)[0]\n",
    "    Total_SF = np.nan_to_num(hist_weighted/hist_unweighted)\n",
    "    stat_err = np.sqrt(hist_unweighted)*Total_SF\n",
    "    return stat_err\n",
    "\n",
    "def make_stat_err_new(var, bins, weights_times_SF): \n",
    "    hist_squared = np.histogram(var,bins=bins,weights=weights_times_SF**2)[0]\n",
    "    stat_err=np.sqrt(hist_squared)\n",
    "    return stat_err\n",
    "\n",
    "def SF_times_weights_sample_names(sample_dict, sample, HNL_mass, Run, SF_test, SF_signal):\n",
    "    if Run == \"run1\": POT_norm = Constants.run1_POT_scaling_dict\n",
    "    if Run == \"run3\": POT_norm = Constants.run3_POT_scaling_dict\n",
    "    \n",
    "    if (sample == 'overlay') or (sample == 'dirtoverlay'): SF = sample_dict[sample]['weight']*POT_norm[sample]*SF_test\n",
    "    elif sample == 'beamoff': SF = np.ones(len(sample_dict[sample][f'run']))*POT_norm[sample]*SF_test\n",
    "    elif sample =='beamgood': SF = np.ones(len(sample_dict[sample][f'run']))*POT_norm[sample]\n",
    "    else: \n",
    "        SF = np.ones(len(sample_dict[sample][f'BDT_output_{HNL_mass}MeV']))*(POT_norm[sample]*SF_test*SF_signal)\n",
    "    return SF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ebca0-be3e-4a28-97c9-86fb959899cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving BDT output for reweighting systematics plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee823212-6e51-4aaf-bc9d-dcb8b03a66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_results_loc = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/Results/\"\n",
    "if Params[\"Load_pi0_signal\"] == True: pkl_results_loc += \"pi0/\"\n",
    "\n",
    "save_name = end_string\n",
    "\n",
    "print_vals = input(\"Do you want to save the new BDT scores to .pkl files? y/n \")\n",
    "if print_vals == \"y\":\n",
    "    \n",
    "    columns = Variables.event_vars + Variables.weight_related + [\"weight\"]\n",
    "    for HNL_mass in merged_bins_dict:\n",
    "        columns.append(f'BDT_output_{HNL_mass}MeV')\n",
    "    print(columns)\n",
    "    \n",
    "    overlay_to_save = sample_test_dict['overlay'][columns].copy()\n",
    "    overlay_to_save.to_pickle(pkl_results_loc + f\"overlay_results{save_name}.pkl\")\n",
    "    \n",
    "    print(overlay_to_save.keys())\n",
    "    \n",
    "    print(pkl_results_loc + f\"overlay_results{save_name}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750fff37-4cbe-441f-9052-8f078c9c7659",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving to .root files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cdc2be-7561-4aa5-87b7-8ab3065f6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_SF_train = {}\n",
    "for sample in samples:\n",
    "    if sample in saved_train_fracs.keys(): All_SF_train[sample] = saved_train_fracs[sample]\n",
    "    elif sample in sample_names: All_SF_train[sample] = saved_train_fracs[\"signal\"]\n",
    "    else: All_SF_train[sample] = 0.0\n",
    "    \n",
    "if (Params[\"Load_lepton_dirac\"] == True) or (Params[\"Load_pi0_dirac\"]):\n",
    "    for sample in sample_names:\n",
    "        All_SF_train[sample] = 0.0\n",
    "    \n",
    "print(All_SF_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857c0409-eb43-4c0a-b71d-9202946dffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE_UP_dict = {2:5,10:5,20:200,50:50,100:10,150:5,180:4,200:4,220:4,240:4,245:4} #Doing this because the scale factor used in pyhf is bounded\n",
    "SCALE_UP_dict = {2:1.0,10:1.0,20:1.0,50:1.0,100:1.0,150:1.0, 180:1.0, 200:1.0, 220:1.0, 240:1.0, 245:1.0}\n",
    "\n",
    "new_theta_dict, dict_for_root = {}, {}\n",
    "\n",
    "bkg_samples = []\n",
    "for sample in samples:\n",
    "    if sample in sample_names: continue\n",
    "    else: bkg_samples.append(sample)\n",
    "\n",
    "name_type = Functions.Get_signal_name_type(Params)\n",
    "\n",
    "print(\"Saving minimum BDT score of \" + str(merged_bins_dict[150][0]))\n",
    "\n",
    "for i, HNL_mass in enumerate(HNL_masses):\n",
    "    print(HNL_mass)\n",
    "    sig_name = sample_names[i]\n",
    "    original_theta = Constants.theta_mu_4_dict[HNL_mass]\n",
    "    new_theta = original_theta*SCALE_UP_dict[HNL_mass]\n",
    "    new_theta_dict[HNL_mass] = original_theta*SCALE_UP_dict[HNL_mass]\n",
    "    SCALE_EVENTS = SCALE_UP_dict[HNL_mass]**4 #The Number of events is proportional to theta^4\n",
    "    \n",
    "    bins = merged_bins_dict[HNL_mass]\n",
    "    sample_list = bkg_samples + [sig_name]\n",
    "    \n",
    "    for sample in sample_list:\n",
    "        train_vs_test_fraction = All_SF_train[sample]\n",
    "        SF_test = 1.0/(1-train_vs_test_fraction)\n",
    "        \n",
    "        score = Functions.logit(sample_test_dict[sample][f'BDT_output_{HNL_mass}MeV'])\n",
    "        SF_list = SF_times_weights_sample_names(sample_test_dict, sample, HNL_mass, Params[\"Run\"], SF_test, SCALE_EVENTS)\n",
    "        stat_err = make_stat_err_true(score, bins, SF_list) #Should generally use this\n",
    "        # stat_err = make_stat_err_new(score, bins, SF_list) #Can use if I get a Nan or inf error\n",
    "        hist = np.histogram(score, bins=bins, weights=SF_list)\n",
    "        dict_for_root[sample] = {'bins': np.array(hist[1]), 'hist': np.array(hist[0]), 'err': np.array(stat_err)}\n",
    "        \n",
    "    hist_samples = {\"bkg_overlay\":dict_for_root['overlay'],\"bkg_dirt\":dict_for_root['dirtoverlay'],\n",
    "                    \"bkg_EXT\":dict_for_root['beamoff'],\"signal\":dict_for_root[sample]}\n",
    "    if 'beamgood' in bkg_samples:\n",
    "        hist_samples.update({\"data\":dict_for_root['beamgood']})\n",
    "        \n",
    "    save_name = Params[\"Run\"]+f\"_{HNL_mass}_{name_type}{end_string}.root\"\n",
    "    \n",
    "    if (Params[\"Load_pi0_signal\"]==True) or (Params[\"Load_pi0_dirac\"]==True):\n",
    "        SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"bdt_output/pi0/\"+save_name)\n",
    "        SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"Uncertainties/pi0/\"+save_name)\n",
    "    else:\n",
    "        SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"bdt_output/\"+save_name)\n",
    "        SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"Uncertainties/\"+save_name)\n",
    "        \n",
    "print(\"Saved all\")\n",
    "print(f\"Saved with names like {save_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9a478-704c-4b02-b0e3-a06763631a98",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculating \"efficiency\" of BDT separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44822b-e324-4216-b3d7-e4967c8c4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_efficiency(df, file_name, Run, BDT_cutoff, HNL_mass, train_fracs, debug=False):\n",
    "    if Run == \"run1\": ev_num_dict = Constants.run1_event_numbers\n",
    "    if Run == \"run3\": ev_num_dict = Constants.run3_event_numbers\n",
    "    \n",
    "    train_vs_test_fraction = train_fracs[file_name] #Should be saved in in script 3.0\n",
    "    norm = 1.0/(1-train_vs_test_fraction) #Need to scale up to account for events lost\n",
    "    \n",
    "    total_ev_num = len(df)\n",
    "    ev_num_initial = ev_num_dict[file_name]\n",
    "    cut_df = df.query(f\"BDT_output_{HNL_mass}MeV > {BDT_cutoff}\")\n",
    "    new_ev_num = len(cut_df)\n",
    "    \n",
    "    BDT_efficiency = new_ev_num/total_ev_num\n",
    "    Total_efficiency = norm*(new_ev_num/ev_num_initial)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Total events is {total_ev_num}\")\n",
    "        print(f\"Events after cut is {new_ev_num}\")   \n",
    "    \n",
    "    return BDT_efficiency, Total_efficiency\n",
    "\n",
    "def calculate_ev_numbers(df, file_name, Run, BDT_cutoff, HNL_mass, train_fracs, debug=False):\n",
    "    if Run == \"run1\": \n",
    "        ev_num_dict = Constants.run1_event_numbers\n",
    "        POT_scaling_dict = Constants.run1_POT_scaling_dict\n",
    "    if Run == \"run3\": \n",
    "        ev_num_dict = Constants.run3_event_numbers\n",
    "        POT_scaling_dict = Constants.run3_POT_scaling_dict\n",
    "        \n",
    "    train_vs_test_fraction = train_fracs[file_name] #Should be saved in in script 3.0\n",
    "    norm = 1.0/(1-train_vs_test_fraction) #Need to scale up to account for events lost\n",
    "        \n",
    "    cut_df = df.query(f\"BDT_output_{HNL_mass}MeV > {BDT_cutoff}\")\n",
    "    \n",
    "    if file_name == \"overlay\" or file_name == \"dirtoverlay\":\n",
    "        sum_weights = cut_df[\"weight\"].sum()\n",
    "    else: sum_weights = len(cut_df)\n",
    "    \n",
    "    Number_events = sum_weights*POT_scaling_dict[file_name]*norm\n",
    "    \n",
    "    return Number_events\n",
    "    \n",
    "\n",
    "def sum_total_weights(df): #This is to check that the weighting doesn't affect the overall efficiency significantly (i.e av weight is approx 1)\n",
    "    total_evs = len(df)\n",
    "    sum_weights = df[\"weight\"].sum()\n",
    "    \n",
    "    weight_per_ev = sum_weights/total_evs\n",
    "    \n",
    "    print(f\"Sum of weights is {sum_weights}\")\n",
    "    print(f\"Average weight is {weight_per_ev}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3134f-a034-452a-83bf-09f98a074793",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_test_dict.keys())\n",
    "# BDT_name = end_string\n",
    "BDT_name = \"_full_Finished_10\"\n",
    "\n",
    "if Params[\"Load_lepton_signal\"]==True: loc = \"bdts/\"\n",
    "if Params[\"Load_pi0_signal\"]==True: loc = \"bdts/pi0_selection/\"\n",
    "\n",
    "with open(loc+f\"Training_fractions/{BDT_name}_\"+Params[\"Run\"], \"rb\") as fp:   # Unpickling\n",
    "    saved_train_fracs = pickle.load(fp)\n",
    "    \n",
    "# for sample in ['overlay', 'dirtoverlay', 'beamoff']: #sample_test_dict\n",
    "for sample in sample_test_dict:\n",
    "    if sample in Constants.HNL_ee_samples_names + Constants.HNL_mass_pi0_samples_names: \n",
    "        saved_train_fracs[sample] = saved_train_fracs[\"signal\"]\n",
    "    elif sample not in saved_train_fracs.keys(): saved_train_fracs[sample] = 0.0\n",
    "\n",
    "print(saved_train_fracs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de30022-b330-406a-89e7-356b9a8a4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_bins = 3\n",
    "\n",
    "BDT_cut = {}\n",
    "\n",
    "for sample in sample_names:\n",
    "    HNL_mass = int(sample.split(\"_\")[0])\n",
    "    BDT_cut[sample] = Functions.invlogit(merged_bins_dict[HNL_mass][-1*Number_of_bins])\n",
    "    \n",
    "if name_type == \"ee\": standard_BDT_cut = BDT_cut[\"100_ee\"]\n",
    "if name_type == \"pi0\": standard_BDT_cut = BDT_cut[\"200_pi0\"]\n",
    "print(BDT_cut)\n",
    "print(standard_BDT_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59979a5c-4225-479a-8a81-3a762dd82a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logit_BDT_cut = Functions.logit(standard_BDT_cut)\n",
    "print(f\"Using a logit BDT cut of {logit_BDT_cut}, which corresponds to {standard_BDT_cut} in [0,1] BDT score.\")\n",
    "\n",
    "BDT_efficiency_dict = {}\n",
    "Total_efficiency_dict = {}\n",
    "Scaled_events_dict = {}\n",
    "\n",
    "for file in sample_names: #Just signal samples\n",
    "# for file in sample_test_dict:\n",
    "    HNL_mass_number = file.split(\"_\")[0]\n",
    "    print(file)\n",
    "    BDT_efficiency_dict[file], Total_efficiency_dict[file] = calculate_efficiency(sample_test_dict[file], file, Params[\"Run\"], BDT_cut[file], HNL_mass_number,\n",
    "                                                 saved_train_fracs, debug=True)\n",
    "    Scaled_events_dict[file] = calculate_ev_numbers(sample_test_dict[file], file, Params[\"Run\"], BDT_cut[file], HNL_mass_number,\n",
    "                                                 saved_train_fracs, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebfe02-3497-4c7e-beec-b8419bd06b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if name_type == \"ee\": bkg_mass_model = \"100_ee\"\n",
    "if name_type == \"pi0\": bkg_mass_model = \"200_pi0\"\n",
    "\n",
    "for file in ['overlay', 'dirtoverlay', 'beamoff']: #Calculating efficiency for 100MeV BDT model score\n",
    "    print(file)\n",
    "    bkg_mass = int(bkg_mass_model.split(\"_\")[0])\n",
    "    BDT_efficiency_dict[file], Total_efficiency_dict[file] = calculate_efficiency(sample_test_dict[file], file, Params[\"Run\"], standard_BDT_cut, bkg_mass, \n",
    "                                                 saved_train_fracs, debug=True)\n",
    "    Scaled_events_dict[file] = calculate_ev_numbers(sample_test_dict[file], file, Params[\"Run\"], standard_BDT_cut, bkg_mass,\n",
    "                                                 saved_train_fracs, debug=True)\n",
    "    \n",
    "Total_predicted_presel = calculate_ev_numbers(sample_test_dict[\"overlay\"], \"overlay\", Params[\"Run\"], 0.0, HNL_mass, saved_train_fracs)\n",
    "Total_predicted_presel += calculate_ev_numbers(sample_test_dict[\"dirtoverlay\"], \"dirtoverlay\", Params[\"Run\"], 0.0, HNL_mass, saved_train_fracs)\n",
    "Total_predicted_presel += calculate_ev_numbers(sample_test_dict[\"beamoff\"], \"beamoff\", Params[\"Run\"], 0.0, HNL_mass, saved_train_fracs)\n",
    "\n",
    "print(f\"Total presel events is {Total_predicted_presel}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d484621-3f26-4510-9c8b-37a7dc0d643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Params[\"Run\"] + \" efficiencies are:\")\n",
    "for file in sample_test_dict:\n",
    "    if file==\"beamgood\": continue\n",
    "    # print(f\"{file} efficiency is \" + str(efficiency_dict[file]))\n",
    "    print(f\"{file} BDT efficiency is \" + str(BDT_efficiency_dict[file]*100) + \" %\")\n",
    "    print(f\"{file} Total efficiency is \" + str(Total_efficiency_dict[file]*100) + \" %\")\n",
    "    \n",
    "    print(f\"{file} scaled number of events is \" + str(Scaled_events_dict[file]))\n",
    "    \n",
    "predicted_sum = Scaled_events_dict[\"overlay\"] + Scaled_events_dict[\"dirtoverlay\"] + Scaled_events_dict[\"beamoff\"]\n",
    "print(\"----------------------------------\")\n",
    "print(f\"Total predicted bkg events is {predicted_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16600aaa-fe40-42fc-8a62-d7e1a9d591f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Total_ev_numbers = {}\n",
    "if Params[\"Run\"] == \"run1\":\n",
    "    Presel_effic = {\"beamoff\":0.007, \"overlay\":0.049, \"dirtoverlay\":0.012, \"combined\":0.012}\n",
    "    Ev_nums_dict = Constants.run1_event_numbers\n",
    "if Params[\"Run\"] == \"run3\":\n",
    "    Presel_effic = {\"beamoff\":0.004, \"overlay\":0.038, \"dirtoverlay\":0.008, \"combined\":0.009}\n",
    "    Ev_nums_dict = Constants.run3_event_numbers\n",
    "\n",
    "for file in [\"beamoff\", \"overlay\", \"dirtoverlay\"]:\n",
    "    \n",
    "    total_effic = BDT_efficiency_dict[file]*Presel_effic[file]\n",
    "    print(file + \": \" + str(total_effic))\n",
    "    \n",
    "print(\"------------------------\")\n",
    "sum_predictions_eff = predicted_sum/Total_predicted_presel\n",
    "print(f\"sum prediction BDT eff: {sum_predictions_eff*100}\")\n",
    "sum_predictions_total_eff = sum_predictions_eff*Presel_effic[\"combined\"]\n",
    "print(f\"sum prediction Total eff: {sum_predictions_total_eff*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46e5f7-ea37-4d7a-9ab0-818012d23026",
   "metadata": {
    "tags": []
   },
   "source": [
    "## \"Closure test\" i.e testing adjacent model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2b29a-74bf-4a4a-baed-41ed669d1360",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"10_ee\"\n",
    "sample_mass = int(sample.split(\"_\")[0])\n",
    "model = 20\n",
    "SCALE_UP_dict = {2:1.0,10:1.0,20:1.0,50:1.0,100:1.0,150:1.0, 180:1.0, 200:1.0, 220:1.0, 240:1.0, 245:1.0}\n",
    "\n",
    "train_vs_test_fraction = All_SF_train[sample]\n",
    "SF_test = 1.0/(1-train_vs_test_fraction)\n",
    "\n",
    "# bins = merged_bins_dict[sample_mass] #Should these bins be used?\n",
    "bins = merged_bins_dict[model]\n",
    "\n",
    "score = Functions.logit(sample_test_dict[sample][f'BDT_output_{model}MeV'])\n",
    "SF_list = SF_times_weights_sample_names(sample_test_dict, sample, sample_mass, Params[\"Run\"], SF_test, SCALE_UP_dict[sample_mass])\n",
    "stat_err = make_stat_err_true(score, bins, SF_list) #Should generatlly use this\n",
    "# stat_err = make_stat_err_new(score, bins, SF_list)\n",
    "hist, bin_edges = np.histogram(score, bins=bins, weights=SF_list)\n",
    "\n",
    "print(hist)\n",
    "print(bin_edges)\n",
    "print(bins)\n",
    "\n",
    "\n",
    "# cents = bin_cents[sample_mass]\n",
    "cents = bin_cents[model]\n",
    "\n",
    "plot = plt.hist(cents, bins=bins, weights=hist, histtype=\"step\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7f3d3-d021-442a-b8b3-efee1d8fb74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"10_ee\"\n",
    "sample_mass = int(sample.split(\"_\")[0])\n",
    "model = 20\n",
    "\n",
    "# sample_test_dict[sample][f'BDT_output_{model}MeV']\n",
    "bins = merged_bins_dict[sample_mass]\n",
    "\n",
    "plt.hist(Functions.logit(sample_test_dict[sample][f'BDT_output_{model}MeV']), bins=bins, histtype=\"step\", label=f\"{sample}_{model}\")\n",
    "plt.hist(Functions.logit(sample_test_dict[sample][f'BDT_output_{sample_mass}MeV']), bins=bins, histtype=\"step\", label=f\"{sample}_{sample_mass}\")\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe1f99-000e-4c78-b394-ee4eedc26202",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"20_ee\"\n",
    "sample_mass = int(sample.split(\"_\")[0])\n",
    "model = 10\n",
    "\n",
    "# sample_test_dict[sample][f'BDT_output_{model}MeV']\n",
    "bins = merged_bins_dict[sample_mass]\n",
    "bins_model = merged_bins_dict[model]\n",
    "\n",
    "# plt.hist(Functions.logit(sample_test_dict[sample][f'BDT_output_{model}MeV']), bins=bins, histtype=\"step\", label=f\"{sample}_{model}\")\n",
    "plt.hist(Functions.logit(sample_test_dict[sample][f'BDT_output_{model}MeV']), bins=bins_model, histtype=\"step\", label=f\"{sample}_{model}\")\n",
    "# plt.hist(Functions.logit(sample_test_dict[sample][f'BDT_output_{sample_mass}MeV']), bins=bins, histtype=\"step\", label=f\"{sample}_{sample_mass}\")\n",
    "plt.hist(Functions.logit(sample_test_dict[sample][f'BDT_output_{sample_mass}MeV']), bins=bins, histtype=\"step\", label=f\"{sample}_{sample_mass}\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ba96b-4609-474b-9a9c-6d6935a64080",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_theta_dict, dict_for_root = {}, {}\n",
    "\n",
    "SCALE_UP_dict = {2:1.0,10:1.0,20:1.0,50:1.0,100:1.0,150:1.0, 180:1.0, 200:1.0, 220:1.0, 240:1.0, 245:1.0}\n",
    "\n",
    "bkg_samples = []\n",
    "for sample in samples:\n",
    "    if sample in sample_names: continue\n",
    "    else: bkg_samples.append(sample)\n",
    "\n",
    "name_type = Functions.Get_signal_name_type(Params)\n",
    "\n",
    "print(\"Saving minimum BDT score of \" + str(merged_bins_dict[150][0]))\n",
    "\n",
    "mass_list = list(merged_bins_dict.keys())\n",
    "\n",
    "for i, test_sig in enumerate(sample_names):\n",
    "    print(test_sig)\n",
    "    HNL_mass = int(test_sig.split(\"_\")[0])\n",
    "    original_theta = Constants.theta_mu_4_dict[HNL_mass]\n",
    "    new_theta = original_theta*SCALE_UP_dict[HNL_mass]\n",
    "    new_theta_dict[HNL_mass] = original_theta*SCALE_UP_dict[HNL_mass]\n",
    "    SCALE_EVENTS = SCALE_UP_dict[HNL_mass]**4 #The Number of events is proportional to theta^4\n",
    "    \n",
    "    # bins = merged_bins_dict[HNL_mass] #Should these bins be used?\n",
    "\n",
    "    sample_list = bkg_samples + [test_sig]\n",
    "\n",
    "    model_masses = [] #for masses either side of test point (if available)\n",
    "    \n",
    "    if i > 0: model_masses.append(mass_list[i-1])\n",
    "    if i < len(sample_names)-1: model_masses.append(mass_list[i+1])\n",
    "    \n",
    "    for model in model_masses: #Looping over adjacent models\n",
    "        bins = merged_bins_dict[model]\n",
    "        for sample in sample_list:\n",
    "            train_vs_test_fraction = All_SF_train[sample]\n",
    "            SF_test = 1.0/(1-train_vs_test_fraction)\n",
    "\n",
    "            score = Functions.logit(sample_test_dict[sample][f'BDT_output_{model}MeV'])\n",
    "            SF_list = SF_times_weights_sample_names(sample_test_dict, sample, HNL_mass, Params[\"Run\"], SF_test, SCALE_EVENTS)\n",
    "            stat_err = make_stat_err_true(score, bins, SF_list) #Should generally use this\n",
    "            # stat_err = make_stat_err_new(score, bins, SF_list)\n",
    "            hist, bin_edges = np.histogram(score, bins=bins, weights=SF_list)\n",
    "            # dict_for_root[sample] = {'bins': np.array(hist[1]), 'hist': np.array(hist[0]), 'err': np.array(stat_err)}\n",
    "            dict_for_root[sample] = {'bins': bin_edges, 'hist': hist, 'err': stat_err}\n",
    "\n",
    "        hist_samples = {\"bkg_overlay\":dict_for_root['overlay'],\"bkg_dirt\":dict_for_root['dirtoverlay'],\n",
    "                        \"bkg_EXT\":dict_for_root['beamoff'],\"signal\":dict_for_root[sample]}\n",
    "        if 'beamgood' in bkg_samples:\n",
    "            hist_samples.update({\"data\":dict_for_root['beamgood']})\n",
    "\n",
    "        save_name = Params[\"Run\"]+f\"_corrected_Test_{HNL_mass}_{name_type}_model_{model}{end_string}.root\"\n",
    "\n",
    "        if (Params[\"Load_pi0_signal\"]==True) or (Params[\"Load_pi0_dirac\"]==True):\n",
    "            # SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"bdt_output/pi0/adjacent_models/\"+save_name)\n",
    "            # SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"Uncertainties/pi0/adjacent_models/\"+save_name)\n",
    "            SaveToRoot_new(merged_bins_dict[model],xlims,hist_samples,theta=new_theta,fileName=\"bdt_output/pi0/adjacent_models/\"+save_name)\n",
    "            SaveToRoot_new(merged_bins_dict[model],xlims,hist_samples,theta=new_theta,fileName=\"Uncertainties/pi0/adjacent_models/\"+save_name)\n",
    "        else:\n",
    "            # SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"bdt_output/adjacent_models/\"+save_name)\n",
    "            # SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"Uncertainties/adjacent_models/\"+save_name)\n",
    "            SaveToRoot_new(merged_bins_dict[model],xlims,hist_samples,theta=new_theta,fileName=\"bdt_output/adjacent_models/\"+save_name)\n",
    "            SaveToRoot_new(merged_bins_dict[model],xlims,hist_samples,theta=new_theta,fileName=\"Uncertainties/adjacent_models/\"+save_name)\n",
    "        \n",
    "print(\"Saved all\")\n",
    "print(f\"Saved with names like {save_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b637f-ce6a-4900-b20e-c6a82035e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass_point = 50\n",
    "mass_point = 240\n",
    "\n",
    "savefig = False\n",
    "\n",
    "test_models = [180, 245]\n",
    "\n",
    "test_results, test_labels = [], []\n",
    "\n",
    "plt.figure(figsize=[12,8],facecolor='white')\n",
    "\n",
    "for mass_model in test_models:\n",
    "    test_results.append(Functions.logit(samples_plot[mass_point][f'BDT_output_{mass_model}MeV']))\n",
    "    test_labels.append(f\"{mass_model} MeV model\")\n",
    "\n",
    "plot=plt.hist(Functions.logit(samples_plot[mass_point][f'BDT_output_{mass_point}MeV']),label=f\"Correct {mass_point} MeV model\",\n",
    "                         range=xlims,bins=merged_bins_dict[mass_point],histtype=\"step\",\n",
    "                         stacked=False,density=False,linewidth=4)\n",
    "plot=plt.hist(test_results,label=test_labels,range=xlims,bins=merged_bins_dict[mass_point],histtype=\"step\",\n",
    "              stacked=False,density=False,linewidth=2)\n",
    "\n",
    "plt.xlabel(f\"BDT score for {mass_point} MeV sample\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "if savefig == True:\n",
    "    plt.savefig(\"plots/BDT_output/Adjacent_models_tests/\" + Params[\"Run\"] + \"_\" + str(mass_point) + \"MeV_adjcent_models.pdf\")\n",
    "    plt.savefig(\"plots/BDT_output/Adjacent_models_tests/\" + Params[\"Run\"] + \"_\" + str(mass_point) + \"MeV_adjcent_models.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a07a5-acf6-4e6c-9ca0-ff5c930fabd1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Looking at variables for signal-like events vs. full sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695497be-be6d-45c3-9125-351686a4dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.95\n",
    "signal_like = {}\n",
    "frac_retained = {}\n",
    "pkl_variable_tests_loc = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/Variable_tests/\"\n",
    "\n",
    "Number_signal = 1000\n",
    "top_signal = {}\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for signal_sample in sample_names:\n",
    "    HNL_mass = int(signal_sample.split(\"_\")[0])\n",
    "    signal_like[signal_sample] = sample_test_dict[signal_sample].query(f\"BDT_output_{HNL_mass}MeV > {cutoff}\")\n",
    "    frac_retained[signal_sample] = len(signal_like[signal_sample])/len(sample_test_dict[signal_sample])\n",
    "    \n",
    "    # signal_like[HNL_mass].to_pickle(pkl_variable_tests_loc + f\"signal_like_{HNL_mass}MeV.pkl\")\n",
    "    \n",
    "    top_signal[signal_sample] = sample_test_dict[signal_sample].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(Number_signal)\n",
    "    \n",
    "print(frac_retained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b417309-8164-4f36-97b9-d1f6c6da1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,8],facecolor='white')\n",
    "\n",
    "if name_type == \"ee\": HNL_mass = \"100_ee\"\n",
    "if name_type == \"pi0\": HNL_mass = \"200_pi0\"\n",
    "\n",
    "variable = 'shr_theta_v'\n",
    "#'shr_theta_v', 'shr_phi_v', 'trk_theta_v', 'trk_phi_v'\n",
    "\n",
    "bins = 40\n",
    "xlims = [0,3.2]\n",
    "\n",
    "plt.hist(sample_test_dict[HNL_mass][variable],\n",
    "#               label=[f\"HNL ({mass} MeV) \\n $|U_{{\\mu4}}|^2=\"+sci_notation(sample_info[\"300\"][\"theta_u2\"]) +f\" (x{HNLplotscale})\"],\n",
    "         label=[f\"All preselected {HNL_mass} MeV HNL\"],\n",
    "         range=xlims,bins=bins,\n",
    "         stacked=False,density=True,\n",
    "         histtype=\"step\",lw=3)\n",
    "\n",
    "plt.hist(top_signal[HNL_mass][variable],\n",
    "#               label=[f\"HNL ({mass} MeV) \\n $|U_{{\\mu4}}|^2=\"+sci_notation(sample_info[\"300\"][\"theta_u2\"]) +f\" (x{HNLplotscale})\"],\n",
    "         label=[f\"Signal-like {HNL_mass} MeV HNL\"],\n",
    "         range=xlims,bins=bins,\n",
    "         stacked=False,density=True,\n",
    "         histtype=\"step\",lw=3)\n",
    "plt.xlabel(variable)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25909893-a728-4a26-b351-cb43c089e251",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Make list of run subrun event which are most signal-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a7a88-8e83-4b3f-87d8-97f5cc9c8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 50\n",
    "csv_location = \"bdt_output/signal_like_rse_lists/\"\n",
    "for signal_sample in sample_names:\n",
    "    HNL_mass = int(signal_sample.split(\"_\")[0])\n",
    "    signal_sorted = sample_test_dict[signal_sample].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "    overlay_sorted = sample_test_dict['overlay'].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "    dirt_sorted = sample_test_dict['dirtoverlay'].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "    beamoff_sorted = sample_test_dict['beamoff'].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "    beamgood_sorted = sample_test_dict['beamgood'].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "    \n",
    "    signal_run_sub_event = signal_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "    overlay_run_sub_event = overlay_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "    dirt_run_sub_event = dirt_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "    beamoff_run_sub_event = beamoff_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "    beamgood_run_sub_event = beamgood_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "    \n",
    "    csv_name = csv_location + Params[\"Run\"] + f\"_signal_{signal_sample}.csv\"\n",
    "    csv_overlay_name = csv_location + Params[\"Run\"] + f\"_overlay_{signal_sample}.csv\"\n",
    "    csv_dirt_name = csv_location + Params[\"Run\"] + f\"_dirtoverlay_{signal_sample}.csv\"\n",
    "    csv_beamoff_name = csv_location + Params[\"Run\"] + f\"_beamoff_{signal_sample}.csv\"\n",
    "    csv_beamgood_name = csv_location + Params[\"Run\"] + f\"_beamgood_{signal_sample}.csv\"\n",
    "\n",
    "    signal_run_sub_event.to_csv(csv_name, sep=\" \", header=False, index=False) #i.e separating by a space, removing column names\n",
    "    overlay_run_sub_event.to_csv(csv_overlay_name, sep=\" \", header=False, index=False)\n",
    "    dirt_run_sub_event.to_csv(csv_dirt_name, sep=\" \", header=False, index=False)\n",
    "    beamoff_run_sub_event.to_csv(csv_beamoff_name, sep=\" \", header=False, index=False)\n",
    "    beamgood_run_sub_event.to_csv(csv_beamgood_name, sep=\" \", header=False, index=False)\n",
    "    \n",
    "    #signal\n",
    "    csv_file = csv_name\n",
    "    txt_file = csv_location + Params[\"Run\"] + f\"_signal_{signal_sample}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "    with open(txt_file, \"w\") as my_output_file:\n",
    "        with open(csv_file, \"r\") as my_input_file:\n",
    "            [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "        my_output_file.close()\n",
    "        \n",
    "    #overlay\n",
    "    csv_file = csv_overlay_name\n",
    "    txt_file = csv_location + Params[\"Run\"] + f\"_overlay_{signal_sample}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "    with open(txt_file, \"w\") as my_output_file:\n",
    "        with open(csv_file, \"r\") as my_input_file:\n",
    "            [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "        my_output_file.close()\n",
    "        \n",
    "    #dirt\n",
    "    csv_file = csv_dirt_name\n",
    "    txt_file = csv_location + Params[\"Run\"] + f\"_dirtoverlay_{signal_sample}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "    with open(txt_file, \"w\") as my_output_file:\n",
    "        with open(csv_file, \"r\") as my_input_file:\n",
    "            [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "        my_output_file.close()\n",
    "        \n",
    "    #beamoff\n",
    "    csv_file = csv_beamoff_name\n",
    "    txt_file = csv_location + Params[\"Run\"] + f\"_beamoff_{signal_sample}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "    with open(txt_file, \"w\") as my_output_file:\n",
    "        with open(csv_file, \"r\") as my_input_file:\n",
    "            [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "        my_output_file.close()\n",
    "        \n",
    "    #beamgood\n",
    "    csv_file = csv_beamgood_name\n",
    "    txt_file = csv_location + Params[\"Run\"] + f\"_beamgood_{signal_sample}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "    with open(txt_file, \"w\") as my_output_file:\n",
    "        with open(csv_file, \"r\") as my_input_file:\n",
    "            [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "        my_output_file.close()\n",
    "        \n",
    "print(\"Saved all!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275803f-0b98-43c4-9871-b8f16419b88f",
   "metadata": {},
   "source": [
    "# End of code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
