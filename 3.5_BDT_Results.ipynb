{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab81f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.28/00\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "import os, sys, string, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from platform import python_version\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import math\n",
    "from matplotlib.patches import Rectangle\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from importlib import reload\n",
    "import csv\n",
    "import ROOT\n",
    "from array import array\n",
    "\n",
    "import Utilities.Plotter as PT\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Variables_list as Variables\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print ('Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a3504-392d-4b23-a225-0db439fcb0f5",
   "metadata": {},
   "source": [
    "## Reading in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de87920a-3a93-4217-93ab-1963f9066403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading these run3 samples: \n",
      "['overlay', 'dirtoverlay', 'beamoff', '150_pi0', '180_pi0', '200_pi0', '220_pi0', '240_pi0', '245_pi0', 'beamgood']\n"
     ]
    }
   ],
   "source": [
    "Params = {\"Run\":\"run1\", #The run number, so far either \"run1\" or \"run3\"\n",
    "          \"Load_single_file\":False, #This will override everything else, put the desired file in the \"single_file\" line\n",
    "          \"single_file\":245,\n",
    "          \"Load_standard\":True, #bkgs\n",
    "          \"Load_lepton_signal\":False,\n",
    "          \"Load_lepton_dirac\":False,\n",
    "          \"Load_pi0_signal\":True,\n",
    "          \"Load_pi0_dirac\":False,\n",
    "          \"Load_DetVars\":False, #This is for overlay\n",
    "          \"Only_keep_common_DetVar_evs\":True,\n",
    "          \"Load_Signal_DetVars\":False, #Don't do here, but in seperate script\n",
    "          'Load_pi0_signal_DetVars':False, #Don't do here, but in seperate script\n",
    "          \"Load_data\":True,\n",
    "          \"FLATTEN\":True, #Have one row per reconstructed object in the analysis dataframe\n",
    "          \"only_presel\":False, #Create small files containing only variables necessary for pre-selection, for making pre-selection plots\n",
    "          \"EXT_in_training\":True,\n",
    "          \"Use_logit\":True,\n",
    "          \"nbins\":5} \n",
    "\n",
    "feature_names = Variables.First_pass_vars_for_BDT #All variables\n",
    "feature_names_MC = feature_names + [\"weight\"]\n",
    "loc_pkls = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/my_vars/\"\n",
    "\n",
    "samples = Functions.create_test_samples_list(Params)\n",
    "\n",
    "if Params[\"Load_pi0_signal\"] == True:\n",
    "    pi0_sample_strings = [] #Unfortunately need to make, to discriminate lepton final states from pi0 final states for signal\n",
    "    for pi0_point in Constants.HNL_mass_pi0_samples:\n",
    "        pi0_sample_strings += [str(pi0_point)+\"_pi0\"]\n",
    "        \n",
    "# end_string = \"_FINAL\"\n",
    "end_string = \"_full_Finished\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2627116c-1b50-4017-a0dd-4c3b380f7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_preselected_pkls(samples, Params, loc_pkls, filename, Test_filename):\n",
    "    \"\"\"\n",
    "    For loading in preselected pkl files, including BDT test pkls. \n",
    "    \"\"\"\n",
    "    sample_test_dict = {}\n",
    "    # sig_names_list = Constants.HNL_ee_samples_names+Constants.HNL_mass_pi0_samples_names+Constants.HNL_ee_dirac_names+Constants.HNL_pi0_dirac_names\n",
    "    sig_names_list = Constants.HNL_ee_samples_names\n",
    "    if Params[\"Load_DetVars\"] == True: loc_pkls += \"DetVars/\"\n",
    "    elif Params[\"Load_Signal_DetVars\"] == True: loc_pkls += \"Signal_DetVars/\"\n",
    "    elif Params['Load_pi0_signal_DetVars'] == True: loc_pkls += \"Signal_DetVars/pi0/\"\n",
    "    \n",
    "    for sample in samples:\n",
    "        if (sample == \"overlay\") or (sample in sig_names_list): start_str = loc_pkls + \"BDT_Test_dfs/Test_\"\n",
    "        elif (sample == \"beamoff\") and (Params[\"EXT_in_training\"] == True): start_str = loc_pkls + \"BDT_Test_dfs/Test_\"\n",
    "        elif sample in Constants.HNL_mass_pi0_samples_names: start_str = loc_pkls + \"BDT_Test_dfs/pi0_selection/Test_\"\n",
    "        else: start_str = loc_pkls + \"Preselected_\"\n",
    "        # sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"Preselected_{sample}_\"+Params[\"Run\"]+f\"_flattened{filename}.pkl\")\n",
    "        if ((sample == \"beamoff\") or (sample == \"overlay\") or (sample in sig_names_list)) and Params[\"EXT_in_training\"]==True:\n",
    "            sample_test_dict[sample] = pd.read_pickle(start_str+f\"{sample}_\"+Params[\"Run\"]+f\"_flattened{Test_filename}.pkl\")\n",
    "        else:\n",
    "            sample_test_dict[sample] = pd.read_pickle(start_str+f\"{sample}_\"+Params[\"Run\"]+f\"_flattened{filename}.pkl\")\n",
    "    \n",
    "    return sample_test_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f117d8-87e9-4d73-9649-d2b278fcb712",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_dict = Load_preselected_pkls(samples, Params, loc_pkls, end_string, \"_full_Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f174a06-5b6e-4796-b465-fa69292b845c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['overlay', 'dirtoverlay', 'beamoff', '150_pi0', '180_pi0', '200_pi0', '220_pi0', '240_pi0', '245_pi0', 'beamgood'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f30854c-0b46-43eb-806f-367c42e9a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_BDT_models(sample_dict, Params, BDT_name):\n",
    "    \"\"\"\n",
    "    Load variable list then loads BDT models for samples.\n",
    "    \"\"\"\n",
    "    if (Params[\"Load_lepton_signal\"] == True) or (Params[\"Load_lepton_dirac\"] == True): loc = \"bdts/\"\n",
    "    else: loc = \"bdts/pi0_selection/\"\n",
    "    \n",
    "    with open(loc+f\"input_vars/{BDT_name}_\"+Params[\"Run\"], \"rb\") as fp:   # Unpickling\n",
    "        saved_variables = pickle.load(fp)\n",
    "    \n",
    "    bdt_vars = saved_variables\n",
    "    xgb_test_dict = {}\n",
    "    \n",
    "    for sample in sample_dict:\n",
    "        xgb_test_dict[sample] = xgboost.DMatrix(sample_dict[sample][bdt_vars])\n",
    "    \n",
    "    if Params[\"Load_lepton_signal\"] == True: signal_names = Constants.HNL_ee_samples_names\n",
    "    elif Params[\"Load_pi0_signal\"] == True: signal_names = Constants.HNL_mass_pi0_samples_names\n",
    "    # elif Params[\"Load_lepton_dirac\"] == True: signal_names = Constants.HNL_ee_dirac_names\n",
    "    elif Params[\"Load_lepton_dirac\"] == True: signal_names = [\"10_ee\", \"100_ee\", \"150_ee\"] #Using Majorana models currently\n",
    "    # elif Params[\"Load_pi0_dirac\"] == True: signal_names = Constants.HNL_pi0_dirac_names\n",
    "    elif Params[\"Load_pi0_dirac\"] == True: signal_names = [\"150_pi0\", \"200_pi0\", \"245_pi0\"]\n",
    "    elif Params[\"Load_single_file\"] == True: signal_names = Params[\"single_file\"]\n",
    "    \n",
    "    for HNL_mass in signal_names:\n",
    "        bdt = xgboost.Booster()\n",
    "        bdt.load_model(loc+Params[\"Run\"]+f\"_{HNL_mass}{BDT_name}.json\")\n",
    "        mass_val = int(HNL_mass.split(\"_\")[0])\n",
    "        for sample in xgb_test_dict:\n",
    "            results = bdt.predict(xgb_test_dict[sample])\n",
    "            sample_dict[sample][f\"BDT_output_{mass_val}MeV\"] = results\n",
    "\n",
    "    return sample_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d3a1430-45be-4df0-a1e5-53713c21b86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['overlay', 'dirtoverlay', 'beamoff', '150_pi0', '180_pi0', '200_pi0', '220_pi0', '240_pi0', '245_pi0', 'beamgood'])\n"
     ]
    }
   ],
   "source": [
    "sample_test_dict = Load_BDT_models(sample_test_dict, Params, \"_full_Finished\")\n",
    "\n",
    "# sample_test_dict = Load_BDT_models(sample_test_dict, Params, end_string)\n",
    "\n",
    "print(sample_test_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c40481-4ea8-4c77-879d-6024bdee383f",
   "metadata": {},
   "source": [
    "## Checking max BDT score (for logit transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd70bcba-723a-481c-896b-b37562c25864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum score for all signal samples is 6.74228048324585\n",
      "Maximum integer for bins is 7.0\n"
     ]
    }
   ],
   "source": [
    "reload(Constants)\n",
    "if Params[\"Load_lepton_signal\"] == True: \n",
    "    HNL_masses = Constants.HNL_mass_samples\n",
    "    sample_names = Constants.HNL_ee_samples_names\n",
    "if Params[\"Load_pi0_signal\"] == True: \n",
    "    HNL_masses = Constants.HNL_mass_pi0_samples\n",
    "    sample_names = Constants.HNL_mass_pi0_samples_names\n",
    "if Params[\"Load_lepton_dirac\"] == True: \n",
    "    HNL_masses = Constants.HNL_ee_dirac_mass_samples\n",
    "    sample_names = Constants.HNL_ee_dirac_names\n",
    "if Params[\"Load_pi0_dirac\"] == True: \n",
    "    HNL_masses = Constants.HNL_pi0_dirac_mass_samples\n",
    "    sample_names = Constants.HNL_pi0_dirac_names\n",
    "elif Params[\"Load_single_file\"] == True: HNL_masses = [Params[\"single_file\"]]\n",
    "\n",
    "max_scores = []\n",
    "for HNL_mass in HNL_masses:\n",
    "    if Params[\"Load_lepton_signal\"] == True:\n",
    "        max_scores.append(max(Functions.logit(sample_test_dict[str(HNL_mass)+\"_ee\"][f\"BDT_output_{HNL_mass}MeV\"])))\n",
    "    if Params[\"Load_pi0_signal\"] == True:\n",
    "        max_scores.append(max(Functions.logit(sample_test_dict[str(HNL_mass)+\"_pi0\"][f\"BDT_output_{HNL_mass}MeV\"])))\n",
    "    if Params[\"Load_lepton_dirac\"] == True:\n",
    "        max_scores.append(max(Functions.logit(sample_test_dict[str(HNL_mass)+\"_ee_dirac\"][f\"BDT_output_{HNL_mass}MeV\"])))\n",
    "    if Params[\"Load_pi0_dirac\"] == True:\n",
    "        max_scores.append(max(Functions.logit(sample_test_dict[str(HNL_mass)+\"_pi0_dirac\"][f\"BDT_output_{HNL_mass}MeV\"])))\n",
    "    \n",
    "max_all_scores = max(max_scores)\n",
    "print(\"Maximum score for all signal samples is \" + str(max_all_scores))\n",
    "\n",
    "max_score_int = np.ceil(max_all_scores)\n",
    "print(\"Maximum integer for bins is \" + str(max_score_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9596562e-2410-4e48-ac46-0a19839a5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Params[\"Load_Signal_DetVars\"] == True: max_score_int = 7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c952252-1786-400e-aecd-7b94517223af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150, 180, 200, 220, 240, 245]\n"
     ]
    }
   ],
   "source": [
    "print(HNL_masses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f93b37-bb23-4c7c-a7a9-11b5ff5eb6e3",
   "metadata": {},
   "source": [
    "## Merging bins with zero bkg prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c11a3394-63e1-4a4e-9d60-247777d0429b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial nbins is 12\n",
      "With range of [-5.0, 7.0]\n",
      "Requiring more than 3 overlay events in each bin\n"
     ]
    }
   ],
   "source": [
    "#Checking which bins to merge Need to properly write this\n",
    "merged_bins_dict = {}\n",
    "min_score_cut = -5.0\n",
    "xlims = [min_score_cut,max_score_int]\n",
    "nbins = int(xlims[1]-xlims[0])\n",
    "min_overlay = 3 #The threshold of minimum overlay events required in a bin, used to be 1\n",
    "print(\"Initial nbins is \" + str(nbins))\n",
    "print(\"With range of \" + str(xlims))\n",
    "print(f\"Requiring more than {min_overlay} overlay events in each bin\")\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    bins_list = np.histogram(sample_test_dict['overlay'][f'BDT_output_{HNL_mass}MeV'],bins=nbins,range=xlims)[1]\n",
    "    if Params[\"Use_logit\"] == True:\n",
    "        bkg_scores=[Functions.logit(sample_test_dict['overlay'][f'BDT_output_{HNL_mass}MeV']),\n",
    "                    Functions.logit(sample_test_dict['dirtoverlay'][f'BDT_output_{HNL_mass}MeV']),\n",
    "                    Functions.logit(sample_test_dict['beamoff'][f'BDT_output_{HNL_mass}MeV'])]\n",
    "    totbkg=np.histogram(bkg_scores[0],bins=nbins,range=xlims)[0]+np.histogram(bkg_scores[1],bins=nbins,range=xlims)[0]+np.histogram(bkg_scores[2],bins=nbins,range=xlims)[0]\n",
    "    offbkg=np.histogram(bkg_scores[2],bins=nbins,range=xlims)[0]\n",
    "    overlaybkg=np.histogram(bkg_scores[0],bins=nbins,range=xlims)[0]\n",
    "    dirtbkg=np.histogram(bkg_scores[1],bins=nbins,range=xlims)[0]\n",
    "    bins_new=[]\n",
    "    for i,bin_bkg in enumerate(totbkg):\n",
    "        if(overlaybkg[i]>min_overlay): #Checking if unweighted overlay bkg has at least one event in the bin\n",
    "            bins_new.append(bins_list[i])\n",
    "\n",
    "    bins_new.append(bins_list[-1])\n",
    "    merged_bins_dict[HNL_mass] = bins_new\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306655e8-a2bf-47ce-ab6c-6044b43baa4d",
   "metadata": {},
   "source": [
    "## Plotting BDT outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896b786-2bd2-4eb3-91bf-7e28f82bc70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to add dirac samples into plot dict here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8085c6-d61b-4f6b-a2cb-e3aa6ba00d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_sample_norms(Params, sample_dict, sig_names, sig_train, overlay_train, EXT_train):\n",
    "    \"\"\"\n",
    "    Input Params, dict of sample dataframes with weights (if applicable) and the training fractions used.\n",
    "    Returns a dict of sample_norms. \n",
    "    \"\"\"\n",
    "    SF_test_sig = 1.0/(1-sig_train)\n",
    "    SF_test_overlay = 1.0/(1-overlay_train)\n",
    "    SF_EXT = 1.0/(1-EXT_train)\n",
    "\n",
    "    if Params[\"Run\"] == \"run1\": POT_scale_dict = Constants.run1_POT_scaling_dict\n",
    "    elif Params[\"Run\"] == \"run3\": POT_scale_dict = Constants.run3_POT_scaling_dict\n",
    "        \n",
    "    overlay_scale = POT_scale_dict[\"overlay\"]*SF_test_overlay\n",
    "    beamoff_scale = POT_scale_dict[\"beamoff\"]*SF_EXT\n",
    "    dirtoverlay_scale = POT_scale_dict[\"dirtoverlay\"] #dirt not used in training\n",
    "    \n",
    "    sample_norms={'overlay_test':np.array(sample_dict['overlay'][\"weight\"]*overlay_scale),\n",
    "                  'dirtoverlay':np.array(sample_dict['dirtoverlay'][\"weight\"]*dirtoverlay_scale),\n",
    "                  'beamoff':np.ones(len(sample_dict['beamoff']['n_pfps']))*beamoff_scale}\n",
    "    \n",
    "    for HNL_mass in sig_names:\n",
    "        signal_scale_list = np.ones(len(sample_dict[HNL_mass]['n_pfps']))*SF_test_sig\n",
    "        sample_norms[HNL_mass]=signal_scale_list\n",
    "            \n",
    "    return sample_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "912923d0-818e-4422-b043-a1fe0644e27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4285714285714286\n",
      "Creating the sample and normalisation dictionaries\n",
      "Adding signal samples to sample plot dictionary\n"
     ]
    }
   ],
   "source": [
    "#Needs to be rewritten, perhaps made into a function\n",
    "train_vs_test_fraction = 0.7 #This was set in 3_BDT_training, need to change if I have changed this there.\n",
    "SF_test = 1.0/(1-train_vs_test_fraction)\n",
    "\n",
    "EXT_train = 0.3\n",
    "SF_EXT = 1.0/(1-EXT_train)\n",
    "print(SF_EXT)\n",
    "\n",
    "if (Params[\"Load_lepton_dirac\"]) or (Params[\"Load_pi0_dirac\"]): SF_test=1.0\n",
    "    \n",
    "if Params[\"Run\"] == \"run1\":\n",
    "    overlay_scale = Constants.SF_overlay_run1*SF_test\n",
    "    if Params[\"EXT_in_training\"] == True:\n",
    "        EXT_scale_list = np.ones(len(sample_test_dict['beamoff'][f'BDT_output_{HNL_mass}MeV']))*Constants.SF_EXT_run1*SF_EXT\n",
    "    if Params[\"EXT_in_training\"] == False:\n",
    "        EXT_scale_list = np.ones(len(sample_test_dict['beamoff'][f'BDT_output_{HNL_mass}MeV']))*Constants.SF_EXT_run1\n",
    "    dirt_scale = Constants.SF_dirt_run1\n",
    "        \n",
    "elif Params[\"Run\"] == \"run3\":\n",
    "    overlay_scale = Constants.SF_overlay_run3*SF_test\n",
    "    if Params[\"EXT_in_training\"] == True:\n",
    "        EXT_scale_list = np.ones(len(sample_test_dict['beamoff'][f'BDT_output_{HNL_mass}MeV']))*Constants.SF_EXT_run3*SF_EXT\n",
    "    if Params[\"EXT_in_training\"] == False:\n",
    "        EXT_scale_list = np.ones(len(sample_test_dict['beamoff'][f'BDT_output_{HNL_mass}MeV']))*Constants.SF_EXT_run3\n",
    "    dirt_scale = Constants.SF_dirt_run3\n",
    "\n",
    "print(\"Creating the sample and normalisation dictionaries\")    \n",
    "samples_plot={'overlay_test':sample_test_dict['overlay'],\n",
    "         'dirtoverlay':sample_test_dict['dirtoverlay'],\n",
    "         'beamoff':sample_test_dict['beamoff']}\n",
    "\n",
    "sample_norms={'overlay_test':np.array(sample_test_dict['overlay'][\"weight\"]*overlay_scale),\n",
    "         'dirtoverlay':np.array(sample_test_dict['dirtoverlay'][\"weight\"]*dirt_scale),\n",
    "         'beamoff':EXT_scale_list}\n",
    "\n",
    "print(\"Adding signal samples to sample plot dictionary\")\n",
    "if Params[\"Load_single_file\"] == True:\n",
    "    HNL_mass = Params[\"single_file\"]\n",
    "    signal_scale_list = np.ones(len(sample_test_dict[HNL_mass][f'BDT_output_{HNL_mass}MeV']))*SF_test\n",
    "    samples_plot[HNL_mass]=sample_test_dict[HNL_mass]\n",
    "    sample_norms[HNL_mass]=signal_scale_list\n",
    "    samples_plot[\"beamgood\"]=sample_test_dict[\"beamgood\"]\n",
    "    sample_norms[\"beamgood\"]=np.ones(len(sample_test_dict[\"beamgood\"]))\n",
    "    \n",
    "elif Params[\"Load_lepton_signal\"] == True:\n",
    "    for HNL_mass in Constants.HNL_mass_samples:\n",
    "        signal_scale_list = np.ones(len(sample_test_dict[str(HNL_mass)+\"_ee\"][f'BDT_output_{HNL_mass}MeV']))*SF_test\n",
    "        samples_plot[HNL_mass]=sample_test_dict[str(HNL_mass)+\"_ee\"]\n",
    "        sample_norms[HNL_mass]=signal_scale_list\n",
    "if Params[\"Load_pi0_signal\"] == True:\n",
    "    for HNL_mass in Constants.HNL_mass_pi0_samples:\n",
    "        signal_scale_list = np.ones(len(sample_test_dict[str(HNL_mass)+\"_pi0\"][f'BDT_output_{HNL_mass}MeV']))*SF_test\n",
    "        samples_plot[str(HNL_mass)+\"_pi0\"]=sample_test_dict[str(HNL_mass)+\"_pi0\"]\n",
    "        # samples_plot[HNL_mass]=sample_test_dict[HNL_mass]\n",
    "        sample_norms[str(HNL_mass)+\"_pi0\"]=signal_scale_list\n",
    "    \n",
    "else: \n",
    "    for i, HNL_mass in enumerate(HNL_masses):\n",
    "        sig_name = sample_names[i]\n",
    "        signal_scale_list = np.ones(len(sample_test_dict[sig_name][f'BDT_output_{HNL_mass}MeV']))*SF_test\n",
    "        samples_plot[sig_name]=sample_test_dict[sig_name]\n",
    "        sample_norms[sig_name]=signal_scale_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a9e59-e952-4a06-b5ed-7b03a430a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_plot.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b235c8-35d8-486b-a77f-94c0be052bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(PT)\n",
    "\n",
    "# if Params[\"Load_single_file\"] == True:\n",
    "#     HNL_masses = [Params[\"single_file\"]]\n",
    "# elif Params[\"Load_lepton_signal\"] == True:\n",
    "#     HNL_masses = Constants.HNL_mass_samples\n",
    "# if Params[\"Load_pi0_signal\"] == True:\n",
    "#     HNL_masses = Constants.HNL_mass_pi0_samples\n",
    "    \n",
    "xlims=[0,max_score_int]    \n",
    "BINS = int(xlims[1]-xlims[0])\n",
    "    \n",
    "PT.Plot_BDT_output(HNL_masses=HNL_masses, signal_names=sample_names, samples=samples_plot, sample_norms=sample_norms, colours={}, xlims=xlims,\n",
    "                   bins=BINS,figsize=[12,8], MergeBins=True, density=False, legloc=\"best\",logy=False, savefig=False, \n",
    "                   save_str = \"_full_Finished\", Run=Params[\"Run\"], logit = Params[\"Use_logit\"], HNL_scale=0.01)\n",
    "\n",
    "#If plotting data too: Write this\n",
    "# PT.Plot_BDT_output_data(HNL_masses=HNL_masses, samples=samples_plot, sample_norms=sample_norms, colours={}, xlims=xlims,\n",
    "#                    bins=BINS,figsize=[12,8], MergeBins=True, density=False, legloc=\"best\",logy=False, savefig=True, \n",
    "#                    save_str = BDT_name, Run=Params[\"Run\"], logit = Params[\"Use_logit\"], HNL_scale=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70862c0a-eb3f-49db-bf68-d323d2ae6190",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving BDT output to .root files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "265d31dc-fc06-4287-81ce-719371104775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_one_hist(hist,name,nbins,xlims):\n",
    "    tData = ROOT.TH1F(name,name,len(nbins)-1,array(\"d\",nbins))\n",
    "    for i in range(len(nbins)-1):\n",
    "        tData.SetBinContent(i+1,hist['hist'][i])\n",
    "        tData.SetBinError(i+1,hist['err'][i])\n",
    "    return tData\n",
    "    \n",
    "def SaveToRoot_new(nbins,xlims,hist_samples,theta,fileName='test.root'): \n",
    "    rFile = ROOT.TFile(f'{fileName}','RECREATE')\n",
    "    tData = ROOT.TH1F(\"theta\",\"theta\",1,array(\"d\",[0,1]))\n",
    "    tData.SetBinContent(1,theta)\n",
    "    rFile.Write()\n",
    "    for i, name in enumerate(hist_samples):\n",
    "        tData = Save_one_hist(hist_samples[name],name,nbins,xlims)\n",
    "        rFile.Write()\n",
    "    rFile.Close()\n",
    "\n",
    "def make_stat_err(hist, SF): #This doesn't take the weights into account, assumes they average to 1.0\n",
    "    stat_err = []\n",
    "    for i in range(0,len(hist[0])):\n",
    "        error = np.sqrt(hist[0][i])*np.sqrt(SF)\n",
    "        stat_err.append(error)\n",
    "    return stat_err\n",
    "\n",
    "def make_stat_err_true(var, bins, weights_times_SF): #Saved in Functions.py\n",
    "    hist_unweighted = np.histogram(var,bins=bins)[0]\n",
    "    hist_weighted = np.histogram(var,bins=bins,weights=weights_times_SF)[0]\n",
    "    Total_SF = np.nan_to_num(hist_weighted/hist_unweighted)\n",
    "    stat_err = np.sqrt(hist_unweighted)*Total_SF\n",
    "    return stat_err\n",
    "\n",
    "def make_stat_err_new(var, bins, weights_times_SF): \n",
    "    hist_squared = np.histogram(var,bins=bins,weights=weights_times_SF**2)[0]\n",
    "    stat_err=np.sqrt(hist_squared)\n",
    "    return stat_err\n",
    "\n",
    "def SF_times_weights(sample_dict, sample, Run, SF_test, SF_signal):\n",
    "    if Run == \"run1\": POT_norm = Constants.run1_POT_scaling_dict\n",
    "    if Run == \"run3\": POT_norm = Constants.run3_POT_scaling_dict\n",
    "    \n",
    "    if sample == 'overlay': SF = sample_dict[sample]['weight']*POT_norm[sample]*SF_test\n",
    "    elif sample == 'dirtoverlay': SF = sample_dict[sample]['weight']*POT_norm[sample]\n",
    "    elif (sample == 'beamoff') or (sample =='beamgood'): SF = np.ones(len(sample_dict[sample][f'run']))*POT_norm[sample]\n",
    "    else: \n",
    "        SF = np.ones(len(sample_dict[str(sample)+\"_pi0\"][f'BDT_output_{sample}MeV']))*(POT_norm[sample]*SF_test*SF_signal)\n",
    "    return SF\n",
    "\n",
    "def SF_times_weights_pi0(sample_dict, sample, Run, SF_test, SF_signal):\n",
    "    if Run == \"run1\": POT_norm = Constants.run1_POT_scaling_dict\n",
    "    if Run == \"run3\": POT_norm = Constants.run3_POT_scaling_dict\n",
    "    \n",
    "    if sample == 'overlay': SF = sample_dict[sample]['weight']*POT_norm[sample]*SF_test\n",
    "    elif sample == 'dirtoverlay': SF = sample_dict[sample]['weight']*POT_norm[sample]\n",
    "    elif (sample == 'beamoff') or (sample =='beamgood'): SF = np.ones(len(sample_dict[sample][f'run']))*POT_norm[sample]\n",
    "    else: \n",
    "        SF = np.ones(len(sample_dict[str(sample)+\"_pi0\"][f'BDT_output_{sample}MeV']))*(POT_norm[str(sample)+\"_pi0\"]*SF_test*SF_signal)\n",
    "    return SF\n",
    "\n",
    "def SF_times_weights_sample_names(sample_dict, sample, HNL_mass, Run, SF_test, SF_signal):\n",
    "    if Run == \"run1\": POT_norm = Constants.run1_POT_scaling_dict\n",
    "    if Run == \"run3\": POT_norm = Constants.run3_POT_scaling_dict\n",
    "    \n",
    "    if sample == 'overlay': SF = sample_dict[sample]['weight']*POT_norm[sample]*SF_test\n",
    "    elif sample == 'dirtoverlay': SF = sample_dict[sample]['weight']*POT_norm[sample]\n",
    "    elif (sample == 'beamoff') or (sample =='beamgood'): SF = np.ones(len(sample_dict[sample][f'run']))*POT_norm[sample]\n",
    "    else: \n",
    "        SF = np.ones(len(sample_dict[sample][f'BDT_output_{HNL_mass}MeV']))*(POT_norm[sample]*SF_test*SF_signal)\n",
    "    return SF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ebca0-be3e-4a28-97c9-86fb959899cd",
   "metadata": {},
   "source": [
    "## Saving BDT output for reweighting systematics plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df8e0d0f-5968-47d9-9114-1badbc39f7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([150, 180, 200, 220, 240, 245])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_bins_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee823212-6e51-4aaf-bc9d-dcb8b03a66c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to save the new BDT scores to .pkl files? y/n  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'sub', 'evt', 'weightSplineTimesTune', 'ppfx_cv', 'npi0', 'weight', 'BDT_output_150MeV', 'BDT_output_180MeV', 'BDT_output_200MeV', 'BDT_output_220MeV', 'BDT_output_240MeV', 'BDT_output_245MeV']\n",
      "Index(['run', 'sub', 'evt', 'weightSplineTimesTune', 'ppfx_cv', 'npi0',\n",
      "       'weight', 'BDT_output_150MeV', 'BDT_output_180MeV', 'BDT_output_200MeV',\n",
      "       'BDT_output_220MeV', 'BDT_output_240MeV', 'BDT_output_245MeV'],\n",
      "      dtype='object')\n",
      "pkl_files/run3/current_files/Results/pi0/overlay_results_EXT_full_Finished.pkl\n"
     ]
    }
   ],
   "source": [
    "pkl_results_loc = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/Results/\"\n",
    "if Params[\"Load_pi0_signal\"] == True: pkl_results_loc += \"pi0/\"\n",
    "\n",
    "# print(sample_test_dict['overlay'].keys())\n",
    "if Params[\"EXT_in_training\"]==True:\n",
    "    save_name = f\"_EXT{end_string}\"\n",
    "if Params[\"EXT_in_training\"]==False:\n",
    "    save_name = f\"{end_string}\"\n",
    "    \n",
    "print_vals = input(\"Do you want to save the new BDT scores to .pkl files? y/n \")\n",
    "if print_vals == \"y\":\n",
    "    \n",
    "    columns = Variables.event_vars + Variables.weight_related + [\"weight\"]\n",
    "    for HNL_mass in merged_bins_dict:\n",
    "        columns.append(f'BDT_output_{HNL_mass}MeV')\n",
    "    print(columns)\n",
    "    \n",
    "    overlay_to_save = sample_test_dict['overlay'][columns].copy()\n",
    "    #df2 = overlay_merged_to_save.loc[:,~overlay_merged_to_save.columns.duplicated()] #Getting rid of duplicate \"weight\" column\n",
    "    #df2.to_pickle(pkl_results_loc + \"overlay_results.pkl\")\n",
    "    overlay_to_save.to_pickle(pkl_results_loc + f\"overlay_results{save_name}.pkl\")\n",
    "    \n",
    "    print(overlay_to_save.keys())\n",
    "    \n",
    "print(pkl_results_loc + f\"overlay_results{save_name}.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750fff37-4cbe-441f-9052-8f078c9c7659",
   "metadata": {},
   "source": [
    "## Using sample_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc59975b-1c4a-4c48-9a91-d616c3906a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([150, 180, 200, 220, 240, 245])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_bins_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f7f78db-1bcf-4bef-b4f4-77812f7868fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Utilities.Functions' from '/home/david/HNL/ee_decays/Utilities/Functions.py'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "857c0409-eb43-4c0a-b71d-9202946dffc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving minimum BDT score of -5.0\n",
      "150\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23051/3677000671.py:28: RuntimeWarning: invalid value encountered in divide\n",
      "  Total_SF = np.nan_to_num(hist_weighted/hist_unweighted)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245\n",
      "Saved all\n",
      "Saved with names like run3_245_pi0_EXT_full_Finished.root\n"
     ]
    }
   ],
   "source": [
    "# SCALE_UP_dict = {2:5,10:5,20:200,50:50,100:10,150:5,180:4,200:4,220:4,240:4,245:4} #Doing this because the scale factor used in pyhf is bounded\n",
    "SCALE_UP_dict = {2:1.0,10:1.0,20:1.0,50:1.0,100:1.0,150:1.0, 180:1.0, 200:1.0, 220:1.0, 240:1.0, 245:1.0}\n",
    "\n",
    "new_theta_dict, dict_for_root = {}, {}\n",
    "\n",
    "# if Params[\"Load_pi0_signal\"] == True: exit() #This cell is for lepton samples\n",
    "\n",
    "bkg_samples = []\n",
    "for sample in samples:\n",
    "    if isinstance(sample,str): bkg_samples.append(sample)\n",
    "\n",
    "train_vs_test_fraction = 0.7 #This was set in 3_BDT_training, need to change if I have changed this there.\n",
    "SF_test = 1.0/(1-train_vs_test_fraction)\n",
    "if (Params[\"Load_lepton_dirac\"]) or (Params[\"Load_pi0_dirac\"]): SF_test=1.0 #Didn't train on these samples so don't scale up again. \n",
    "\n",
    "# name_type = \"ee\"\n",
    "name_type = Functions.Get_signal_name_type(Params)\n",
    "\n",
    "print(\"Saving minimum BDT score of \" + str(merged_bins_dict[150][0]))\n",
    "\n",
    "for i, HNL_mass in enumerate(HNL_masses):\n",
    "    print(HNL_mass)\n",
    "    sig_name = sample_names[i]\n",
    "    original_theta = Constants.theta_mu_4_dict[HNL_mass]\n",
    "    new_theta = original_theta*SCALE_UP_dict[HNL_mass]\n",
    "    new_theta_dict[HNL_mass] = original_theta*SCALE_UP_dict[HNL_mass]\n",
    "    SCALE_EVENTS = SCALE_UP_dict[HNL_mass]**4 #The Number of events is proportional to theta^4\n",
    "    \n",
    "    bins = merged_bins_dict[HNL_mass]\n",
    "    sample_list = bkg_samples + [sig_name]\n",
    "    # if Params[\"Load_data\"] == True: sample_list += [\"beamgood\"]\n",
    "    for sample in sample_list:\n",
    "        score = Functions.logit(sample_test_dict[sample][f'BDT_output_{HNL_mass}MeV'])\n",
    "        SF_list = SF_times_weights_sample_names(sample_test_dict, sample, HNL_mass, Params[\"Run\"], SF_test, SCALE_EVENTS)\n",
    "        stat_err = make_stat_err_true(score, bins, SF_list) #Should generatlly use this\n",
    "        # stat_err = make_stat_err_new(score, bins, SF_list)\n",
    "        hist = np.histogram(score, bins=bins, weights=SF_list)\n",
    "        dict_for_root[sample] = {'bins': np.array(hist[1]), 'hist': np.array(hist[0]), 'err': np.array(stat_err)}\n",
    "        \n",
    "    hist_samples = {\"bkg_overlay\":dict_for_root['overlay'],\"bkg_dirt\":dict_for_root['dirtoverlay'],\n",
    "                    \"bkg_EXT\":dict_for_root['beamoff'],\"signal\":dict_for_root[sample]}\n",
    "    if 'beamgood' in bkg_samples:\n",
    "        hist_samples.update({\"data\":dict_for_root['beamgood']})\n",
    "        \n",
    "    # save_name = Params[\"Run\"]+f\"_{HNL_mass}MeV_{name_type}{end_string}.root\"\n",
    "    if Params[\"EXT_in_training\"]==True:\n",
    "        save_name = Params[\"Run\"]+f\"_{HNL_mass}_{name_type}_EXT{end_string}.root\"\n",
    "    if Params[\"EXT_in_training\"]==False:\n",
    "        save_name = Params[\"Run\"]+f\"_{HNL_mass}_{name_type}{end_string}.root\"\n",
    "    \n",
    "    if (Params[\"Load_pi0_signal\"]==True) or (Params[\"Load_pi0_dirac\"]==True):\n",
    "        SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"bdt_output/pi0/\"+save_name)\n",
    "        SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"Uncertainties/pi0/\"+save_name)\n",
    "    else:\n",
    "        SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"bdt_output/\"+save_name)\n",
    "        SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=\"Uncertainties/\"+save_name)\n",
    "        \n",
    "print(\"Saved all\")\n",
    "print(f\"Saved with names like {save_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9a478-704c-4b02-b0e3-a06763631a98",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculating efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44822b-e324-4216-b3d7-e4967c8c4d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_efficiency(df, file_name, Run, BDT_cutoff, HNL_mass, debug=False):\n",
    "    if Run == \"run1\": ev_num_dict = Constants.run1_event_numbers\n",
    "    if Run == \"run3\": ev_num_dict = Constants.run3_event_numbers\n",
    "    \n",
    "    if file_name == \"beamoff\" or file_name == \"dirtoverlay\":\n",
    "        norm = 1.0\n",
    "    else:\n",
    "        train_vs_test_fraction = 0.7 #Set in script 3.0\n",
    "        norm = 1.0/(1-train_vs_test_fraction) #Need to scale up to account for events lost\n",
    "    \n",
    "    total_ev_num = len(df)\n",
    "    ev_num_initial = ev_num_dict[file_name]\n",
    "    cut_df = df.query(f\"BDT_output_{HNL_mass}MeV > {BDT_cutoff}\")\n",
    "    new_ev_num = len(cut_df)\n",
    "    \n",
    "    efficiency = norm*(new_ev_num/ev_num_initial)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Total events is {total_ev_num}\")\n",
    "        print(f\"Events after cut is {new_ev_num}\")\n",
    "    \n",
    "    return efficiency\n",
    "\n",
    "def sum_total_weights(df): #This is to check that the weighting doesn't affect the overall efficiency significantly (i.e av weight is approx 1)\n",
    "    total_evs = len(df)\n",
    "    sum_weights = df[\"weight\"].sum()\n",
    "    \n",
    "    weight_per_ev = sum_weights/total_evs\n",
    "    \n",
    "    print(f\"Sum of weights is {sum_weights}\")\n",
    "    print(f\"Average weight is {weight_per_ev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3134f-a034-452a-83bf-09f98a074793",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_test_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59979a5c-4225-479a-8a81-3a762dd82a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_BDT_cut = -20\n",
    "# logit_BDT_cut = 2.0\n",
    "standard_BDT_cut = Functions.invlogit(logit_BDT_cut)\n",
    "print(f\"Using a logit BDT cut of {logit_BDT_cut}, which corresponds to {standard_BDT_cut} in [0,1] BDT score.\")\n",
    "\n",
    "efficiency_dict = {}\n",
    "\n",
    "for file in Constants.HNL_mass_samples: #Just signal samples\n",
    "# for file in sample_test_dict:\n",
    "\n",
    "    efficiency_dict[file] = calculate_efficiency(sample_test_dict[file], file, Params[\"Run\"], standard_BDT_cut, file, debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebfe02-3497-4c7e-beec-b8419bd06b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in ['overlay', 'dirtoverlay', 'beamoff']: #Calculating efficiency for 100MeV BDT model score\n",
    "\n",
    "    efficiency_dict[file] = calculate_efficiency(sample_test_dict[file], file, Params[\"Run\"], standard_BDT_cut, 100, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d484621-3f26-4510-9c8b-37a7dc0d643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Params[\"Run\"] + \" pre-selectoin efficiencies are\")\n",
    "for file in sample_test_dict:\n",
    "    # print(f\"{file} efficiency is \" + str(efficiency_dict[file]))\n",
    "    print(f\"{file} efficiency is \" + str(efficiency_dict[file]*100) + \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46e5f7-ea37-4d7a-9ab0-818012d23026",
   "metadata": {
    "tags": []
   },
   "source": [
    "## \"Closure test\" i.e testing adjacent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b637f-ce6a-4900-b20e-c6a82035e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass_point = 50\n",
    "mass_point = 240\n",
    "\n",
    "savefig = False\n",
    "\n",
    "test_models = [180, 245]\n",
    "\n",
    "test_results, test_labels = [], []\n",
    "\n",
    "plt.figure(figsize=[12,8],facecolor='white')\n",
    "\n",
    "for mass_model in test_models:\n",
    "    test_results.append(Functions.logit(samples_plot[mass_point][f'BDT_output_{mass_model}MeV']))\n",
    "    test_labels.append(f\"{mass_model} MeV model\")\n",
    "\n",
    "plot=plt.hist(Functions.logit(samples_plot[mass_point][f'BDT_output_{mass_point}MeV']),label=f\"Correct {mass_point} MeV model\",\n",
    "                         range=xlims,bins=merged_bins_dict[mass_point],histtype=\"step\",\n",
    "                         stacked=False,density=False,linewidth=4)\n",
    "plot=plt.hist(test_results,label=test_labels,range=xlims,bins=merged_bins_dict[mass_point],histtype=\"step\",\n",
    "              stacked=False,density=False,linewidth=2)\n",
    "\n",
    "plt.xlabel(f\"BDT score for {mass_point} MeV sample\")\n",
    "plt.ylabel(\"Events\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "if savefig == True:\n",
    "    plt.savefig(\"plots/BDT_output/Adjacent_models_tests/\" + Params[\"Run\"] + \"_\" + str(mass_point) + \"MeV_adjcent_models.pdf\")\n",
    "    plt.savefig(\"plots/BDT_output/Adjacent_models_tests/\" + Params[\"Run\"] + \"_\" + str(mass_point) + \"MeV_adjcent_models.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfb395-b3dc-4fcc-83ad-60a025d0e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the output in a .root file\n",
    "bkg_list = ['overlay', 'dirtoverlay', 'beamoff']\n",
    "new_theta = original_theta*SCALE_UP_dict[mass_point]\n",
    "bkg_hist_dict = {}\n",
    "# test_models += [mass_point]\n",
    "# res = [*set(test_models)]\n",
    "# print(res)\n",
    "# print(test_models)\n",
    "for mass_model in test_models+[mass_point]:\n",
    "    # for bkg in bkg_list:\n",
    "    #     bkg_hist_dict[bkg] = Functions.logit(sample_test_dict[bkg][f'BDT_output_{mass_model}MeV'])\n",
    "    SF_signal_list = np.ones(len(sample_test_dict[mass_point][f'BDT_output_{mass_model}MeV']))*Constants.run1_POT_scaling_dict[HNL_mass]*SF_test*SCALE_EVENTS\n",
    "    SF_overlay_list = sample_test_dict['overlay']['weight']*Constants.SF_overlay_run1*SF_test\n",
    "    SF_dirt_list = sample_test_dict['dirtoverlay']['weight']*Constants.SF_dirt_run1\n",
    "    SF_EXT_list = np.ones(len(sample_test_dict['beamoff'][f'BDT_output_{mass_model}MeV']))*Constants.SF_EXT_run1\n",
    "    \n",
    "    bins=merged_bins_dict[mass_point]\n",
    "    signal_bdt_hist = np.histogram(Functions.logit(sample_test_dict[mass_point][f'BDT_output_{mass_model}MeV']), \n",
    "                               bins=bins, weights=SF_signal_list, range = xlims_logit)\n",
    "    overlay_bdt_hist = np.histogram(Functions.logit(sample_test_dict['overlay'][f'BDT_output_{mass_model}MeV']), \n",
    "                                bins=bins, weights=SF_overlay_list, range = xlims_logit)\n",
    "    dirt_bdt_hist = np.histogram(Functions.logit(sample_test_dict['dirtoverlay'][f'BDT_output_{mass_model}MeV']), \n",
    "                             bins=bins, weights=SF_dirt_list, range = xlims_logit)\n",
    "    EXT_bdt_hist = np.histogram(Functions.logit(sample_test_dict['beamoff'][f'BDT_output_{mass_model}MeV']), \n",
    "                            bins=bins, weights=SF_EXT_list, range = xlims_logit)\n",
    "\n",
    "    signal_bdt_err = make_stat_err(signal_bdt_hist, Constants.run1_POT_scaling_dict[HNL_mass]*SF_test*SCALE_EVENTS)\n",
    "    overlay_bdt_err = make_stat_err(overlay_bdt_hist, Constants.SF_overlay_run1*SF_test)\n",
    "    dirt_bdt_err = make_stat_err(dirt_bdt_hist, Constants.SF_dirt_run1)\n",
    "    EXT_bdt_err = make_stat_err(EXT_bdt_hist, Constants.SF_EXT_run1)\n",
    "\n",
    "    data_h = overlay_bdt_hist[0]+dirt_bdt_hist[0]+EXT_bdt_hist[0]\n",
    "\n",
    "    bkg_overlay = {'bins': np.array(overlay_bdt_hist[1]), 'hist': np.array(overlay_bdt_hist[0]), 'err': np.array(overlay_bdt_err)}\n",
    "    bkg_dirt = {'bins': np.array(dirt_bdt_hist[1]), 'hist': np.array(dirt_bdt_hist[0]), 'err': np.array(dirt_bdt_err)}\n",
    "    bkg_EXT = {'bins': np.array(EXT_bdt_hist[1]), 'hist': np.array(EXT_bdt_hist[0]), 'err': np.array(EXT_bdt_err)}\n",
    "    sig = {'bins': np.array(signal_bdt_hist[1]), 'hist': np.array(signal_bdt_hist[0]), 'err': np.array(signal_bdt_err)}\n",
    "    data = {'bins': np.array(overlay_bdt_hist[1]), 'hist': np.array(data_h), 'err': np.array(overlay_bdt_err)}\n",
    "\n",
    "    hist_samples = {\"bkg_overlay\":bkg_overlay,\"bkg_dirt\":bkg_dirt,\"bkg_EXT\":bkg_EXT,\"signal\":sig,\"data\":data}\n",
    "\n",
    "    save_name = f\"bdt_output/adjacent_models/\"+ Params[\"Run\"] + f\"_{mass_point}MeV_{mass_model}MeV_model.root\"\n",
    "    SaveToRoot_new(merged_bins_dict[HNL_mass],xlims,hist_samples,theta=new_theta,fileName=save_name)\n",
    "    print(\"Saved \" + save_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9fbe06-875c-4ef8-8d36-638646360096",
   "metadata": {},
   "source": [
    "## Saving only high BDT Score events for variable study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695497be-be6d-45c3-9125-351686a4dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.95\n",
    "signal_like = {}\n",
    "frac_retained = {}\n",
    "pkl_variable_tests_loc = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/Variable_tests/\"\n",
    "\n",
    "Number_signal = 1000\n",
    "top_signal = {}\n",
    "\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    signal_like[HNL_mass] = sample_test_dict[HNL_mass].query(f\"BDT_output_{HNL_mass}MeV > {cutoff}\")\n",
    "    frac_retained[HNL_mass] = len(signal_like[HNL_mass])/len(sample_test_dict[HNL_mass])\n",
    "    \n",
    "    signal_like[HNL_mass].to_pickle(pkl_variable_tests_loc + f\"signal_like_{HNL_mass}MeV.pkl\")\n",
    "    \n",
    "    top_signal[HNL_mass] = sample_test_dict[HNL_mass].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(Number_signal)\n",
    "    \n",
    "print(frac_retained)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a07a5-acf6-4e6c-9ca0-ff5c930fabd1",
   "metadata": {},
   "source": [
    "## Looking at variables for signal-like events vs. full sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1346efb-8389-4995-a70a-d6fadc614ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load signal-like pkls\n",
    "pkl_variable_tests_loc = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/Variable_tests/\"\n",
    "\n",
    "signal_like[HNL_mass] = {}\n",
    "\n",
    "for HNL_mass in [20,50,100]:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b417309-8164-4f36-97b9-d1f6c6da1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,8],facecolor='white')\n",
    "\n",
    "HNL_mass = 100\n",
    "\n",
    "variable = 'shr_theta_v'\n",
    "#'shr_theta_v', 'shr_phi_v', 'trk_theta_v', 'trk_phi_v'\n",
    "\n",
    "bins = 40\n",
    "xlims = [0,3.2]\n",
    "\n",
    "plt.hist(sample_test_dict[HNL_mass][variable],\n",
    "#               label=[f\"HNL ({mass} MeV) \\n $|U_{{\\mu4}}|^2=\"+sci_notation(sample_info[\"300\"][\"theta_u2\"]) +f\" (x{HNLplotscale})\"],\n",
    "         label=[f\"All preselected {HNL_mass} MeV HNL\"],\n",
    "         range=xlims,bins=bins,\n",
    "         stacked=False,density=True,\n",
    "         histtype=\"step\",lw=3)\n",
    "\n",
    "plt.hist(top_signal[HNL_mass][variable],\n",
    "#               label=[f\"HNL ({mass} MeV) \\n $|U_{{\\mu4}}|^2=\"+sci_notation(sample_info[\"300\"][\"theta_u2\"]) +f\" (x{HNLplotscale})\"],\n",
    "         label=[f\"Signal-like {HNL_mass} MeV HNL\"],\n",
    "         range=xlims,bins=bins,\n",
    "         stacked=False,density=True,\n",
    "         histtype=\"step\",lw=3)\n",
    "plt.xlabel(variable)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25909893-a728-4a26-b351-cb43c089e251",
   "metadata": {},
   "source": [
    "## Make list of run subrun event which are most signal-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65122c06-640e-450a-b49c-2f0ef6966a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_CV_100 = sample_test_dict['100_CV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622ac58-ed31-41dd-a030-746a64eb8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 50\n",
    "csv_location = \"bdt_output/signal_like_rse_lists/CV_samples/\"\n",
    "HNL_mass = 100\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "signal_sorted = sample_CV_100.sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "signal_run_sub_event = signal_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "csv_name = csv_location + Params[\"Run\"] + f\"_signal_CV_{HNL_mass}.csv\"\n",
    "signal_run_sub_event.to_csv(csv_name, sep=\" \", header=False, index=False)\n",
    "\n",
    "txt_file = csv_location + Params[\"Run\"] + f\"_signal_CV_{HNL_mass}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_name, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe7d6bf-df1d-47df-b86b-77fcfd424490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_event_list(df, HNL_mass, N_events, output_name):\n",
    "    sorted_scores = df.sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(N_events)\n",
    "    run_sub_event = sorted_scores[[\"run\", \"sub\", \"evt\"]]\n",
    "    run_sub_event.to_csv(output_name+\".csv\", sep=\" \", header=False, index=False)\n",
    "    \n",
    "    csv_file = output_name+\".csv\"\n",
    "    txt_file = output_name + \".list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "    with open(txt_file, \"w\") as my_output_file:\n",
    "        with open(csv_file, \"r\") as my_input_file:\n",
    "            [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "        my_output_file.close()\n",
    "    \n",
    "    print(f\"Saved as {output_name}.list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a7a88-8e83-4b3f-87d8-97f5cc9c8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_N = 50\n",
    "csv_location = \"bdt_output/signal_like_rse_lists/\"\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    signal_sorted = sample_test_dict[HNL_mass].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "    overlay_sorted = sample_test_dict['overlay'].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "    dirt_sorted = sample_test_dict['dirtoverlay'].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "    beamoff_sorted = sample_test_dict['beamoff'].sort_values(f\"BDT_output_{HNL_mass}MeV\", ascending=False).head(top_N)\n",
    "    signal_run_sub_event = signal_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "    overlay_run_sub_event = overlay_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "    dirt_run_sub_event = dirt_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "    beamoff_run_sub_event = beamoff_sorted[[\"run\", \"sub\", \"evt\"]]\n",
    "    # print(df_run_sub_event.keys())\n",
    "    csv_name = csv_location + Params[\"Run\"] + f\"_signal_{HNL_mass}.csv\"\n",
    "    csv_overlay_name = csv_location + Params[\"Run\"] + f\"_overlay_{HNL_mass}.csv\"\n",
    "    csv_dirt_name = csv_location + Params[\"Run\"] + f\"_dirtoverlay_{HNL_mass}.csv\"\n",
    "    csv_beamoff_name = csv_location + Params[\"Run\"] + f\"_beamoff_{HNL_mass}.csv\"\n",
    "\n",
    "    signal_run_sub_event.to_csv(csv_name, sep=\" \", header=False, index=False) #i.e separating by a space, removing column names\n",
    "    overlay_run_sub_event.to_csv(csv_overlay_name, sep=\" \", header=False, index=False)\n",
    "    dirt_run_sub_event.to_csv(csv_dirt_name, sep=\" \", header=False, index=False)\n",
    "    beamoff_run_sub_event.to_csv(csv_beamoff_name, sep=\" \", header=False, index=False)\n",
    "    \n",
    "    #signal\n",
    "    csv_file = csv_name\n",
    "    txt_file = csv_location + Params[\"Run\"] + f\"_signal_{HNL_mass}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "    with open(txt_file, \"w\") as my_output_file:\n",
    "        with open(csv_file, \"r\") as my_input_file:\n",
    "            [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "        my_output_file.close()\n",
    "        \n",
    "    #overlay\n",
    "    csv_file = csv_overlay_name\n",
    "    txt_file = csv_location + Params[\"Run\"] + f\"_overlay_{HNL_mass}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "    with open(txt_file, \"w\") as my_output_file:\n",
    "        with open(csv_file, \"r\") as my_input_file:\n",
    "            [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "        my_output_file.close()\n",
    "        \n",
    "    #dirt\n",
    "    csv_file = csv_dirt_name\n",
    "    txt_file = csv_location + Params[\"Run\"] + f\"_dirtoverlay_{HNL_mass}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "    with open(txt_file, \"w\") as my_output_file:\n",
    "        with open(csv_file, \"r\") as my_input_file:\n",
    "            [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "        my_output_file.close()\n",
    "        \n",
    "    #beamoff\n",
    "    csv_file = csv_beamoff_name\n",
    "    txt_file = csv_location + Params[\"Run\"] + f\"_beamoff_{HNL_mass}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "    with open(txt_file, \"w\") as my_output_file:\n",
    "        with open(csv_file, \"r\") as my_input_file:\n",
    "            [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "        my_output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275803f-0b98-43c4-9871-b8f16419b88f",
   "metadata": {},
   "source": [
    "# End of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3185ad5-5c66-4bca-9623-fbc3e9226383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old .pkl reading code\n",
    "\n",
    "# sample_test_dict = {}\n",
    "# for sample in samples:\n",
    "#     if sample in Constants.Detector_variations: #Reading in an overlay DetVar sample\n",
    "#         loc=loc_pkls+\"DetVars/\"+\"Preselected_overlay_\"+Params[\"Run\"]+\"_my_vars\"+f\"_{sample}_flattened_reduced_evs.pkl\"\n",
    "#         sample_test_dict[sample] = pd.read_pickle(loc)\n",
    "#     elif Params[\"Load_Signal_DetVars\"] == True:\n",
    "#         loc=loc_pkls+\"Signal_DetVars/\"+\"Preselected_\"+Params[\"Run\"]+f\"_{sample}_reduced_evs_final.pkl\"\n",
    "#         sample_test_dict[sample] = pd.read_pickle(loc)\n",
    "#     elif Params[\"Load_pi0_signal\"] == True:\n",
    "#         if sample == 'overlay':\n",
    "#             # sample_test_dict[sample] = pd.read_pickle(loc_pkls+\"BDT_Test_dfs/pi0_selection/Test_overlay_\"+Params[\"Run\"]+\".pkl\")\n",
    "#             sample_test_dict[sample] = pd.read_pickle(loc_pkls+\"BDT_Test_dfs/Test_overlay_\"+Params[\"Run\"]+\"_FINAL.pkl\")\n",
    "#         elif sample in pi0_sample_strings:\n",
    "#             # sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"BDT_Test_dfs/pi0_selection/Test_signal_{sample}_\"+Params[\"Run\"]+\"_FIXED.pkl\")\n",
    "#             sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"BDT_Test_dfs/pi0_selection/Test_signal_{sample}_\"+Params[\"Run\"]+\".pkl\")\n",
    "#         elif (sample == 'beamoff') and (Params[\"EXT_in_training\"] == True): #EXT only if extra EXT has been added\n",
    "#             sample_test_dict[sample] = pd.read_pickle(loc_pkls+\"BDT_Test_dfs/pi0_selection/Test_beamoff_\"+Params[\"Run\"]+\".pkl\")\n",
    "#         else: \n",
    "#             sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"Preselected_{sample}_\"+Params[\"Run\"]+\"_my_vars_flattened_FINAL.pkl\")\n",
    "#     else: #Standard sample types\n",
    "#         if sample == 'overlay':\n",
    "#             sample_test_dict[sample] = pd.read_pickle(loc_pkls+\"BDT_Test_dfs/Test_overlay_\"+Params[\"Run\"]+\"_FINAL.pkl\")\n",
    "#         elif (Params[\"Load_single_file\"] == True) and (isinstance(sample,int)):\n",
    "#             sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"BDT_Test_dfs/Test_signal_{sample}_\"+Params[\"Run\"]+\"_FINAL.pkl\")\n",
    "#         elif sample in Constants.HNL_mass_samples:\n",
    "#             sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"BDT_Test_dfs/Test_signal_{sample}_\"+Params[\"Run\"]+\"_FINAL.pkl\")\n",
    "#         elif (sample == 'beamoff') and (Params[\"EXT_in_training\"] == True): #EXT only if extra EXT has been added\n",
    "#             sample_test_dict[sample] = pd.read_pickle(loc_pkls+\"BDT_Test_dfs/Test_beamoff_\"+Params[\"Run\"]+\"_FINAL.pkl\")\n",
    "#         else: \n",
    "#             # sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"Preselected_{sample}_\"+Params[\"Run\"]+\"_my_vars_flattened_FINAL.pkl\")\n",
    "#             sample_test_dict[sample] = pd.read_pickle(loc_pkls+f\"Preselected_{sample}_\"+Params[\"Run\"]+\"_flattened_FINAL.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3e868-178b-4885-b310-cf1b587b5a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old BDT result saving code\n",
    "\n",
    "# # if Params[\"Run\"] == \"run1\": BDT_name = \"ultimate_full_reduced_benchmark\"\n",
    "# # if Params[\"Run\"] == \"run3\": BDT_name = \"ultimate_benchmark_plus_flashmatch\"\n",
    "# if (Params[\"Load_lepton_signal\"] == True) or (Params[\"Load_lepton_dirac\"] == True): \n",
    "#     BDT_name = \"ee_FINAL_2\"\n",
    "#     with open(f\"bdts/input_vars/{BDT_name}_\"+Params[\"Run\"], \"rb\") as fp:   # Unpickling\n",
    "#         saved_variables = pickle.load(fp)\n",
    "# if (Params[\"Load_pi0_signal\"] == True) or (Params[\"Load_pi0_dirac\"] == True): \n",
    "#     BDT_name = \"pi0_FINAL_2\"\n",
    "#     with open(f\"bdts/pi0_selection/input_vars/{BDT_name}_\"+Params[\"Run\"], \"rb\") as fp:   # Unpickling\n",
    "#         saved_variables = pickle.load(fp)\n",
    "\n",
    "# # if Params[\"Load_pi0_signal\"] == False:\n",
    "# #     with open(f\"bdts/input_vars/{BDT_name}_\"+Params[\"Run\"], \"rb\") as fp:   # Unpickling\n",
    "# #         saved_variables = pickle.load(fp)\n",
    "# # elif Params[\"Load_pi0_signal\"] == True:\n",
    "# #     with open(f\"bdts/pi0_selection/input_vars/{BDT_name}_\"+Params[\"Run\"], \"rb\") as fp:   # Unpickling\n",
    "# #         saved_variables = pickle.load(fp)\n",
    "\n",
    "# bdt_vars = saved_variables\n",
    "# xgb_test_dict = {}\n",
    "\n",
    "# for sample in sample_test_dict:\n",
    "#     xgb_test_dict[sample] = xgboost.DMatrix(sample_test_dict[sample][bdt_vars])\n",
    "#     # print(\"Done \" + str(sample))\n",
    "    \n",
    "# if Params[\"Load_single_file\"] == True:\n",
    "#     HNL_mass = Params[\"single_file\"]\n",
    "#     bdt = xgboost.Booster()\n",
    "#     bdt.load_model(f\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "#     for sample in xgb_test_dict:\n",
    "#         results = bdt.predict(xgb_test_dict[sample])\n",
    "#         sample_test_dict[sample][f\"BDT_output_{HNL_mass}MeV\"] = results\n",
    "    \n",
    "# elif Params[\"Load_lepton_signal\"] == True:\n",
    "#     for HNL_mass in Constants.HNL_mass_samples:\n",
    "#         bdt = xgboost.Booster()\n",
    "#         bdt.load_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "#         # bdt.load_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}_MeV_{BDT_name}.json\")\n",
    "#         for sample in xgb_test_dict:\n",
    "#             results = bdt.predict(xgb_test_dict[sample])\n",
    "#             sample_test_dict[sample][f\"BDT_output_{HNL_mass}MeV\"] = results\n",
    "            \n",
    "# elif Params[\"Load_pi0_signal\"] == True:\n",
    "#     for HNL_mass in Constants.HNL_mass_pi0_samples:\n",
    "#         bdt = xgboost.Booster()\n",
    "#         # bdt.load_model(f\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}_MeV_pi0_FIXED.json\")\n",
    "#         bdt.load_model(f\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "#         for sample in xgb_test_dict:\n",
    "#             results = bdt.predict(xgb_test_dict[sample])\n",
    "#             sample_test_dict[sample][f\"BDT_output_{HNL_mass}MeV\"] = results\n",
    "            \n",
    "# elif Params[\"Load_lepton_dirac\"] == True:\n",
    "#     for HNL_mass in Constants.HNL_ee_dirac_mass_samples:\n",
    "#         bdt = xgboost.Booster()\n",
    "#         # bdt.load_model(f\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}_MeV_pi0_FIXED.json\")\n",
    "#         bdt.load_model(f\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "#         for sample in xgb_test_dict:\n",
    "#             results = bdt.predict(xgb_test_dict[sample])\n",
    "#             sample_test_dict[sample][f\"BDT_output_{HNL_mass}MeV\"] = results\n",
    "            \n",
    "# elif Params[\"Load_pi0_dirac\"] == True:\n",
    "#     for HNL_mass in Constants.HNL_pi0_dirac_mass_samples:\n",
    "#         bdt = xgboost.Booster()\n",
    "#         # bdt.load_model(f\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}_MeV_pi0_FIXED.json\")\n",
    "#         bdt.load_model(f\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "#         for sample in xgb_test_dict:\n",
    "#             results = bdt.predict(xgb_test_dict[sample])\n",
    "#             sample_test_dict[sample][f\"BDT_output_{HNL_mass}MeV\"] = results\n",
    "\n",
    "# print(len(bdt_vars))\n",
    "# print(bdt_vars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
