{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe0cab0-578b-42ea-86af-07342852e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<pyhf.tensor.numpy_backend.numpy_backend object at 0x7f1dc57e1a40>, <pyhf.optimize.scipy_optimizer object at 0x7f1dc2056bb0>)\n",
      "pyhf version: 0.7.1\n",
      "Successful!\n"
     ]
    }
   ],
   "source": [
    "#Loading libraries\n",
    "import os, sys, string, time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import uproot\n",
    "import math\n",
    "import pickle\n",
    "import csv\n",
    "from importlib import reload\n",
    "import copy\n",
    "import pyhf\n",
    "import csv\n",
    "import matplotlib.ticker as mticker\n",
    "from datetime import date\n",
    "# from pyhf.contrib.viz import brazil\n",
    "\n",
    "import Utilities.Plotter as PT\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print(pyhf.get_backend())\n",
    "print(\"pyhf version:\",pyhf.__version__)\n",
    "\n",
    "print(\"Successful!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d9b75b",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook takes the output BDT score distributions and their uncertainties, saved in .root files, and calculates an upper limit on the HNL mixing angle using the CL_s method. <br>\n",
    "The limits are saved in .csv files. These .csv files are loaded in script 5, but also for the \"Brazil\" plot that is made in this script. <br>\n",
    "The calculations in this notebook can take a very long time (several hours on my local machine), depending on how sophisticated the pyhf models are and how many bins are used for the calculations. <br>\n",
    "Since the signal distributions are peaked at the high BDT scores, generally all of the sensitivity comes from the final few bins. Hence using 8 bins, you will usually get the same answer as using the full distributions. <br>\n",
    "As is stands the correlations between different sources of uncertainty are not fully accounted for here. This is due to not being natively available in pyhf. However, Pawel Guzowski has mentioned that he has found a way to \"hack\" these correlations in by using the covariance matrices and eigen vectors. I know nothing more about this, but it is worth asking him for his code if you properly want to account for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6939111e-d766-4112-ab60-1fd7c65239ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fully evaluated systematic uncertainty for background. Dirt error is 75.0%.\n",
      "Using fully evaluated systematic uncertainty for signal. Using 30.0% flux error.\n",
      "[2, 10, 20, 50, 100, 150]\n"
     ]
    }
   ],
   "source": [
    "Params_pyhf = {\"Stats_only\":False,\n",
    "               \"Use_flat_sys\":False,\n",
    "               \"Num_bins_for_calc\":8,\n",
    "               \"Use_part_only\":True,\n",
    "               \"Use_toys\":True,\n",
    "               \"Num_toys\":1000,\n",
    "               \"Load_lepton_hists\":True,\n",
    "               \"Load_pi0_hists\":False,\n",
    "               \"Flat_bkg_overlay_frac\":0.3,\n",
    "               \"Flat_bkg_dirt_frac\":0.75,\n",
    "               \"Flat_bkg_EXT_frac\":0.0,\n",
    "               \"Flat_sig_detvar\":0.2, #This is very conservative, could be fed in per mass point from signal detvar script\n",
    "               \"Signal_flux_error\":0.3, #This comes from the KDAR flux uncertainty.\n",
    "               \"Overlay_detvar_frac\":0.3,\n",
    "               \"Load_lepton_dirac\":False,\n",
    "               \"Load_pi0_dirac\":False,\n",
    "               \"Load_single_r1_file\":False}\n",
    "\n",
    "scaled = False #Just to keep track of if the histograms have been scaled\n",
    "\n",
    "if Params_pyhf[\"Load_lepton_hists\"] ==True: \n",
    "    name_type=\"ee\"\n",
    "    HNL_masses_list = Constants.HNL_mass_samples\n",
    "if Params_pyhf[\"Load_pi0_hists\"] ==True: \n",
    "    name_type=\"pi0\"\n",
    "    HNL_masses_list = Constants.HNL_mass_pi0_samples\n",
    "if Params_pyhf[\"Load_lepton_dirac\"] ==True: \n",
    "    name_type=\"ee_dirac\"\n",
    "    HNL_masses_list = Constants.HNL_ee_dirac_mass_samples\n",
    "if Params_pyhf[\"Load_pi0_dirac\"] ==True: \n",
    "    name_type=\"pi0_dirac\"\n",
    "    HNL_masses_list = Constants.HNL_pi0_dirac_mass_samples\n",
    "BDT_name = \"_full_Finished_10\"\n",
    "# filename = name_type+'_EXT_full_Finished'\n",
    "filename = name_type+BDT_name\n",
    "\n",
    "Functions.pyhf_params(Params_pyhf)\n",
    "\n",
    "print(HNL_masses_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431c728d-46f7-48d0-8cf8-e76c5fa5738f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing histograms in Run1\n",
      "thetas are:\n",
      "{2: 0.1, 10: 0.01, 20: 1e-04, 50: 1e-04, 100: 1e-04, 150: 1e-04}\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# HNL_masses_list = [150,200,245]\n",
    "\n",
    "hist_dict_run1, hist_dict_run3, theta_dict = Functions.New_Load_pyhf_files(f\"{filename}.root\",\n",
    "                                                                           Params_pyhf, HNL_masses = HNL_masses_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2e45e-7b0c-45cc-a21a-65bfd2fafdc3",
   "metadata": {},
   "source": [
    "## Creating dictionaries containing all necessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c966e86-eb23-4594-a4fc-a03c6fe96440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['TOT_BKG_VALS', 'TOT_SIGNAL_VALS', 'OVERLAY_VALS', 'DIRT_VALS', 'BEAMOFF_VALS', 'OVERLAY_STAT', 'DIRT_STAT', 'BEAMOFF_STAT', 'TOT_BKG_ERR', 'TOT_SIGNAL_ERR', 'BKG_STAT', 'BKG_SHAPESYS', 'BKG_DETVAR', 'BKG_DIRT', 'BKG_MULTISIM', 'BKG_DETVAR_MULTISIM', 'SIGNAL_STAT', 'SIGNAL_SHAPESYS', 'SIGNAL_DETVAR', 'data', 'BKG_DIRT_R1', 'BKG_DIRT_R3'])\n"
     ]
    }
   ],
   "source": [
    "reload(Functions)\n",
    "zero_bins_errors_run1 = Functions.make_zero_bin_unc(hist_dict_run1, Constants.run1_POT_scaling_dict, Params_pyhf)\n",
    "if (Params_pyhf[\"Load_lepton_dirac\"]) or (Params_pyhf[\"Load_pi0_dirac\"]): SF_dict_run3 = Constants.run1_POT_scaling_dict\n",
    "else: SF_dict_run3 = Constants.run3_POT_scaling_dict\n",
    "zero_bins_errors_run3 = Functions.make_zero_bin_unc(hist_dict_run3, SF_dict_run3, Params_pyhf)\n",
    "\n",
    "TOT_R1_ERR = Functions.Full_calculate_total_uncertainty(Params_pyhf, hist_dict_run1, zero_bins_errors_run1)\n",
    "TOT_R3_ERR = Functions.Full_calculate_total_uncertainty(Params_pyhf, hist_dict_run3, zero_bins_errors_run3)\n",
    "\n",
    "R1_BKG, R1_SIGNAL = Functions.Add_bkg_hists_make_signal(hist_dict_run1)\n",
    "R3_BKG, R3_SIGNAL = Functions.Add_bkg_hists_make_signal(hist_dict_run3)\n",
    "\n",
    "R1_output = Functions.Make_into_lists(Params_pyhf, R1_BKG, R1_SIGNAL, TOT_R1_ERR)\n",
    "R3_output = Functions.Make_into_lists(Params_pyhf, R3_BKG, R3_SIGNAL, TOT_R3_ERR)\n",
    "\n",
    "list_input_dicts = [R1_output, R3_output]\n",
    "\n",
    "Total_dict_both = Functions.Create_final_appended_runs_dict(list_input_dicts)\n",
    "Total_dict_run1 = Functions.Create_final_appended_runs_dict([R1_output])\n",
    "Total_dict_run3 = Functions.Create_final_appended_runs_dict([R3_output])\n",
    "\n",
    "if Params_pyhf[\"Use_part_only\"]==True:\n",
    "    NUMBINS = Params_pyhf[\"Num_bins_for_calc\"]\n",
    "else: NUMBINS=30 #This will just give the full hist\n",
    "    \n",
    "if 'data;1' in hist_dict_run1[150]:\n",
    "    Total_dict_run1=Functions.add_data(Total_dict_run1, hist_dict_run1, NUMBINS)\n",
    "    Total_dict_run3=Functions.add_data(Total_dict_run3, hist_dict_run3, NUMBINS)\n",
    "    Total_dict_both=Functions.add_data_appended(Total_dict_both, hist_dict_run1, hist_dict_run3, NUMBINS)\n",
    "    \n",
    "#Create separate dirt normalisation uncertainties for Run1 and Run3 \n",
    "for HNL_mass in Total_dict_both:\n",
    "\n",
    "    dirt_vals = Total_dict_both[HNL_mass]['BKG_DIRT']\n",
    "    numbins = int(len(dirt_vals)/2)\n",
    "    r1_dirt = dirt_vals[:numbins] + list(np.zeros(numbins))\n",
    "    r3_dirt = list(np.zeros(numbins)) + dirt_vals[numbins:] \n",
    "    Total_dict_both[HNL_mass]['BKG_DIRT_R1'] = r1_dirt\n",
    "    Total_dict_both[HNL_mass]['BKG_DIRT_R3'] = r3_dirt\n",
    "    \n",
    "print(Total_dict_both[150].keys())\n",
    "\n",
    "sig_stat, bkg_stat = Functions.create_stat_unc_safe(Total_dict_both)\n",
    "\n",
    "sig_stat_r1, bkg_stat_r1 = Functions.create_stat_unc_safe(Total_dict_run1)\n",
    "sig_stat_r3, bkg_stat_r3 = Functions.create_stat_unc_safe(Total_dict_run3)\n",
    "\n",
    "sig_stat, overlay_stat, dirt_stat, beamoff_stat = Functions.create_individual_stat_unc_safe(Total_dict_both)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2375f3f-d469-410b-9b5f-017e5d28d82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOT_BKG_VALS 16\n",
      "TOT_SIGNAL_VALS 16\n",
      "OVERLAY_VALS 16\n",
      "DIRT_VALS 16\n",
      "BEAMOFF_VALS 16\n",
      "OVERLAY_STAT 16\n",
      "DIRT_STAT 16\n",
      "BEAMOFF_STAT 16\n",
      "TOT_BKG_ERR 16\n",
      "TOT_SIGNAL_ERR 16\n",
      "BKG_STAT 16\n",
      "BKG_SHAPESYS 16\n",
      "BKG_DETVAR 16\n",
      "BKG_DIRT 16\n",
      "BKG_MULTISIM 16\n",
      "BKG_DETVAR_MULTISIM 16\n",
      "SIGNAL_STAT 16\n",
      "SIGNAL_SHAPESYS 16\n",
      "SIGNAL_DETVAR 16\n",
      "data 16\n",
      "BKG_DIRT_R1 16\n",
      "BKG_DIRT_R3 16\n"
     ]
    }
   ],
   "source": [
    "for hist in Total_dict_both[150].keys():\n",
    "    print(hist+ \" \" + str(len(Total_dict_both[150][hist])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b092e9e5-a61e-448a-97f5-2a5bee784aeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Making uncertainty breakdown plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c7c8d-4915-4c9c-abf9-bcdc206025cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 150\n",
    "hist_dict_run3[HNL_mass].keys()\n",
    "Uncertainties_list = [\"Statistics\", \"Detector Modelling\", r\"$\\nu$ Flux\", r\"$\\nu$ Cross Section\", \"Reinteractions\", r\"Out-Cryo $\\nu$ Normalisation\", \n",
    "                      \"Flux Rate\", \"Total\"]\n",
    "Unc_colors = {\"Statistics\":\"black\", \"Detector Modelling\":\"C1\", r\"$\\nu$ Flux\":\"C2\", r\"$\\nu$ Cross Section\":\"C3\",\n",
    "              \"Reinteractions\":\"C4\", r\"Out-Cryo $\\nu$ Normalisation\":\"C5\", \"Flux Rate\":\"C2\", \"Total\":\"gray\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17113cd8-a250-49c8-bd6b-2e2f5ddf978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bins_dicts(hist_dict, Params):\n",
    "    \"\"\"\n",
    "    Given a histogram dict returns the bins and bin centres dictionaries.\n",
    "    \"\"\"\n",
    "    bins_dict, bins_cents_dict = {}, {}\n",
    "    if Params[\"Use_part_only\"] == False:\n",
    "        for HNL_mass in hist_dict:\n",
    "            bins = hist_dict[HNL_mass]['bkg_overlay'].to_numpy()[1]\n",
    "            bin_cents = (bins[:-1]+bins[1:])/2\n",
    "            bins_dict[HNL_mass] = bins\n",
    "            bins_cents_dict[HNL_mass] = bin_cents\n",
    "    if Params[\"Use_part_only\"] == True:\n",
    "        for HNL_mass in hist_dict:\n",
    "            Num_bins=Params[\"Num_bins_for_calc\"]\n",
    "            bins = hist_dict[HNL_mass]['bkg_overlay'].to_numpy()[1]\n",
    "            bins = bins[-1*(Num_bins+1):]\n",
    "            bin_cents = (bins[:-1]+bins[1:])/2\n",
    "            bins_dict[HNL_mass] = bins\n",
    "            bins_cents_dict[HNL_mass] = bin_cents\n",
    "            \n",
    "    return bins_dict, bins_cents_dict\n",
    "    \n",
    "bins_dict_r1, bins_cent_dict_r1 = make_bins_dicts(hist_dict_run1, Params_pyhf)\n",
    "bins_dict_r3, bins_cent_dict_r3 = make_bins_dicts(hist_dict_run3, Params_pyhf)\n",
    "\n",
    "bins_overflow_r1, bins_cents_overflow_r1 = Functions.make_overflow_bin(bins_dict_r1, bins_cent_dict_r1)\n",
    "bins_overflow_r3, bins_cents_overflow_r3 = Functions.make_overflow_bin(bins_dict_r3, bins_cent_dict_r3)\n",
    "\n",
    "def make_xlims_dict(bins_dict, spacing, lower = None):\n",
    "    \"\"\"\n",
    "    Making a dict of xlims for plotting several mass points at once.\n",
    "    Also returns a dict of xticks for the purpose of indicating the overflow.\n",
    "    \"\"\"\n",
    "    xlims_adjusted, xticks_adjusted = {}, {}\n",
    "    vals_dict={}\n",
    "    for HNL_mass in bins_dict:\n",
    "        if isinstance(lower,(int, float)): lower_val = lower\n",
    "        else: lower_val = bins_dict[HNL_mass][0]\n",
    "        xlims_adjusted[HNL_mass] = [lower_val,bins_dict[HNL_mass][-1]]\n",
    "        ticks = np.arange(bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1], spacing)\n",
    "        if ticks[-1] != bins_dict[HNL_mass][-2]: ticks = np.append(ticks, bins_dict[HNL_mass][-1]-1)\n",
    "        ticks_strings = []\n",
    "        vals = []\n",
    "        for val in ticks:\n",
    "            ticks_strings.append(str(int(val)))\n",
    "            vals.append(val)\n",
    "        ticks_strings[-1] = str(ticks_strings[-1])+\"+\"\n",
    "        xticks_adjusted[HNL_mass] = ticks_strings\n",
    "        vals_dict[HNL_mass] = vals\n",
    "        \n",
    "    return xlims_adjusted, xticks_adjusted, vals_dict\n",
    "\n",
    "xlims_dict_r1, xticks_dict_r1, vals_dict_r1 = make_xlims_dict(bins_overflow_r1, 1)\n",
    "xlims_dict_r3, xticks_dict_r3, vals_dict_r3 = make_xlims_dict(bins_overflow_r3, 1)\n",
    "\n",
    "def Make_hist_dict_from_Total_dict(Total_dict):\n",
    "    \"\"\"\n",
    "    Creating a dict of just hists from Total dict.\n",
    "    For the purposes of creating bins.\n",
    "    \"\"\"\n",
    "    bins_dict, bins_cents_dict = {}, {}\n",
    "    for HNL_mass in Total_dict:\n",
    "        bins = np.arange(len(Total_dict[HNL_mass]['TOT_BKG_VALS'])+1)\n",
    "        bin_cents = (bins[:-1]+bins[1:])/2\n",
    "        bins_dict[HNL_mass] = bins\n",
    "        bins_cents_dict[HNL_mass] = bin_cents\n",
    "        \n",
    "    return bins_dict, bins_cents_dict\n",
    "\n",
    "Total_bins_dict, Total_bins_cent_dict = Make_hist_dict_from_Total_dict(Total_dict_both)\n",
    "Total_bins_overflow, Total_bins_cents_overflow = Functions.make_overflow_bin(Total_bins_dict, Total_bins_cent_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bdbe24-bc7c-40be-8db8-1c4e6656429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Functions)\n",
    "reload(PT)\n",
    "\n",
    "sig_stat, bkg_stat = Functions.create_stat_unc_safe(Total_dict_both)\n",
    "\n",
    "sig_stat_r1, bkg_stat_r1 = Functions.create_stat_unc_safe(Total_dict_run1)\n",
    "sig_stat_r3, bkg_stat_r3 = Functions.create_stat_unc_safe(Total_dict_run3)\n",
    "\n",
    "sig_stat, overlay_stat, dirt_stat, beamoff_stat = Functions.create_individual_stat_unc_safe(Total_dict_both)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198406b-55f8-42ad-b0dd-8eab81934653",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(PT)\n",
    "\n",
    "Run = \"run3\"\n",
    "\n",
    "if Run == \"run1\": \n",
    "    hist_dict, bins_overflow, bins_cents = hist_dict_run1, bins_overflow_r1, bins_cents_overflow_r1\n",
    "    bkg_stat_to_use, sig_stat_to_use = bkg_stat_r1, sig_stat_r1\n",
    "    xticks_dict, vals_dict = xticks_dict_r1, vals_dict_r1\n",
    "if Run == \"run3\": \n",
    "    hist_dict, bins_overflow, bins_cents = hist_dict_run3, bins_overflow_r3, bins_cents_overflow_r3\n",
    "    bkg_stat_to_use, sig_stat_to_use = bkg_stat_r3, sig_stat_r3\n",
    "    xticks_dict, vals_dict = xticks_dict_r3, vals_dict_r3\n",
    "\n",
    "PT.plot_bkg_total_unc_contributions(hist_dict, bkg_stat_to_use, bins_overflow, bins_cents, xticks_dict, vals_dict,\n",
    "                                    Params_pyhf, Unc_colors, Run=Run, name_type=name_type, plot_total=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8809f7b6-c9f8-4111-9f91-a82f29596882",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(PT)\n",
    "\n",
    "PT.plot_signal_total_unc_contributions(hist_dict, sig_stat_to_use, bins_overflow, bins_cents, xticks_dict, vals_dict, \n",
    "                                       Params_pyhf,Unc_colors, Run=Run, name_type=name_type, \n",
    "                                       KDAR_unc=Params_pyhf[\"Signal_flux_error\"], plot_total=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f989ea01-4670-4ccd-9feb-cbc0cf3cc1c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Looking at background composition in the signal-region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335943c9-322e-4680-956a-57f81f46a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 10\n",
    "#hist_dict_run1\n",
    "\n",
    "signal_region = 14 #number of bins to define as signal region\n",
    "\n",
    "def Get_bkg_composition(hist_dict, signal_region):\n",
    "\n",
    "    in_cryo_frac, out_cryo_frac, beam_off_frac = {}, {}, {}\n",
    "    in_cryo_plus_out = {}\n",
    "    \n",
    "    for HNL_mass in hist_dict:\n",
    "        tot_bkg = hist_dict[HNL_mass]['bkg_overlay'].values()+hist_dict[HNL_mass]['bkg_dirt'].values()+hist_dict[HNL_mass]['bkg_EXT'].values()\n",
    "\n",
    "        frac_in_cryo = hist_dict[HNL_mass]['bkg_overlay'].values()[-1*signal_region:]/tot_bkg[-1*signal_region:]\n",
    "        frac_out_cryo = hist_dict[HNL_mass]['bkg_dirt'].values()[-1*signal_region:]/tot_bkg[-1*signal_region:]\n",
    "        frac_beam_off = hist_dict[HNL_mass]['bkg_EXT'].values()[-1*signal_region:]/tot_bkg[-1*signal_region:]\n",
    "\n",
    "        av_frac_in_cryo = sum(hist_dict[HNL_mass]['bkg_overlay'].values()[-1*signal_region:])/sum(tot_bkg[-1*signal_region:])\n",
    "        av_frac_out_cryo = sum(hist_dict[HNL_mass]['bkg_dirt'].values()[-1*signal_region:])/sum(tot_bkg[-1*signal_region:])\n",
    "        av_frac_beam_off = sum(hist_dict[HNL_mass]['bkg_EXT'].values()[-1*signal_region:])/sum(tot_bkg[-1*signal_region:])\n",
    "\n",
    "        # print(f\"In-Cryo fraction is {av_frac_in_cryo}\")\n",
    "        # print(f\"Out-Cryo fraction is {av_frac_out_cryo}\")\n",
    "        # print(f\"Beam-off fraction is {av_frac_beam_off}\")\n",
    "        \n",
    "        in_cryo_frac[HNL_mass] = av_frac_in_cryo\n",
    "        out_cryo_frac[HNL_mass] = av_frac_out_cryo\n",
    "        beam_off_frac[HNL_mass] = av_frac_beam_off\n",
    "        in_cryo_plus_out[HNL_mass] = av_frac_in_cryo+av_frac_out_cryo\n",
    "        \n",
    "    return in_cryo_frac, out_cryo_frac, beam_off_frac, in_cryo_plus_out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40927b5-e868-4688-a024-5e528e61b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 150\n",
    "\n",
    "#Run1 \n",
    "r1_in_cryo, r1_out_cryo, r1_beam_off, r1_in_out = Get_bkg_composition(hist_dict_run1, signal_region)\n",
    "r3_in_cryo, r3_out_cryo, r3_beam_off, r3_in_out = Get_bkg_composition(hist_dict_run3, signal_region)\n",
    "\n",
    "print(\"Run 1:\")\n",
    "print(f\"In-cryo frac: {r1_in_cryo[HNL_mass]}\")\n",
    "print(f\"Out-cryo frac: {r1_out_cryo[HNL_mass]}\")\n",
    "print(f\"Beam-off frac: {r1_beam_off[HNL_mass]}\")\n",
    "\n",
    "print(\"Run 3:\")\n",
    "print(f\"In-cryo frac: {r3_in_cryo[HNL_mass]}\")\n",
    "print(f\"Out-cryo frac: {r3_out_cryo[HNL_mass]}\")\n",
    "print(f\"Beam-off frac: {r3_beam_off[HNL_mass]}\")\n",
    "\n",
    "r1_sum_in_cryo_fracs, r1_sum_out_cryo_fracs, r1_sum_beamoff_fracs = 0, 0, 0\n",
    "for HNL_mass in r1_in_cryo:\n",
    "    r1_sum_in_cryo_fracs+=r1_in_cryo[HNL_mass]\n",
    "    r1_sum_out_cryo_fracs+=r1_out_cryo[HNL_mass]\n",
    "    r1_sum_beamoff_fracs+=r1_beam_off[HNL_mass]\n",
    "    \n",
    "av_r1_in_cryo = r1_sum_in_cryo_fracs/(len(r1_in_cryo))\n",
    "av_r1_out_cryo = r1_sum_out_cryo_fracs/(len(r1_in_cryo))\n",
    "av_r1_beam_off = r1_sum_beamoff_fracs/(len(r1_in_cryo))\n",
    "\n",
    "r3_sum_in_cryo_fracs, r3_sum_out_cryo_fracs, r3_sum_beamoff_fracs = 0, 0, 0\n",
    "for HNL_mass in r3_in_cryo:\n",
    "    r3_sum_in_cryo_fracs+=r3_in_cryo[HNL_mass]\n",
    "    r3_sum_out_cryo_fracs+=r3_out_cryo[HNL_mass]\n",
    "    r3_sum_beamoff_fracs+=r3_beam_off[HNL_mass]\n",
    "    \n",
    "av_r3_in_cryo = r3_sum_in_cryo_fracs/(len(r3_in_cryo))\n",
    "av_r3_out_cryo = r3_sum_out_cryo_fracs/(len(r3_in_cryo))\n",
    "av_r3_beam_off = r3_sum_beamoff_fracs/(len(r3_in_cryo))\n",
    "\n",
    "print()\n",
    "print(\"Run1 averages\")\n",
    "print(f\"In-Cryo {av_r1_in_cryo}\")\n",
    "print(f\"Out-Cryo {av_r1_out_cryo}\")\n",
    "print(f\"Beam-off {av_r1_beam_off}\")\n",
    "\n",
    "print(av_r1_in_cryo+av_r1_out_cryo+av_r1_beam_off)\n",
    "\n",
    "print()\n",
    "print(\"Run3 averages\")\n",
    "print(f\"In-Cryo {av_r3_in_cryo}\")\n",
    "print(f\"Out-Cryo {av_r3_out_cryo}\")\n",
    "print(f\"Beam-off {av_r3_beam_off}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec24a0-e892-4c55-a0a5-d5b93b27779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for HNL_mass in r1_in_cryo.keys():\n",
    "\n",
    "x_int_list = range(0, len(r1_in_cryo.keys()))\n",
    "plt.bar(x_int_list, r1_in_cryo.values(), label=r\"In-Cryo $\\nu$\", tick_label=list(r1_in_cryo.keys()))\n",
    "plt.bar(x_int_list, r1_out_cryo.values(), label=r\"Out-Cryo $\\nu$\", tick_label=list(r1_in_cryo.keys()), bottom=list(r1_in_cryo.values()))\n",
    "plt.bar(x_int_list, r1_beam_off.values(), label=r\"Beam-off\", tick_label=list(r1_in_cryo.keys()),bottom=list(r1_in_out.values()))\n",
    "\n",
    "plt.axhline(1.0,ls=\"--\", color=\"black\")\n",
    "plt.ylabel(\"Fraction of events\")\n",
    "plt.xlabel(\"HNL mass [MeV]\")\n",
    "plt.xlim(-0.5, 8)\n",
    "plt.legend(fontsize=22, loc=\"center right\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199e0a7-4f37-44cc-b073-595e33b3bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_int_list = range(0, len(r3_in_cryo.keys()))\n",
    "plt.bar(x_int_list, r3_in_cryo.values(), label=r\"In-Cryo $\\nu$\", tick_label=list(r3_in_cryo.keys()))\n",
    "plt.bar(x_int_list, r3_out_cryo.values(), label=r\"Out-Cryo $\\nu$\", tick_label=list(r3_in_cryo.keys()), bottom=list(r3_in_cryo.values()))\n",
    "plt.bar(x_int_list, r3_beam_off.values(), label=r\"Beam-off\", tick_label=list(r3_in_cryo.keys()),bottom=list(r3_in_out.values()))\n",
    "\n",
    "plt.axhline(1.0,ls=\"--\", color=\"black\")\n",
    "plt.ylabel(\"Fraction of events\")\n",
    "plt.xlabel(\"HNL mass [MeV]\")\n",
    "plt.xlim(-0.5, 8)\n",
    "plt.legend(fontsize=22, loc=\"center right\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd6b13-1435-4cdb-a37a-6ccddd5722be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scaling signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9c1437b-4f00-499a-afb3-c0e2cc5bd296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_signal(Total_dict, theta_dict, scaling_dict={}):\n",
    "    \"\"\"\n",
    "    Scales the number of events by the number in the scaling dict.\n",
    "    Returns the new dict of histograms and the new thetas.\n",
    "    \"\"\"\n",
    "    if(scaling_dict=={}): raise Exception(\"Specify scalings\")\n",
    "    Total_dict_scaled, new_theta_dict = copy.deepcopy(Total_dict), {}\n",
    "    for HNL_mass in Total_dict.keys():\n",
    "        \n",
    "        new_signal_hist = np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])*scaling_dict[HNL_mass]\n",
    "        new_signal_err_hist = np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR'])*scaling_dict[HNL_mass]\n",
    "        new_signal_stat_err = np.array(Total_dict[HNL_mass]['SIGNAL_STAT'])*scaling_dict[HNL_mass]\n",
    "        new_signal_shapesys = np.array(Total_dict[HNL_mass]['SIGNAL_SHAPESYS'])*scaling_dict[HNL_mass]\n",
    "        new_signal_detvar = np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR'])*scaling_dict[HNL_mass]\n",
    "        new_theta = theta_dict[HNL_mass]*scaling_dict[HNL_mass]**(1/4) # Number of events is proportional to theta**4\n",
    "        \n",
    "        Total_dict_scaled[HNL_mass]['TOT_SIGNAL_VALS'] = list(new_signal_hist)\n",
    "        Total_dict_scaled[HNL_mass]['TOT_SIGNAL_ERR'] = list(new_signal_err_hist)\n",
    "        Total_dict_scaled[HNL_mass]['SIGNAL_STAT'] = list(new_signal_stat_err)\n",
    "        Total_dict_scaled[HNL_mass]['SIGNAL_SHAPESYS'] = list(new_signal_shapesys)\n",
    "        Total_dict_scaled[HNL_mass]['SIGNAL_DETVAR'] = list(new_signal_detvar)\n",
    "        \n",
    "        new_theta_dict[HNL_mass] = new_theta\n",
    "        \n",
    "    return Total_dict_scaled, new_theta_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fad4601-b177-49a6-93d0-a7e4387d6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_SF = 2000\n",
    "if (Params_pyhf[\"Load_lepton_hists\"]) or (Params_pyhf[\"Load_lepton_dirac\"]):\n",
    "    scaling_dict = {2:5000,10:2000,20:5e9,50:2e7,100:2e5,150:2e4} #Scaling for both r1 and r3 combined\n",
    "    scaling_dict_r1 = {2:5000,10:10000,20:2e9,50:5e6,100:1e6,150:5e4}\n",
    "    scaling_dict_r3 = {2:5000,10:2000,20:5e9,50:2e7,100:2e5,150:2e4}\n",
    "elif Params_pyhf[\"Load_pi0_hists\"]==True or Params_pyhf[\"Load_pi0_dirac\"]==True:\n",
    "    # scaling_dict = {150:100,180:10,200:5,220:2,240:2,245:2}\n",
    "    scaling_dict = {150:500,180:50,200:25,220:10,240:10,245:10}\n",
    "    scaling_dict_r1 = {150:500,180:50,200:25,220:10,240:10,245:10}\n",
    "    scaling_dict_r3 = {150:500,180:50,200:25,220:10,240:10,245:10}\n",
    "elif Params_pyhf[\"Load_single_r1_file\"]==True: #Currently using this for pi0 Dirac samples\n",
    "    scaling_dict = {150:1000,180:50,200:50,220:102,240:20,245:20}\n",
    "scaled=True\n",
    "\n",
    "Total_dict, theta_dict_scaled  = scale_signal(Total_dict_both, theta_dict, scaling_dict)\n",
    "Total_dict_run1_scaled, theta_dict_scaled_r1  = scale_signal(Total_dict_run1, theta_dict, scaling_dict_r1)\n",
    "Total_dict_run3_scaled, theta_dict_scaled_r3  = scale_signal(Total_dict_run3, theta_dict, scaling_dict_r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "166e1e7a-7f27-4d2a-b1e1-efde4c7f5745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['TOT_BKG_VALS', 'TOT_SIGNAL_VALS', 'OVERLAY_VALS', 'DIRT_VALS', 'BEAMOFF_VALS', 'OVERLAY_STAT', 'DIRT_STAT', 'BEAMOFF_STAT', 'TOT_BKG_ERR', 'TOT_SIGNAL_ERR', 'BKG_STAT', 'BKG_SHAPESYS', 'BKG_DETVAR', 'BKG_DIRT', 'BKG_MULTISIM', 'BKG_DETVAR_MULTISIM', 'SIGNAL_STAT', 'SIGNAL_SHAPESYS', 'SIGNAL_DETVAR', 'data', 'BKG_DIRT_R1', 'BKG_DIRT_R3'])\n"
     ]
    }
   ],
   "source": [
    "print(Total_dict[150].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae12345-a8e7-471b-926a-bea2bc7a2c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plotting example Total hist after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1d9e985-2c90-457e-bde4-7b60889d0a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 150\n",
    "theta = theta_dict_scaled[HNL_mass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b68d6513-ed99-432d-90e7-79f05c62fc7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['font.size'] = 16\n",
    "plt.figure(figsize=(7,4),facecolor='white',dpi=100)\n",
    "\n",
    "num_bins = len(Total_dict[HNL_mass]['TOT_BKG_VALS'])\n",
    "\n",
    "bins = np.arange(0,num_bins+1,1)\n",
    "bins_cents=(bins[:-1]+bins[1:])/2\n",
    "plt.hist(bins_cents, weights=Total_dict[HNL_mass]['TOT_SIGNAL_VALS'], bins=bins, histtype=\"step\", lw=2, label=f\"Signal \\n\" + fr\"{HNL_mass}MeV $\\theta$={theta:.5f}\")\n",
    "plt.hist(bins_cents, weights=Total_dict[HNL_mass]['TOT_BKG_VALS'], bins=bins, histtype=\"step\",lw=2, label=\"Background\")\n",
    "\n",
    "bkg_up=np.append((Total_dict[HNL_mass]['TOT_BKG_VALS']+np.array(Total_dict[HNL_mass]['TOT_BKG_ERR'])),(Total_dict[HNL_mass]['TOT_BKG_VALS']+np.array(Total_dict[HNL_mass]['TOT_BKG_ERR']))[-1])\n",
    "bkg_down=np.append((Total_dict[HNL_mass]['TOT_BKG_VALS']-np.array(Total_dict[HNL_mass]['TOT_BKG_ERR'])),(Total_dict[HNL_mass]['TOT_BKG_VALS']-np.array(Total_dict[HNL_mass]['TOT_BKG_ERR']))[-1])\n",
    "sig_up=np.append((Total_dict[HNL_mass]['TOT_SIGNAL_VALS']+np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR'])),(Total_dict[HNL_mass]['TOT_SIGNAL_VALS']+np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR']))[-1])\n",
    "sig_down=np.append((Total_dict[HNL_mass]['TOT_SIGNAL_VALS']-np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR'])),(Total_dict[HNL_mass]['TOT_SIGNAL_VALS']-np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR']))[-1])\n",
    "\n",
    "plt.fill_between(bins, bkg_down, bkg_up,step=\"post\",hatch='///',alpha=0,zorder=2)\n",
    "plt.fill_between(bins, sig_down, sig_up,step=\"post\",hatch='///',alpha=0,zorder=2)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"BDT score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d44019-e751-4595-b5c9-6166719fa92f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating model (only do once happy with scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d0f27d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ee\n"
     ]
    }
   ],
   "source": [
    "print(name_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3661ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_json(Total_dict, Params_pyhf, sig_stat, name_type, HNL_mass, JSON_name=\"Test\"):\n",
    "  \"\"\"Save a .json file that can be loaded directly for use in a pyhf model.\"\"\"\n",
    "  sig_norm = {\"hi\": 1.0+Params_pyhf[\"Signal_flux_error\"], \"lo\": 1.0-Params_pyhf[\"Signal_flux_error\"]}\n",
    "  dirt_norm = {\"hi\": 1.0+Params_pyhf[\"Flat_bkg_dirt_frac\"], \"lo\": 1.0-Params_pyhf[\"Flat_bkg_dirt_frac\"]}\n",
    "  \n",
    "  JSON_to_save = {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be scanned over in the hypo tests\n",
    "                {\"name\": \"stat_siguncrt\", \"type\": \"staterror\", \"data\": sig_stat[HNL_mass]},\n",
    "                {\"name\": \"norm_siguncrt\", \"type\": \"normsys\", \"data\": sig_norm}, #NuMI absorber KDAR rate\n",
    "                {\"name\": \"Detvar_sig\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['SIGNAL_DETVAR']} #shapesys assumes uncorrelated\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"overlay\",\n",
    "              \"data\": Total_dict[HNL_mass]['OVERLAY_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_overlay\", \"type\": \"staterror\", \"data\": overlay_stat[HNL_mass]},\n",
    "                {\"name\": \"Multisim_overlay\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_MULTISIM']},\n",
    "                {\"name\": \"Detvar_overlay\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_DETVAR']} #shapesys assumes uncorrelated\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"dirt\",\n",
    "              \"data\": Total_dict[HNL_mass]['DIRT_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_dirt\", \"type\": \"staterror\", \"data\": dirt_stat[HNL_mass]},\n",
    "                {\"name\": \"dirt_norm\", \"type\": \"normsys\", \"data\": dirt_norm}\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"beamoff\",\n",
    "              \"data\": Total_dict[HNL_mass]['BEAMOFF_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_beamoff\", \"type\": \"staterror\", \"data\": beamoff_stat[HNL_mass]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "\n",
    "  with open(f'./jsons/{HNL_mass}_{name_type}_{JSON_name}.json', 'w') as f:\n",
    "      json.dump(JSON_to_save, f, ensure_ascii=False)\n",
    "      print(f\"Saved as \" + f'./jsons/{HNL_mass}_{name_type}_{JSON_name}.json')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "861f1dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as ./jsons/10_ee_Test.json\n"
     ]
    }
   ],
   "source": [
    "Make_json(Total_dict, Params_pyhf, sig_stat, name_type, 10, JSON_name=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6617d0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'channels': [{'name': 'singlechannel', 'samples': [{'name': 'signal', 'data': [0.15326810535043478, 0.25642933906055987, 0.48927898751571774, 0.6572843994945288, 1.03161227889359, 1.5297336503863335, 2.0190125796943903, 4.872157238423824, 0.49950770335271955, 0.9301177924498916, 1.3894352596253157, 2.0726697985082865, 3.4104317892342806, 4.466861952096224, 4.908955190330744, 8.658133447170258], 'modifiers': [{'name': 'mu', 'type': 'normfactor', 'data': None}, {'name': 'stat_siguncrt', 'type': 'staterror', 'data': [0.13867505442238315, 0.10721125388832235, 0.07761504989177741, 0.06696495310051545, 0.05345224882684524, 0.04389512809466395, 0.03820803675328124, 0.024595949518437136, 0.10721125237867819, 0.07856741932198838, 0.06428243219539191, 0.05263158086831404, 0.04103049797076055, 0.035851736911014793, 0.03419927683377842, 0.025751310096627306]}, {'name': 'norm_siguncrt', 'type': 'normsys', 'data': {'hi': 1.3, 'lo': 0.7}}, {'name': 'Detvar_sig', 'type': 'shapesys', 'data': [0.030653628378479736, 0.051285880039615706, 0.09785582083378375, 0.13145691124066827, 0.20632250496982474, 0.30594680302065136, 0.40380261221290015, 0.974431680007326, 0.0999015406705439, 0.18602355848997829, 0.25653600698310597, 0.4145339597016572, 0.40974048231347326, 0.394424712405031, 0.541560698124444, 1.1812105254258414]}]}, {'name': 'overlay', 'data': [664.782958984375, 328.790771484375, 153.85205078125, 84.567626953125, 45.826416015625, 22.9365234375, 9.428955078125, 4.62109375, 626.6376953125, 276.4951171875, 140.72802734375, 86.63330078125, 48.07080078125, 16.7685546875, 9.328125, 3.2919921875], 'modifiers': [{'name': 'stat_overlay', 'type': 'staterror', 'data': [0.015837144604852662, 0.022518867455552254, 0.03219113899898252, 0.04343722427630694, 0.05661385170722979, 0.08333333333333333, 0.11624763874381927, 0.16666666666666669, 0.028016590734026292, 0.0404888165089458, 0.05564148840746572, 0.07624928516630233, 0.0949157995752499, 0.1543033499620919, 0.2182178902359924, 0.3333333333333333]}, {'name': 'Multisim_overlay', 'type': 'shapesys', 'data': [129.05504169725327, 63.739083418194205, 31.777370932503032, 15.811220086522209, 9.138750273328402, 4.7677830188143515, 2.010943085555834, 0.9160964673788494, 123.26532608745725, 55.14138156072539, 26.960652352080206, 21.081059200526184, 9.44963647303558, 2.716785200708332, 2.2446022559536356, 0.9074379723253377]}, {'name': 'Detvar_overlay', 'type': 'shapesys', 'data': [78.76729227597153, 76.17147491890786, 28.61692951360349, 25.370288085937503, 12.894619081787063, 6.72036353405466, 2.8286865234374985, 1.3863281249999992, 169.40170332351585, 70.83187234444362, 42.21840820312501, 25.989990234374986, 14.421240234374991, 5.030566406249997, 2.798437499999998, 0.9875976562499994]}]}, {'name': 'dirt', 'data': [148.01956176757812, 96.86318969726562, 54.79254150390625, 28.32696533203125, 15.5128173828125, 7.726806640625, 3.41864013671875, 1.4547119140625, 68.17282104492188, 40.32318115234375, 18.72381591796875, 12.064178466796875, 2.897796630859375, 1.8658447265625, 0.542877197265625, 0.0], 'modifiers': [{'name': 'stat_dirt', 'type': 'staterror', 'data': [0.03384487217112064, 0.04141576832812911, 0.05812381937190964, 0.07715167498104597, 0.10425720702853736, 0.13867504905630726, 0.20851441405707474, 0.3535533905932738, 0.0669649530182425, 0.086710996952412, 0.125, 0.16012815380508713, 0.30151134457776363, 0.408248290463863, 0.7071067811865476, 0.0]}, {'name': 'dirt_norm', 'type': 'normsys', 'data': {'hi': 1.75, 'lo': 0.25}}]}, {'name': 'beamoff', 'data': [869.8735961914062, 655.4918212890625, 309.7872314453125, 160.50570678710938, 93.16065216064453, 54.99845886230469, 32.55010986328125, 15.713845252990723, 606.7001953125, 326.8273010253906, 161.25128173828125, 76.60980224609375, 40.77618408203125, 14.827703475952148, 5.560389041900635, 1.2356419563293457], 'modifiers': [{'name': 'stat_beamoff', 'type': 'staterror', 'data': [0.0359210595221782, 0.04138029493405687, 0.060192927113470254, 0.08362419994995166, 0.10976426194211909, 0.14285714133927296, 0.1856953273213727, 0.2672612413899073, 0.031911282856650756, 0.04347826075776911, 0.06189844410613166, 0.08980265064722945, 0.12309149344917732, 0.20412414694067527, 0.3333333218310204, 0.7071067871058097]}]}]}]}\n"
     ]
    }
   ],
   "source": [
    "HNL_mass=10\n",
    "f = open(f'./jsons/{HNL_mass}_{name_type}_Test.json')\n",
    "\n",
    "data = json.load(f)\n",
    "print(data)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c47f6f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyhf.pdf.Model'>\n",
      "{\n",
      "  \"channels\": [\n",
      "    {\n",
      "      \"name\": \"singlechannel\",\n",
      "      \"samples\": [\n",
      "        {\n",
      "          \"name\": \"signal\",\n",
      "          \"data\": [\n",
      "            0.15326810535043478,\n",
      "            0.25642933906055987,\n",
      "            0.48927898751571774,\n",
      "            0.6572843994945288,\n",
      "            1.03161227889359,\n",
      "            1.5297336503863335,\n",
      "            2.0190125796943903,\n",
      "            4.872157238423824,\n",
      "            0.49950770335271955,\n",
      "            0.9301177924498916,\n",
      "            1.3894352596253157,\n",
      "            2.0726697985082865,\n",
      "            3.4104317892342806,\n",
      "            4.466861952096224,\n",
      "            4.908955190330744,\n",
      "            8.658133447170258\n",
      "          ],\n",
      "          \"modifiers\": [\n",
      "            {\n",
      "              \"name\": \"mu\",\n",
      "              \"type\": \"normfactor\",\n",
      "              \"data\": null\n",
      "            },\n",
      "            {\n",
      "              \"name\": \"stat_siguncrt\",\n",
      "              \"type\": \"staterror\",\n",
      "              \"data\": [\n",
      "                0.13867505442238315,\n",
      "                0.10721125388832235,\n",
      "                0.07761504989177741,\n",
      "                0.06696495310051545,\n",
      "                0.05345224882684524,\n",
      "                0.04389512809466395,\n",
      "                0.03820803675328124,\n",
      "                0.024595949518437136,\n",
      "                0.10721125237867819,\n",
      "                0.07856741932198838,\n",
      "                0.06428243219539191,\n",
      "                0.05263158086831404,\n",
      "                0.04103049797076055,\n",
      "                0.035851736911014793,\n",
      "                0.03419927683377842,\n",
      "                0.025751310096627306\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"name\": \"norm_siguncrt\",\n",
      "              \"type\": \"normsys\",\n",
      "              \"data\": {\n",
      "                \"hi\": 1.3,\n",
      "                \"lo\": 0.7\n",
      "              }\n",
      "            },\n",
      "            {\n",
      "              \"name\": \"Detvar_sig\",\n",
      "              \"type\": \"shapesys\",\n",
      "              \"data\": [\n",
      "                0.030653628378479736,\n",
      "                0.051285880039615706,\n",
      "                0.09785582083378375,\n",
      "                0.13145691124066827,\n",
      "                0.20632250496982474,\n",
      "                0.30594680302065136,\n",
      "                0.40380261221290015,\n",
      "                0.974431680007326,\n",
      "                0.0999015406705439,\n",
      "                0.18602355848997829,\n",
      "                0.25653600698310597,\n",
      "                0.4145339597016572,\n",
      "                0.40974048231347326,\n",
      "                0.394424712405031,\n",
      "                0.541560698124444,\n",
      "                1.1812105254258414\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"overlay\",\n",
      "          \"data\": [\n",
      "            664.782958984375,\n",
      "            328.790771484375,\n",
      "            153.85205078125,\n",
      "            84.567626953125,\n",
      "            45.826416015625,\n",
      "            22.9365234375,\n",
      "            9.428955078125,\n",
      "            4.62109375,\n",
      "            626.6376953125,\n",
      "            276.4951171875,\n",
      "            140.72802734375,\n",
      "            86.63330078125,\n",
      "            48.07080078125,\n",
      "            16.7685546875,\n",
      "            9.328125,\n",
      "            3.2919921875\n",
      "          ],\n",
      "          \"modifiers\": [\n",
      "            {\n",
      "              \"name\": \"stat_overlay\",\n",
      "              \"type\": \"staterror\",\n",
      "              \"data\": [\n",
      "                0.015837144604852662,\n",
      "                0.022518867455552254,\n",
      "                0.03219113899898252,\n",
      "                0.04343722427630694,\n",
      "                0.05661385170722979,\n",
      "                0.08333333333333333,\n",
      "                0.11624763874381927,\n",
      "                0.16666666666666669,\n",
      "                0.028016590734026292,\n",
      "                0.0404888165089458,\n",
      "                0.05564148840746572,\n",
      "                0.07624928516630233,\n",
      "                0.0949157995752499,\n",
      "                0.1543033499620919,\n",
      "                0.2182178902359924,\n",
      "                0.3333333333333333\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"name\": \"Multisim_overlay\",\n",
      "              \"type\": \"shapesys\",\n",
      "              \"data\": [\n",
      "                129.05504169725327,\n",
      "                63.739083418194205,\n",
      "                31.777370932503032,\n",
      "                15.811220086522209,\n",
      "                9.138750273328402,\n",
      "                4.7677830188143515,\n",
      "                2.010943085555834,\n",
      "                0.9160964673788494,\n",
      "                123.26532608745725,\n",
      "                55.14138156072539,\n",
      "                26.960652352080206,\n",
      "                21.081059200526184,\n",
      "                9.44963647303558,\n",
      "                2.716785200708332,\n",
      "                2.2446022559536356,\n",
      "                0.9074379723253377\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"name\": \"Detvar_overlay\",\n",
      "              \"type\": \"shapesys\",\n",
      "              \"data\": [\n",
      "                78.76729227597153,\n",
      "                76.17147491890786,\n",
      "                28.61692951360349,\n",
      "                25.370288085937503,\n",
      "                12.894619081787063,\n",
      "                6.72036353405466,\n",
      "                2.8286865234374985,\n",
      "                1.3863281249999992,\n",
      "                169.40170332351585,\n",
      "                70.83187234444362,\n",
      "                42.21840820312501,\n",
      "                25.989990234374986,\n",
      "                14.421240234374991,\n",
      "                5.030566406249997,\n",
      "                2.798437499999998,\n",
      "                0.9875976562499994\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"dirt\",\n",
      "          \"data\": [\n",
      "            148.01956176757812,\n",
      "            96.86318969726562,\n",
      "            54.79254150390625,\n",
      "            28.32696533203125,\n",
      "            15.5128173828125,\n",
      "            7.726806640625,\n",
      "            3.41864013671875,\n",
      "            1.4547119140625,\n",
      "            68.17282104492188,\n",
      "            40.32318115234375,\n",
      "            18.72381591796875,\n",
      "            12.064178466796875,\n",
      "            2.897796630859375,\n",
      "            1.8658447265625,\n",
      "            0.542877197265625,\n",
      "            0.0\n",
      "          ],\n",
      "          \"modifiers\": [\n",
      "            {\n",
      "              \"name\": \"stat_dirt\",\n",
      "              \"type\": \"staterror\",\n",
      "              \"data\": [\n",
      "                0.03384487217112064,\n",
      "                0.04141576832812911,\n",
      "                0.05812381937190964,\n",
      "                0.07715167498104597,\n",
      "                0.10425720702853736,\n",
      "                0.13867504905630726,\n",
      "                0.20851441405707474,\n",
      "                0.3535533905932738,\n",
      "                0.0669649530182425,\n",
      "                0.086710996952412,\n",
      "                0.125,\n",
      "                0.16012815380508713,\n",
      "                0.30151134457776363,\n",
      "                0.408248290463863,\n",
      "                0.7071067811865476,\n",
      "                0.0\n",
      "              ]\n",
      "            },\n",
      "            {\n",
      "              \"name\": \"dirt_norm\",\n",
      "              \"type\": \"normsys\",\n",
      "              \"data\": {\n",
      "                \"hi\": 1.75,\n",
      "                \"lo\": 0.25\n",
      "              }\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "        {\n",
      "          \"name\": \"beamoff\",\n",
      "          \"data\": [\n",
      "            869.8735961914062,\n",
      "            655.4918212890625,\n",
      "            309.7872314453125,\n",
      "            160.50570678710938,\n",
      "            93.16065216064453,\n",
      "            54.99845886230469,\n",
      "            32.55010986328125,\n",
      "            15.713845252990723,\n",
      "            606.7001953125,\n",
      "            326.8273010253906,\n",
      "            161.25128173828125,\n",
      "            76.60980224609375,\n",
      "            40.77618408203125,\n",
      "            14.827703475952148,\n",
      "            5.560389041900635,\n",
      "            1.2356419563293457\n",
      "          ],\n",
      "          \"modifiers\": [\n",
      "            {\n",
      "              \"name\": \"stat_beamoff\",\n",
      "              \"type\": \"staterror\",\n",
      "              \"data\": [\n",
      "                0.0359210595221782,\n",
      "                0.04138029493405687,\n",
      "                0.060192927113470254,\n",
      "                0.08362419994995166,\n",
      "                0.10976426194211909,\n",
      "                0.14285714133927296,\n",
      "                0.1856953273213727,\n",
      "                0.2672612413899073,\n",
      "                0.031911282856650756,\n",
      "                0.04347826075776911,\n",
      "                0.06189844410613166,\n",
      "                0.08980265064722945,\n",
      "                0.12309149344917732,\n",
      "                0.20412414694067527,\n",
      "                0.3333333218310204,\n",
      "                0.7071067871058097\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model_test = pyhf.Model(data)\n",
    "\n",
    "print(type(model_test))\n",
    "\n",
    "print(json.dumps(model_test.spec, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3737933-44c0-49a8-8753-76418efa4030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dict_separated(Total_dict, debug=False):\n",
    "    \"\"\"\n",
    "    Creating a model where the background samples are fed in individually (takes longer than summing into one total bkg hist).\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    sig_norm = {\"hi\": 1.0+Params_pyhf[\"Signal_flux_error\"], \"lo\": 1.0-Params_pyhf[\"Signal_flux_error\"]}\n",
    "    dirt_norm = {\"hi\": 1.0+Params_pyhf[\"Flat_bkg_dirt_frac\"], \"lo\": 1.0-Params_pyhf[\"Flat_bkg_dirt_frac\"]}\n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be scanned over in the hypo tests\n",
    "                {\"name\": \"stat_siguncrt\", \"type\": \"staterror\", \"data\": sig_stat[HNL_mass]},\n",
    "                {\"name\": \"norm_siguncrt\", \"type\": \"normsys\", \"data\": sig_norm}, #NuMI absorber KDAR rate\n",
    "                {\"name\": \"Detvar_sig\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['SIGNAL_DETVAR']} #shapesys assumes uncorrelated\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"overlay\",\n",
    "              \"data\": Total_dict[HNL_mass]['OVERLAY_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_overlay\", \"type\": \"staterror\", \"data\": overlay_stat[HNL_mass]},\n",
    "                {\"name\": \"Multisim_overlay\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_MULTISIM']},\n",
    "                {\"name\": \"Detvar_overlay\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_DETVAR']} #shapesys assumes uncorrelated\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"dirt\",\n",
    "              \"data\": Total_dict[HNL_mass]['DIRT_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_dirt\", \"type\": \"staterror\", \"data\": dirt_stat[HNL_mass]},\n",
    "                {\"name\": \"dirt_norm\", \"type\": \"normsys\", \"data\": dirt_norm}\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"beamoff\",\n",
    "              \"data\": Total_dict[HNL_mass]['BEAMOFF_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_beamoff\", \"type\": \"staterror\", \"data\": beamoff_stat[HNL_mass]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d595061-c00a-43c5-845f-16e9d176918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dict_same(Total_dict, debug=False):\n",
    "    \"\"\"\n",
    "    Creating a model where the uncertainties are all enveloped in one shapesys modifier for signal and bkg.\n",
    "    The total errors are taken from \\\"TOT_SIGNAL_ERR\\\" and \\\"TOT_BKG_ERR\\\" respectively.\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    \n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                {\"name\": \"uncorr_siguncrt\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass][\"TOT_SIGNAL_ERR\"]}\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"background\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_BKG_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass][\"TOT_BKG_ERR\"]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict\n",
    "\n",
    "def create_model_dict_split(Total_dict, debug=False): \n",
    "    \"\"\"\n",
    "    Creating a model where the uncertainties are split but have no correlations between sig and bkg.\n",
    "    Signal errors are shapesys taken from \\\"SIGNAL_SHAPESYS\\\", staterror taken from sig_stat dict,\n",
    "    and normsys taken from Params_pyhf[\\\"Signal_flux_error\\\"].\n",
    "    Bkg errors are shapesys taken from \\\"BKG_SHAPESYS\\\" and staterror taken from bkg_stat dict.\n",
    "    All bkg sys are enveloped in \\\"BKG_SHAPESYS\\\" here (dirt, detector and multisim).\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    sig_norm = {\"hi\": 1.0+Params_pyhf[\"Signal_flux_error\"], \"lo\": 1.0-Params_pyhf[\"Signal_flux_error\"]}\n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        \n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                {\"name\": \"uncorr_siguncrt\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['SIGNAL_SHAPESYS']},\n",
    "                {\"name\": \"stat_siguncrt\", \"type\": \"staterror\", \"data\": sig_stat[HNL_mass]},  \n",
    "                {\"name\": \"norm_siguncrt\", \"type\": \"normsys\", \"data\": sig_norm}  \n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"background\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_BKG_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_SHAPESYS']},\n",
    "                {\"name\": \"stat_bkguncrt\", \"type\": \"staterror\", \"data\": bkg_stat[HNL_mass]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict\n",
    "\n",
    "def create_model_correlated(Total_dict, debug=False):\n",
    "    \"\"\"\n",
    "    Creating a model where the uncertainties are split and detector variations are taken as fully correlated.\n",
    "    Signal errors are histosys taken from \\\"SIGNAL_DETVAR\\\", staterror taken from sig_stat dict,\n",
    "    and normsys taken from Params_pyhf[\\\"Signal_flux_error\\\"].\n",
    "    Bkg errors are histosys taken from \\\"BKG_DETVAR\\\", staterror taken from bkg_stat dict,\n",
    "    shapesys taken from \\'BKG_MULTISIM'\\, and histosys taken from \\'BKG_DIRT\\' .\n",
    "    Return pyhf model dictionary.\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    sig_norm = {\"hi\": 1.0+Params_pyhf[\"Signal_flux_error\"], \"lo\": 1.0-Params_pyhf[\"Signal_flux_error\"]}\n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        \n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          # \"name\": \"signal\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                {\"name\": \"Detvar\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])-np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR']), \n",
    "                 \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])+np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR'])}},\n",
    "                {\"name\": \"stat_siguncrt\", \"type\": \"staterror\", \"data\": sig_stat[HNL_mass]},  \n",
    "                {\"name\": \"norm_siguncrt\", \"type\": \"normsys\", \"data\": sig_norm}  \n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"background\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_BKG_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"Detvar\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])-np.array(Total_dict[HNL_mass]['BKG_DETVAR']), \n",
    "                 \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])+np.array(Total_dict[HNL_mass]['BKG_DETVAR'])}},\n",
    "                {\"name\": \"Multisim\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_MULTISIM']},\n",
    "                {\"name\": \"Dirt_err\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])-np.array(Total_dict[HNL_mass]['BKG_DIRT']),\n",
    "                  \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])+np.array(Total_dict[HNL_mass]['BKG_DIRT'])}},\n",
    "                {\"name\": \"stat_bkguncrt\", \"type\": \"staterror\", \"data\": bkg_stat[HNL_mass]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b621e-564e-4d5a-bd6d-2a0862a3433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_uncorrelated(Total_dict, debug=False):\n",
    "    \"\"\"\n",
    "    Creating a model where the uncertainties are split and detector variations are taken as fully UNcorrelated.\n",
    "    Signal errors are histosys taken from \\\"SIGNAL_DETVAR\\\", staterror taken from sig_stat dict,\n",
    "    and normsys taken from Params_pyhf[\\\"Signal_flux_error\\\"].\n",
    "    Bkg errors are shapesys taken from \\\"BKG_DETVAR_MULTISIM\\\", staterror taken from bkg_stat dict,\n",
    "    and histosys taken from \\'BKG_DIRT\\' .\n",
    "    Return pyhf model dictionary.\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    sig_norm = {\"hi\": 1.0+Params_pyhf[\"Signal_flux_error\"], \"lo\": 1.0-Params_pyhf[\"Signal_flux_error\"]}\n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        \n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          # \"name\": \"signal\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                # {\"name\": \"Detvar_sig\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])-np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR']), \n",
    "                #  \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])+np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR'])}},\n",
    "                {\"name\": \"Detvar_sig\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['SIGNAL_DETVAR']},\n",
    "                {\"name\": \"stat_siguncrt\", \"type\": \"staterror\", \"data\": sig_stat[HNL_mass]},  \n",
    "                {\"name\": \"norm_siguncrt\", \"type\": \"normsys\", \"data\": sig_norm}  \n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"background\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_BKG_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"Detvar_Multisim\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_DETVAR_MULTISIM']},\n",
    "                {\"name\": \"Dirt_err\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])-np.array(Total_dict[HNL_mass]['BKG_DIRT']),\n",
    "                  \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])+np.array(Total_dict[HNL_mass]['BKG_DIRT'])}},\n",
    "                {\"name\": \"stat_bkguncrt\", \"type\": \"staterror\", \"data\": bkg_stat[HNL_mass]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict\n",
    "\n",
    "def create_model_uncorrelated_dirt_runs_separated(Total_dict, debug=False):\n",
    "    \"\"\"\n",
    "    Creating a model where the uncertainties are split and detector variations are taken as fully UNcorrelated.\n",
    "    Signal errors are histosys taken from \\\"SIGNAL_DETVAR\\\", staterror taken from sig_stat dict,\n",
    "    and normsys taken from Params_pyhf[\\\"Signal_flux_error\\\"].\n",
    "    Bkg errors are shapesys taken from \\\"BKG_DETVAR_MULTISIM\\\", staterror taken from bkg_stat dict,\n",
    "    and histosys taken from \\'BKG_DIRT\\' .\n",
    "    Return pyhf model dictionary.\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    sig_norm = {\"hi\": 1.0+Params_pyhf[\"Signal_flux_error\"], \"lo\": 1.0-Params_pyhf[\"Signal_flux_error\"]}\n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        \n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          # \"name\": \"signal\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'], \n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                # {\"name\": \"Detvar_sig\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])-np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR']), \n",
    "                #  \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])+np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR'])}},\n",
    "                {\"name\": \"Detvar_sig\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['SIGNAL_DETVAR']},\n",
    "                {\"name\": \"stat_siguncrt\", \"type\": \"staterror\", \"data\": sig_stat[HNL_mass]},  \n",
    "                {\"name\": \"norm_siguncrt\", \"type\": \"normsys\", \"data\": sig_norm}  \n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"background\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_BKG_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"Detvar_Multisim\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_DETVAR_MULTISIM']},\n",
    "                {\"name\": \"Dirt_err_r1\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])-np.array(Total_dict[HNL_mass]['BKG_DIRT_R1']),\n",
    "                  \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])+np.array(Total_dict[HNL_mass]['BKG_DIRT_R1'])}},\n",
    "                {\"name\": \"Dirt_err_r3\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])-np.array(Total_dict[HNL_mass]['BKG_DIRT_R3']),\n",
    "                  \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])+np.array(Total_dict[HNL_mass]['BKG_DIRT_R3'])}},\n",
    "                {\"name\": \"stat_bkguncrt\", \"type\": \"staterror\", \"data\": bkg_stat[HNL_mass]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447c26da-ff4f-430b-9b9f-cec8641caabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scaled==False:\n",
    "    print(\"Using unscaled hists\")\n",
    "    Total_dict, theta_dict_scaled = Total_dict_both, theta_dict\n",
    "\n",
    "model_dict_both = create_model_dict_same(Total_dict) #Total_dict_run3_scaled\n",
    "model_dict_run3 = create_model_dict_same(Total_dict_run3_scaled)\n",
    "# model_dict_both = create_model_dict_separated(Total_dict) #All individual samples, all individual errors\n",
    "\n",
    "# model_dict_split = create_model_dict_split(Total_dict_both)\n",
    "\n",
    "# model_dict_both = create_model_correlated(Total_dict)\n",
    "# model_dict_corr = create_model_correlated(Total_dict) \n",
    "# model_dict_uncorr = create_model_uncorrelated(Total_dict) \n",
    "\n",
    "model_dict_run1 = create_model_dict_same(Total_dict_run1_scaled)\n",
    "# model_dict_run3 = create_model_dict_same(Total_dict_run3)\n",
    "print(\"Created models \\n\")\n",
    "\n",
    "print(f'Samples:\\n {model_dict_both[150].config.samples}')\n",
    "print(f'Modifiers are:\\n {model_dict_both[150].config.modifiers}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fbf6f5-63dc-41f9-9d59-f66c8d067793",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Testing MLE etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8586e91-fcc2-4e89-bcf5-689f64d5bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 100\n",
    "\n",
    "model_test = model_dict_corr[HNL_mass]\n",
    "\n",
    "parameters = model_test.config.suggested_init()\n",
    "\n",
    "print(f\"aux data: {model_test.config.auxdata}\")\n",
    "print(f\"nominal: {model_test.expected_data(parameters)}\")\n",
    "print(f'Parameters:\\n {model_test.config.parameters}')\n",
    "print(f'Modifiers:\\n {model_test.config.modifiers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5402aba-8097-4dd1-a338-28585ebfa686",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = model_dict_uncorr[HNL_mass]\n",
    "\n",
    "parameters = model_test.config.suggested_init()\n",
    "\n",
    "print(f\"aux data: {model_test.config.auxdata}\")\n",
    "print(f\"nominal: {model_test.expected_data(parameters)}\")\n",
    "print(f'Parameters:\\n {model_test.config.parameters}')\n",
    "print(f'Modifiers:\\n {model_test.config.modifiers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347b305-d044-4cba-a0e2-c128f0d735ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 10\n",
    "\n",
    "# hist_dict_test = Total_dict_run1_scaled\n",
    "hist_dict_test = Total_dict_both\n",
    "\n",
    "model_bkg = pyhf.Model(\n",
    "    {\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"name\": \"singlechannel\",\n",
    "                \"samples\": [\n",
    "                    {\n",
    "                        \"name\": \"background\",\n",
    "                        # \"name\": \"signal\",\n",
    "                        \"data\": hist_dict_test[HNL_mass]['TOT_BKG_VALS'],\n",
    "                        \"modifiers\": [\n",
    "                            {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": hist_dict_test[HNL_mass][\"TOT_BKG_ERR\"]}\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    poi_name=None,\n",
    ")\n",
    "\n",
    "up_error = np.array(hist_dict_test[HNL_mass]['TOT_BKG_VALS'])+np.array(hist_dict_test[HNL_mass][\"TOT_BKG_ERR\"]) \n",
    "down_error = np.array(hist_dict_test[HNL_mass]['TOT_BKG_VALS'])-np.array(hist_dict_test[HNL_mass][\"TOT_BKG_ERR\"])\n",
    "\n",
    "model_bkg_corr = pyhf.Model(\n",
    "    {\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"name\": \"singlechannel\",\n",
    "                \"samples\": [\n",
    "                    {\n",
    "                        \"name\": \"background\",\n",
    "                        # \"name\": \"signal\",\n",
    "                        \"data\": hist_dict_test[HNL_mass]['TOT_BKG_VALS'],\n",
    "                        \"modifiers\": [\n",
    "                            {\"name\": \"corr_bkguncrt\", \"type\": \"histosys\", \"data\": {\"hi_data\": list(up_error), \"lo_data\": list(down_error)}}\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    poi_name=None,\n",
    ")\n",
    "\n",
    "model_bkg_norm = pyhf.Model(\n",
    "    {\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"name\": \"singlechannel\",\n",
    "                \"samples\": [\n",
    "                    {\n",
    "                        \"name\": \"background\",\n",
    "                        \"data\": hist_dict_test[HNL_mass]['TOT_BKG_VALS'],\n",
    "                        \"modifiers\": [\n",
    "                            {\"name\": \"norm_bkguncrt\", \"type\": \"normsys\", \"data\": {\"hi\": 1.25, \"lo\": 0.75}}\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    poi_name=None,\n",
    ")\n",
    "\n",
    "#----\"shapesys\"----#\n",
    "data = hist_dict_test[HNL_mass][\"data\"]+model_bkg.config.auxdata\n",
    "parameters = model_bkg.config.suggested_init()\n",
    "#----\"histosys\"----#\n",
    "corr_data = hist_dict_test[HNL_mass][\"data\"]+model_bkg_corr.config.auxdata\n",
    "corr_data_edited = list(np.array(hist_dict_test[HNL_mass][\"data\"])*0.7)+model_bkg_corr.config.auxdata\n",
    "corr_parameters = model_bkg_corr.config.suggested_init()\n",
    "#----\"normsys\"----#\n",
    "norm_data = hist_dict_test[HNL_mass][\"data\"]+model_bkg_norm.config.auxdata\n",
    "norm_parameters = model_bkg_norm.config.suggested_init()\n",
    "\n",
    "# print(json.dumps(model_bkg.spec, indent=2))\n",
    "print(f\"aux data: {model_bkg.config.auxdata}\")\n",
    "print(f\"nominal: {model_bkg.expected_data(parameters)}\")\n",
    "print(f'Parameters:\\n {model_bkg.config.parameters}')\n",
    "print(f'Modifiers:\\n {model_bkg.config.modifiers}')\n",
    "\n",
    "best_fit, twice_nll = pyhf.infer.mle.fit(data, model_bkg,return_fitted_val=True)\n",
    "best_fit_corr, twice_nll_corr = pyhf.infer.mle.fit(corr_data, model_bkg_corr,return_fitted_val=True) \n",
    "# best_fit_corr_edited, twice_nll_corr_edited = pyhf.infer.mle.fit(corr_data_edited, model_bkg_corr,return_fitted_val=True)\n",
    "best_fit_norm, twice_nll_norm = pyhf.infer.mle.fit(norm_data, model_bkg_norm,return_fitted_val=True) \n",
    "\n",
    "print(\"best_fit:\")\n",
    "print(best_fit)\n",
    "print(\"Twice NLL:\")\n",
    "print(twice_nll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f869d2e-3671-41e6-8b5f-7ef24f3820ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----\"histosys\"----#\n",
    "print(\"----histosys----\")\n",
    "print(f\"aux data: {model_bkg_corr.config.auxdata}\")\n",
    "print(f\"nominal: {model_bkg_corr.expected_data(corr_parameters)}\")\n",
    "print(f'Parameters:\\n {model_bkg_corr.config.parameters}')\n",
    "print(f'Modifiers:\\n {model_bkg_corr.config.modifiers}')\n",
    "\n",
    "print(best_fit_corr)\n",
    "print(\"Twice NLL:\")\n",
    "print(twice_nll_corr)\n",
    "\n",
    "#----\"normsys\"----#\n",
    "print()\n",
    "print(\"----normsys----\")\n",
    "print(f\"aux data: {model_bkg_norm.config.auxdata}\")\n",
    "print(f\"nominal: {model_bkg_norm.expected_data(corr_parameters)}\")\n",
    "print(f'Parameters:\\n {model_bkg_norm.config.parameters}')\n",
    "print(f'Modifiers:\\n {model_bkg_norm.config.modifiers}')\n",
    "\n",
    "print(best_fit_norm)\n",
    "print(\"Twice NLL:\")\n",
    "print(twice_nll_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369fba94-78a0-4eee-8825-5706ce218198",
   "metadata": {},
   "outputs": [],
   "source": [
    "upvals = np.append(up_error, [0])\n",
    "downvals = np.append(down_error, [0])\n",
    "\n",
    "#----\"shapesys\"----#\n",
    "bkg_times_mle = np.multiply(hist_dict_test[HNL_mass]['TOT_BKG_VALS'],best_fit)\n",
    "\n",
    "plt.hist(Total_bins_cent_dict[HNL_mass], weights=hist_dict_test[HNL_mass]['TOT_BKG_VALS'], bins=Total_bins_overflow[HNL_mass], histtype=\"step\", lw=2, label=\"Bkg prediction\")\n",
    "plt.errorbar(Total_bins_cent_dict[HNL_mass], hist_dict_test[HNL_mass][\"data\"],fmt='.',color='black',lw=5,capsize=5,elinewidth=3,label=\"Data\")\n",
    "plt.hist(Total_bins_cent_dict[HNL_mass], weights=bkg_times_mle, bins=Total_bins_overflow[HNL_mass], histtype=\"step\", lw=2, label=\"Uncorrelated best fit\")\n",
    "#add errors\n",
    "plt.fill_between(Total_bins_overflow[HNL_mass], downvals, upvals, step=\"post\",color=\"grey\",alpha=0.2,zorder=2)\n",
    "\n",
    "#add NLLR\n",
    "# plt.text(0, 800, r\"$-2\\ln (\\lambda) = $\"+str(np.round(twice_nll,1)), fontsize=18)\n",
    "\n",
    "plt.xlabel('BDT score', fontsize=24)\n",
    "plt.ylabel('Events', fontsize=24)\n",
    "plt.legend(fontsize=16, loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "savefig = False\n",
    "\n",
    "if savefig == True:\n",
    "    plt.savefig(f\"plots/CLs_plots/MLE_best_fit_bkg_uncorrelated.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/MLE_best_fit_bkg_uncorrelated.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1727b-620e-4d22-9d05-5ac30614ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----\"histosys\"----#\n",
    "corr_bkg_times_mle = np.multiply(hist_dict_test[HNL_mass][\"TOT_BKG_ERR\"],best_fit_corr)+hist_dict_test[HNL_mass]['TOT_BKG_VALS']\n",
    "\n",
    "plt.hist(Total_bins_cent_dict[HNL_mass], weights=hist_dict_test[HNL_mass]['TOT_BKG_VALS'], bins=Total_bins_overflow[HNL_mass], histtype=\"step\", lw=2, label=\"Bkg prediction\")\n",
    "plt.errorbar(Total_bins_cent_dict[HNL_mass],hist_dict_test[HNL_mass][\"data\"],fmt='.',color='black',lw=5,capsize=5,elinewidth=3,label=\"Data\")\n",
    "plt.hist(Total_bins_cent_dict[HNL_mass], weights=corr_bkg_times_mle, bins=Total_bins_overflow[HNL_mass], histtype=\"step\", lw=2, label=\"Correlated best fit\")\n",
    "#add errors\n",
    "plt.fill_between(Total_bins_overflow[HNL_mass], downvals, upvals, step=\"post\",color=\"grey\",alpha=0.2,zorder=2)\n",
    "\n",
    "# plt.text(0, 800, r\"$-2\\ln (\\lambda) = $\"+str(np.round(twice_nll_corr,1)), fontsize=18)\n",
    "\n",
    "plt.xlabel('BDT score', fontsize=24)\n",
    "plt.ylabel('Events', fontsize=24)\n",
    "plt.legend(fontsize=16, loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "savefig = False\n",
    "\n",
    "if savefig == True:\n",
    "    plt.savefig(f\"plots/CLs_plots/MLE_best_fit_bkg_correlated.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/MLE_best_fit_bkg_correlated.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cecc57-f9e6-4695-bd3e-9b04d1675ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bkg_times_mle)\n",
    "print(np.array(Total_dict_run1_scaled[HNL_mass][\"data\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146707b-b256-4d7c-9a50-8be73facc7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model_dict_run1[HNL_mass].logpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4526ae65-1984-4fa5-82c3-513bde4038f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 10\n",
    "\n",
    "data = Total_dict_run1_scaled[HNL_mass][\"data\"]+model_dict_run1[HNL_mass].config.auxdata\n",
    "parameters = model_dict_run1[HNL_mass].config.suggested_init()\n",
    "\n",
    "best_fit, twice_nll = pyhf.infer.mle.fit(data, model_dict_run1[HNL_mass],return_fitted_val=True)\n",
    "twice_nll_fit = pyhf.infer.mle.twice_nll(parameters, data, model_dict_run1[HNL_mass])\n",
    "\n",
    "logpdf = model_dict_run1[HNL_mass].logpdf(best_fit, data)\n",
    "\n",
    "print(\"best_fit:\")\n",
    "print(best_fit)\n",
    "print(\"Twice NLL fit:\")\n",
    "print(twice_nll)\n",
    "print(twice_nll_fit)\n",
    "print(\"logpdf:\")\n",
    "print(logpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d4058-72cf-461d-8852-3a76db29dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_explore = model_dict_split[HNL_mass]\n",
    "\n",
    "print(f'Samples:\\n {model_to_explore.config.samples}')\n",
    "print(f'Parameters:\\n {model_to_explore.config.parameters}')\n",
    "print(f'Modifiers:\\n {model_to_explore.config.modifiers}')\n",
    "print()\n",
    "\n",
    "print(len(model_to_explore.config.auxdata))\n",
    "print(model_to_explore.config.auxdata)\n",
    "print(model_to_explore.config.poi_index)  \n",
    "print(model_to_explore.config.suggested_bounds()) \n",
    "print(model_to_explore.config.channel_nbins)\n",
    "\n",
    "# print(json.dumps(model_to_explore.spec, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e944bdc-62ce-42bf-930a-5184e6c6c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLs_obs, p_values = pyhf.infer.hypotest(\n",
    "    test_mu, data, model, test_stat=\"qtilde\", return_tail_probs=True\n",
    ")\n",
    "print(f\"Observed CL_s: {CLs_obs}, CL_sb: {p_values[0]}, CL_b: {p_values[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68fc82f-8497-4465-88d7-ea486bdb2f3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Making test statistic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8e66d-3772-4e69-9b66-6b645656e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the model with signal and bkg\n",
    "HNL_mass = 10\n",
    "\n",
    "model_full_uncorr = pyhf.Model(\n",
    "    {\n",
    "  \"channels\": [\n",
    "    {\n",
    "      \"name\": \"singlechannel\",\n",
    "      \"samples\": [\n",
    "        {\n",
    "          \"name\": \"signal\",\n",
    "          \"data\": Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "          \"modifiers\": [\n",
    "            {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "            {\"name\": \"uncorr_siguncrt\", \"type\": \"shapesys\", \"data\": Total_dict_run1_scaled[HNL_mass][\"TOT_SIGNAL_ERR\"]} #Quadsum of ALL uncertainties\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"background\",\n",
    "          \"data\": Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS'],\n",
    "          \"modifiers\": [\n",
    "            {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": Total_dict_run1_scaled[HNL_mass][\"TOT_BKG_ERR\"]} #Quadsum of ALL uncertainties\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    ")\n",
    "\n",
    "sig_up = np.array(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_VALS'])+np.array(Total_dict_run1_scaled[HNL_mass][\"TOT_SIGNAL_ERR\"])\n",
    "sig_down = np.array(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_VALS'])-np.array(Total_dict_run1_scaled[HNL_mass][\"TOT_SIGNAL_ERR\"])\n",
    "bkg_up = np.array(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS'])+np.array(Total_dict_run1_scaled[HNL_mass][\"TOT_BKG_ERR\"]) \n",
    "bkg_down = np.array(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS'])-np.array(Total_dict_run1_scaled[HNL_mass][\"TOT_BKG_ERR\"])\n",
    "\n",
    "model_full_corr = pyhf.Model(\n",
    "    {\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"name\": \"singlechannel\",\n",
    "                \"samples\": [\n",
    "                {\n",
    "                  \"name\": \"signal\",\n",
    "                  \"data\": Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "                  \"modifiers\": [\n",
    "                    {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                    {\"name\": \"corr_siguncrt\", \"type\": \"histosys\", \"data\": {\"hi_data\": list(sig_up), \"lo_data\": list(sig_down)}} \n",
    "                  ]\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"background\",\n",
    "                    \"data\": Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS'],\n",
    "                    \"modifiers\": [\n",
    "                        {\"name\": \"corr_bkguncrt\", \"type\": \"histosys\", \"data\": {\"hi_data\": list(bkg_up), \"lo_data\": list(bkg_down)}} #This is dodgy as includes stat.\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9ff10-074e-47fa-b9b8-e1b706a8c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 10 #Mass point to test\n",
    "\n",
    "obs_scaling = 0.2913938580237639\n",
    "exp_scaling = 0.3709444625002695\n",
    "\n",
    "Use_fully_uncorrelated = True\n",
    "\n",
    "if Use_fully_uncorrelated == True: model_test = model_full_uncorr\n",
    "if Use_fully_uncorrelated == False: model_test = model_full_corr\n",
    "\n",
    "DATA_OBS_dict = {}\n",
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "init_pars = model_test.config.suggested_init()\n",
    "# model_test.expected_actualdata(init_pars) #signal plus bkg expected data\n",
    "\n",
    "bkg_pars = init_pars.copy()\n",
    "# bkg_pars = model_full_uncorr_no_signal.config.suggested_init()\n",
    "bkg_pars[model_test.config.poi_index] = 0 #mu must be zero for the bkg-only model\n",
    "\n",
    "# for i in range(11,21): #Setting siguncrt to 0\n",
    "#     bkg_pars[i] = 0.0\n",
    "    \n",
    "# model_test.expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "print(list(zip(model_test.config.parameters, init_pars)))\n",
    "print(list(zip(model_test.config.parameters, bkg_pars)))\n",
    "\n",
    "pdf_sig = model_test.make_pdf(pyhf.tensorlib.astensor(init_pars)) #Making the pdfs\n",
    "pdf_bkg = model_test.make_pdf(pyhf.tensorlib.astensor(bkg_pars)) #Making the pdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56def80-4891-466e-b064-9a4814d51050",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bkg_pars)\n",
    "# print(model_test.config.par_order)\n",
    "\n",
    "# print(model_test.config.param_set('uncorr_siguncrt').n_parameters)\n",
    "\n",
    "# print(model_test.config.suggested_init())\n",
    "# print(model_test.config.suggested_fixed())\n",
    "# bkg_pars[model_test.config.poi_index] = 0\n",
    "# for i in range(11,21):\n",
    "#     bkg_pars[i] = 0.0\n",
    "print(\"Expected data given init pars\")\n",
    "print(model_test.expected_data(init_pars)) #should this be bkg_pars?\n",
    "print(\"Expected data given bkg pars\")\n",
    "print(model_test.expected_data(bkg_pars)) #should this be bkg_pars?\n",
    "\n",
    "print(Total_dict_run1_scaled[HNL_mass]['data']+model_test.config.auxdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a12a35-c8b3-422d-b8a6-efd8ce89fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "\n",
    "# mu' = 0\n",
    "mc_bkg = pdf_bkg.sample((n_samples,))\n",
    "# mu' = 1\n",
    "mc_sig = pdf_sig.sample((n_samples,))\n",
    "\n",
    "print(mc_bkg.shape)\n",
    "print(mc_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e5770-2881-4cda-856d-0159004cf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_calculator_qtilde_data = pyhf.infer.utils.create_calculator( #only seems to support q-like test statistics\n",
    "    \"toybased\",\n",
    "    # model_test.expected_data(init_pars),\n",
    "    Total_dict_run1_scaled[HNL_mass]['data']+model_test.config.auxdata,\n",
    "    model_test,\n",
    "    ntoys=n_samples,\n",
    "    test_stat=\"qtilde\",\n",
    ")\n",
    "toy_calculator_qtilde_exp = pyhf.infer.utils.create_calculator( #only seems to support q-like test statistics\n",
    "    \"toybased\",\n",
    "    model_test.expected_data(bkg_pars),\n",
    "    # Total_dict_run1_scaled[HNL_mass]['data']+model_test.config.auxdata,\n",
    "    model_test,\n",
    "    ntoys=n_samples,\n",
    "    test_stat=\"qtilde\",\n",
    ")\n",
    "\n",
    "print(\"mu = 1 distributions\")\n",
    "# qtilde_sig, qtilde_bkg = toy_calculator_qtilde.distributions(1.0) #1.0 for full signal contribution, \"null\" defaults to mu=1 in pyhf\n",
    "qtilde_sig_obs, qtilde_bkg_obs = toy_calculator_qtilde_data.distributions(obs_scaling) \n",
    "qtilde_sig_exp, qtilde_bkg_exp = toy_calculator_qtilde_exp.distributions(exp_scaling) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e6560-dc8b-49ab-be8e-43db22fe4ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#small p-value means outcome v unlikely under null hypothesis\n",
    "\n",
    "CLsb, CLb, CLs = toy_calculator_qtilde_exp.expected_pvalues(qtilde_sig_exp, qtilde_bkg_exp)\n",
    "print(\"Expected\")\n",
    "print(\"CLsb \" + str(CLsb[2]))\n",
    "print(\"CLb \" + str(CLb[2]))\n",
    "print(\"CLs \" + str(CLs[2]))\n",
    "print(CLsb[2]/CLb[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bde7b2-0630-4645-9920-bcb691d6ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tilde_obs = toy_calculator_qtilde_data.teststatistic(obs_scaling)\n",
    "\n",
    "CLsb_obs, CLb_obs, CLs_obs = toy_calculator_qtilde_data.pvalues(q_tilde_obs, qtilde_sig_obs, qtilde_bkg_obs)\n",
    "print(\"Observed\")\n",
    "print(\"qtilde sig \" + str(q_tilde_obs))\n",
    "print(\"CLsb \" + str(CLsb_obs))\n",
    "print(\"CLb \" + str(CLb_obs))\n",
    "print(\"CLs \" + str(CLs_obs))\n",
    "\n",
    "print(CLsb_obs/CLb_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f44891d-bcc3-4bed-a23e-8ae07cafe51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Total_dict_run1_scaled[HNL_mass][\"data\"]+model_test.config.auxdata\n",
    "data_exp = model_test.expected_data(bkg_pars)\n",
    "par_bounds = model_test.config.suggested_bounds()\n",
    "fixed_params = model_test.config.suggested_fixed()\n",
    "\n",
    "qmu_tilde_obs = pyhf.infer.test_statistics.qmu_tilde(obs_scaling, data, model_test, init_pars, par_bounds, fixed_params, return_fitted_pars=False)\n",
    "qmu_tilde_exp = pyhf.infer.test_statistics.qmu_tilde(exp_scaling, data_exp, model_test, init_pars, par_bounds, fixed_params, return_fitted_pars=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b565ae07-5324-41e2-a60e-4b27f342e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(qmu_tilde_obs)\n",
    "print(q_tilde_obs)\n",
    "print(qmu_tilde_exp)\n",
    "\n",
    "\n",
    "def Get_proportion(dist, test_stat_val):\n",
    "    \"\"\"\n",
    "    Given a pyhf sample \"dist\" and a given value, \n",
    "    returns the proportion of the total dist up to that point.\n",
    "    \"\"\"\n",
    "    if max(dist) < test_stat_val:\n",
    "        print(\"Value is above max in dist\")\n",
    "        return 0\n",
    "    ordered_test_stat = np.sort(dist) #Test stat vals going from lowest to highest\n",
    "    itemindex = np.where(ordered_test_stat >= test_stat_val)\n",
    "    prop = itemindex[0][0]/len(dist)\n",
    "    \n",
    "    return prop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca77ec-ced8-4126-96d8-ed97303d2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val_b = Get_proportion(qtilde_bkg_obs.samples, qmu_tilde_obs) #Integral of b-only dist where test stat < observed val\n",
    "p_val_sb = 1-Get_proportion(qtilde_sig_obs.samples, qmu_tilde_obs) #Integral of s+b dist where test stat > observed val\n",
    "print(\"Observed\")\n",
    "print(\"p_b: \" + str(p_val_b))\n",
    "print(\"p_sb: \" + str(p_val_sb))\n",
    "\n",
    "print(p_val_sb/(1-p_val_b))\n",
    "\n",
    "p_val_b_exp = Get_proportion(qtilde_bkg_exp.samples, qmu_tilde_exp) \n",
    "p_val_sb_exp = 1-Get_proportion(qtilde_sig_exp.samples, qmu_tilde_exp)\n",
    "\n",
    "print()\n",
    "print(\"Expected\")\n",
    "print(\"p_b: \" + str(p_val_b_exp))\n",
    "print(\"p_sb: \" + str(p_val_sb_exp))\n",
    "\n",
    "print(p_val_sb_exp/(1-p_val_b_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad355c4-c5e1-4840-8f8c-61ca0b68ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max qtilde signal value: \" + str(max(qtilde_sig_obs.samples)))\n",
    "print(\"Max qtilde background value: \" + str(max(qtilde_bkg_obs.samples)))\n",
    "\n",
    "#Finding probability for one value of test statistic\n",
    "ordered_test_stat = np.sort(qtilde_sig_obs.samples)\n",
    "ordered_bkg = np.sort(qtilde_bkg_obs.samples)\n",
    "length =len(ordered_test_stat)\n",
    "\n",
    "prob = 0.9\n",
    "slice_at = 0.9*length\n",
    "print(\"-----signal-----\")\n",
    "print(np.floor(slice_at))\n",
    "print(ordered_test_stat[int(slice_at)])\n",
    "value_for_prob = ordered_test_stat[int(slice_at)]\n",
    "\n",
    "print(\"-----bkg-----\")\n",
    "print(np.floor(slice_at))\n",
    "print(ordered_bkg[int(slice_at)])\n",
    "bkg_expected_qtilde = ordered_bkg[int(0.5*length)]\n",
    "print(bkg_expected_qtilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc300d6a-fdab-47dd-baee-dcd9f0b19349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unedited one cell below\n",
    "savefig=False\n",
    "\n",
    "# x_max = max(qtilde_sig.samples)\n",
    "x_max = max(qtilde_bkg_obs.samples)\n",
    "# x_max=2e-8\n",
    "print(x_max)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "nbins = 30 #Should make an array for the bin edges\n",
    "bins = np.linspace(0,x_max*1.2,nbins) #was 0, 45 before\n",
    "\n",
    "hist, edges = np.histogram(qtilde_sig_obs.samples, bins=bins, density=True)\n",
    "edges = np.repeat(edges, 2)\n",
    "hist = np.hstack((0, np.repeat(hist, 2), 0))\n",
    "\n",
    "hist_bkg, edges_bkg = np.histogram(qtilde_bkg_obs.samples, bins=bins, density=True)\n",
    "edges_bkg = np.repeat(edges_bkg, 2)\n",
    "hist_bkg = np.hstack((0, np.repeat(hist_bkg, 2), 0))\n",
    "\n",
    "hatch_from= qmu_tilde_obs\n",
    "hatch_from_bkg= 0.0\n",
    "print(qmu_tilde_obs)\n",
    "hatch_till= 15\n",
    "hatch_till_bkg= qmu_tilde_obs\n",
    "\n",
    "fill_region = (hatch_from<edges)&(edges<hatch_till)\n",
    "\n",
    "fill_region_bkg = (hatch_from_bkg<edges_bkg)&(edges_bkg<hatch_till_bkg)\n",
    "test_fill_bkg = hist_bkg[fill_region_bkg].copy()\n",
    "# test_fill_bkg[-1] = 0.0\n",
    "new_hist=np.append(test_fill_bkg, test_fill_bkg[-1])\n",
    "new_hist=np.append(new_hist, 0.0)\n",
    "\n",
    "test_fill_edges = edges_bkg[fill_region_bkg].copy()\n",
    "new_edges=np.append(test_fill_edges, qmu_tilde_obs)\n",
    "new_edges=np.append(new_edges, qmu_tilde_obs)\n",
    "\n",
    "\n",
    "plt.hist(\n",
    "    qtilde_sig_obs.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_1|1)$ signal-like\",\n",
    "    color=\"#D62728\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.hist(\n",
    "    qtilde_bkg_obs.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_1|0)$ background-like\",\n",
    "    color=\"#1F77B4\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "plt.fill_betweenx(hist[fill_region], \n",
    "                  edges[fill_region], qmu_tilde_obs,\n",
    "                  color='none', edgecolor='#D62728',\n",
    "                  hatch='xxx', label=r\"$p_{s+b}$\")\n",
    "\n",
    "plt.fill_betweenx(new_hist, \n",
    "                  new_edges, 0,\n",
    "                  color='none', edgecolor='#1F77B4',\n",
    "                  hatch='xxx', label=r\"$p_{b}$\")\n",
    "\n",
    "if Use_fully_uncorrelated == True: model_name = \"fully_uncorrelated\"\n",
    "if Use_fully_uncorrelated == False: model_name = \"correlated\"\n",
    "\n",
    "plt.axvline(x = qmu_tilde_obs, color = 'black', label = r'Data',linestyle=\"dashed\", lw=3)\n",
    "# plt.axvline(x = bkg_expected_qtilde, color = 'gray', label = 'Background expectation',linestyle=\"dashed\", lw=3)\n",
    "\n",
    "# plt.plot(x_plot, test_f_q, color=\"black\")\n",
    "\n",
    "plt.xlabel(r\"$\\tilde{q}_1$\", fontsize=22)\n",
    "plt.ylabel(r\"$f\\,(\\tilde{q}_1|\\mu')$\", fontsize=22)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(3e-3, 2.0)\n",
    "plt.xlim(0,15)\n",
    "plt.legend(fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "if savefig==True:\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_qtilde_obs_asymp_{model_name}_{HNL_mass}MeV.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_qtilde_obs_asymp_{model_name}_{HNL_mass}MeV.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195623a-0f61-454a-a7ef-88bd9e3a0574",
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig=False\n",
    "\n",
    "# x_max = max(qtilde_sig.samples)\n",
    "x_max = max(qtilde_bkg_obs.samples)\n",
    "# x_max=2e-8\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "nbins = 30 #Should make an array for the bin edges\n",
    "bins = np.linspace(0,x_max*1.2,nbins) #was 0, 45 before\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(\n",
    "    qtilde_sig_obs.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_1|1)$ signal-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.hist(\n",
    "    qtilde_bkg_obs.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_1|0)$ background-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "# value_for_prob = 2.16\n",
    "if Use_fully_uncorrelated == True: model_name = \"fully_uncorrelated\"\n",
    "if Use_fully_uncorrelated == False: model_name = \"correlated\"\n",
    "\n",
    "# plt.axvline(x = value_for_prob, color = 'red', label = '90% signal-like',linestyle=\"dashed\", lw=3)\n",
    "plt.axvline(x = qmu_tilde_obs, color = 'black', label = 'Data',linestyle=\"dashed\", lw=3)\n",
    "plt.axvline(x = bkg_expected_qtilde, color = 'gray', label = 'Background expectation',linestyle=\"dashed\", lw=3)\n",
    "\n",
    "plt.xlabel(r\"$\\tilde{q}_1$\", fontsize=22)\n",
    "plt.ylabel(r\"$f\\,(\\tilde{q}_1|\\mu')$\", fontsize=22)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(3e-3, 2.0)\n",
    "plt.legend(fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "if savefig==True:\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_qtilde_obs_{model_name}_{HNL_mass}MeV.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_qtilde_obs_{model_name}_{HNL_mass}MeV.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e9cec-4452-4fc7-84ec-e288239a39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All just zeros, I don't fully understand this way. \n",
    "savefig=False\n",
    "\n",
    "# x_max = max(qtilde_sig.samples)\n",
    "x_max = max(qtilde_sig_q0.samples)\n",
    "# x_max=2e-8\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "nbins = 30 #Should make an array for the bin edges\n",
    "bins = np.linspace(0,x_max*1.2,nbins) #was 0, 45 before\n",
    "plt.hist(\n",
    "    qtilde_sig_q0.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_0|1)$ signal-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.hist(\n",
    "    qtilde_bkg_q0.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_0|0)$ background-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "# value_for_prob = 2.16\n",
    "if Use_fully_uncorrelated == True: model_name = \"fully_uncorrelated\"\n",
    "if Use_fully_uncorrelated == False: model_name = \"correlated\"\n",
    "\n",
    "# plt.axvline(x = value_for_prob, color = 'red', label = '90% signal-like',linestyle=\"dashed\", lw=3)\n",
    "\n",
    "plt.xlabel(r\"$\\tilde{q}_0$\", fontsize=22)\n",
    "plt.ylabel(r\"$f\\,(\\tilde{q}_0|\\mu')$\", fontsize=22)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=22)\n",
    "plt.tight_layout()\n",
    "\n",
    "if savefig==True:\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_qtilde_0_{model_name}_{HNL_mass}MeV.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_qtilde_0_{model_name}_{HNL_mass}MeV.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97268c1e-74e7-4c6d-a8c5-ffa08eb621c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmu_bounds = model_test.config.suggested_bounds()\n",
    "print(f\"Old bounds: {qmu_bounds}\")\n",
    "# qmu_bounds[model_dict[HNL_mass].config.poi_index] = (-10, 10) #Made these larger and didn't get minimization error in followin cell\n",
    "qmu_bounds[model_test.config.poi_index] = (-50, 50)\n",
    "print(f\"New bounds: {qmu_bounds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1247f-80ae-490f-9799-98bf2f5fda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_calculator_qmu = pyhf.infer.utils.create_calculator(\n",
    "    \"toybased\",\n",
    "    model_test.expected_data(model_test.config.suggested_init()),\n",
    "    model_test,\n",
    "    par_bounds=qmu_bounds,\n",
    "    ntoys=n_samples,\n",
    "    test_stat=\"q\",\n",
    ")\n",
    "qmu_sig, qmu_bkg = toy_calculator_qmu.distributions(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cd24f-3977-4835-8406-7b20bad19c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(qmu_sig.samples))\n",
    "print(max(qmu_bkg.samples))\n",
    "print(min(qmu_sig.samples))\n",
    "print(min(qmu_bkg.samples))\n",
    "\n",
    "ordered_test_stat = np.sort(qmu_sig.samples)\n",
    "length =len(ordered_test_stat)\n",
    "\n",
    "prob = 0.9\n",
    "slice_at = 0.9*length\n",
    "print(np.floor(slice_at))\n",
    "print(ordered_test_stat[int(slice_at)])\n",
    "value_for_prob = ordered_test_stat[int(slice_at)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bd896-544c-42f1-9d23-80c783ef6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig=True\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "nbins = 30 #Should make an array for the bin edges\n",
    "x_max = max(qmu_bkg.samples)\n",
    "x_min = min(qmu_sig.samples)\n",
    "bins = np.linspace(0,x_max*1.2,nbins)\n",
    "plt.hist(\n",
    "    qmu_sig.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=\"$f(q_1|1)$ signal-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.hist(\n",
    "    qmu_bkg.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=\"$f(q_1|0)$ background-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "plt.axvline(x = value_for_prob, color = 'red', label = '90% signal-like',linestyle=\"dashed\", lw=3)\n",
    "\n",
    "plt.xlabel(r\"$q_1$\", fontsize=18)\n",
    "plt.ylabel(r\"$f\\,(q_1|\\mu')$\", fontsize=18)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=22)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if savefig==True:\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_q_{model_name}_{HNL_mass}MeV.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_q_{model_name}_{HNL_mass}MeV.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1ef3c-b55b-4830-8f63-436da533686c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Testing single point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90186d-96f5-48bb-88a5-456477ad403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_uncorr = {}\n",
    "model_dict_uncorr[10] = model_full_uncorr\n",
    "# model_dict_uncorr[10] = model_dict_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c533c-cc65-4f3c-8be0-dee7b236807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 150 #The mass point to test\n",
    "n_sigmas = 2\n",
    "if n_sigmas==1: sigma_list = np.arange(-1, 2)\n",
    "if n_sigmas==2: sigma_list = np.arange(-2, 3)\n",
    "\n",
    "# model_dict_to_use = model_dict_both\n",
    "# model_dict_to_use = model_dict_run1 #model_full_uncorr\n",
    "\n",
    "# model_dict_to_use = model_dict_uncorr\n",
    "model_dict_to_use = model_dict_both\n",
    "# Total_dict_to_use = Total_dict_run1_scaled #Total_dict_both, Total_dict_run1, Total_dict_run3\n",
    "Total_dict_to_use = Total_dict_both\n",
    "\n",
    "DATA_OBS_dict = {}\n",
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "init_pars = model_dict_to_use[HNL_mass].config.suggested_init()\n",
    "model_dict_to_use[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "bkg_pars = init_pars.copy()\n",
    "bkg_pars[model_dict_to_use[HNL_mass].config.poi_index] = 0\n",
    "model_dict_to_use[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "if 'data' not in Total_dict_to_use[HNL_mass].keys(): \n",
    "    print(\"No data in this sample, using bkg as data\")\n",
    "    DATA_OBS_dict[HNL_mass] = Total_dict_to_use[HNL_mass]['TOT_BKG_VALS']+model_dict_to_use[HNL_mass].config.auxdata\n",
    "else: \n",
    "    print(\"Using real data for observed limit\")\n",
    "    DATA_OBS_dict[HNL_mass] = Total_dict_to_use[HNL_mass][\"data\"]+model_dict_to_use[HNL_mass].config.auxdata    \n",
    "\n",
    "model_dict_to_use[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass]) #Is this only for the test stat distribution stuff?\n",
    "    \n",
    "poi_values = np.linspace(0.001, 2, 50) #I could make a dict of this and have different pois for different mass points\n",
    "print(\"Max value is \" + str(max(poi_values)))\n",
    "print(\"Min value is \" + str(min(poi_values)))\n",
    "print(\"Next value is \" + str(poi_values[1]))\n",
    "print(\"Next value is \" + str(poi_values[2]) + \"\\n\")\n",
    "\n",
    "obs_limit_single, exp_limits_single, (scan, results) = pyhf.infer.intervals.upper_limits.upper_limit(DATA_OBS_dict[HNL_mass], \n",
    "                                                                                       model_dict_to_use[HNL_mass], poi_values, \n",
    "                                                                                       level=0.1, return_results=True)\n",
    "\n",
    "# obs_limit_single, exp_limits_single, (scan, results) = pyhf.infer.intervals.upper_limits.upper_limit(DATA_OBS_dict[HNL_mass], \n",
    "#                                                                                        model_dict_to_use[HNL_mass], None, \n",
    "#                                                                                        level=0.1, return_results=True)\n",
    "\n",
    "print(f\"Upper limit {HNL_mass}MeV (obs):  = {obs_limit_single:.4f}\")\n",
    "print(f\"Upper limit {HNL_mass}MeV (exp):  = {exp_limits_single[2]:.4f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd568ec-4faf-42d9-afa7-1c6587eca08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No poi values\n",
    "# Upper limit 150MeV (obs):  = 0.2247 #Quite different from when I provide POI values!\n",
    "# Upper limit 150MeV (exp):  = 0.3495\n",
    "\n",
    "# 200 poi values between 0 and 2.0\n",
    "# Upper limit 150MeV (obs):  = 0.1713\n",
    "# Upper limit 150MeV (exp):  = 0.2833\n",
    "\n",
    "test_mu = obs_limit_single\n",
    "\n",
    "test_mu = 0.20\n",
    "\n",
    "CLs_obs, p_values = pyhf.infer.hypotest(\n",
    "    test_mu, DATA_OBS_dict[HNL_mass], model_dict_to_use[HNL_mass], test_stat=\"qtilde\", return_tail_probs=True\n",
    ")\n",
    "print(f\"Observed CL_s: {CLs_obs}, CL_sb: {p_values[0]}, CL_b: {p_values[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a6532-7c06-4691-94b6-3f52c96b3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while CLb > 1.0:\n",
    "#     mu_test -= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca30ed-ab02-4b48-8155-b85796f78ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pyhf.infer.hypotest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a89e3b-4db5-4353-aa14-3d9316a52c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Params_pyhf[\"Use_toys\"] == True:\n",
    "    CLs_obs, CLs_exp = pyhf.infer.intervals.upper_limits.upper_limit(\n",
    "            # 1.0,  # null hypothesis mu\n",
    "            obs_limit_single, #mu\n",
    "            # exp_limits_single[2], #mu\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict_to_use[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"toybased\",\n",
    "            # ntoys=Params_pyhf[\"Num_toys\"],\n",
    "            ntoys=2000,\n",
    "            # track_progress=True, \n",
    "            )\n",
    "for expected_value, n_sigma in zip(CLs_exp, sigma_list): #-2, 3\n",
    "    print(f\"Expected CLs({n_sigma:2d} ): {expected_value:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57adbf-3935-4075-bfb7-079fbafda89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CLs_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30a6bc-7a6c-47b4-89df-7688667c8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = []\n",
    "obs_limit = []\n",
    "exp_1sig_up = [] #entry 3\n",
    "exp_1sig_down = [] #entry 1\n",
    "exp_2sig_up = [] #entry 4\n",
    "exp_2sig_down = [] #entry 0\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in [HNL_mass]:\n",
    "    theta_squared = (theta_dict_scaled[HNL_mass])**2\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_single[2])*theta_squared\n",
    "    LIMIT = np.sqrt(obs_limit_single)*theta_squared\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_single[3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_single[4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_single[1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_single[0])*theta_squared)\n",
    "    obs_limit.append(LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39599ac7-a757-4b36-a2e6-ca778c1e377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for HNL_mass in [HNL_mass]:\n",
    "    theta_squared = (theta_dict_scaled[HNL_mass])**2\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_single[2])*theta_squared\n",
    "    LIMIT = np.sqrt(obs_limit_single)*theta_squared\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_single[3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_single[4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_single[1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_single[0])*theta_squared)\n",
    "    obs_limit.append(LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910cc934-a481-4719-a82d-087aa712e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs_limit)\n",
    "print(\"Expected limit\")\n",
    "print(exp_limit)\n",
    "print(\"1 sigma up\")\n",
    "print(exp_1sig_up)\n",
    "print(\"2 sigma up\")\n",
    "print(exp_2sig_up)\n",
    "print(\"1 sigma down\")\n",
    "print(exp_1sig_down)\n",
    "print(\"2 sigma down\")\n",
    "print(exp_2sig_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a459eea-84c1-4045-90aa-e1af4d11c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_limits_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d29e6-f7f4-4088-b9c5-0ccc2fd1f419",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running hypothesis tests for all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ace42d-de08-48dd-8c1a-ed51a6f79b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dict = model_dict_split #Uncertainties are split into different modifiers, takes several times longer to run.\n",
    "model_dict = model_dict_both #Quicker, uncertainties are entered as one uncertainty which is the quadsum of components.\n",
    "# model_dict = model_dict_run3\n",
    "# Total_dict = Total_dict_run3_scaled\n",
    "# print(\"Using run3-only model\")\n",
    "Total_dict = Total_dict_both\n",
    "\n",
    "# model_dict = model_dict_run1\n",
    "# Total_dict = Total_dict_run1_scaled\n",
    "# print(\"Only loading run1 hists! change after finished checking Dirac limits\")\n",
    "\n",
    "# model_dict = model_dict_both\n",
    "# Total_dict = Total_dict_both\n",
    "\n",
    "if Params_pyhf[\"Load_pi0_hists\"]==True:HNL_masses=Constants.HNL_mass_pi0_samples\n",
    "elif Params_pyhf[\"Load_lepton_hists\"]==True:HNL_masses=Constants.HNL_mass_samples\n",
    "elif Params_pyhf[\"Load_single_r1_file\"]==True: HNL_masses = HNL_masses_list\n",
    "elif Params_pyhf[\"Load_lepton_dirac\"]==True: HNL_masses = HNL_masses_list\n",
    "elif Params_pyhf[\"Load_pi0_dirac\"]==True: HNL_masses = HNL_masses_list\n",
    "    \n",
    "# HNL_masses = HNL_masses_list\n",
    "\n",
    "# list_test = Constants.HNL_mass_samples\n",
    "print(\"Running hypothesis tests for these masses: \" + str(HNL_masses))\n",
    "\n",
    "DATA_OBS_dict = {}\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "    model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "    bkg_pars = init_pars.copy()\n",
    "    bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "    model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "    list_keys = list(Total_dict[HNL_mass].keys())\n",
    "    if \"data\" in list_keys: #haven't made this yet, need to test\n",
    "        DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass][\"data\"]+model_dict[HNL_mass].config.auxdata\n",
    "    else:\n",
    "        DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass]['TOT_BKG_VALS']+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "    model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727ad3d-56ab-42bf-ae92-e24e5b50f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Should take ~10 mins for each with 5 bins, 100 mu values\n",
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "print(\"If the output of an upper limit calculation is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "poi_values = np.linspace(0.001, 5, 100) #Values of mu which are scanned over in hypo tests\n",
    "print(\"Max value is \" + str(max(poi_values)))\n",
    "print(\"Min value is \" + str(min(poi_values)))\n",
    "print(\"Next value is \" + str(poi_values[1]))\n",
    "print(\"Next value is \" + str(poi_values[2]) + \"\\n\")\n",
    "\n",
    "print(\"-----Starting Hypothesis tests-----\" + \"\\n\")\n",
    "for HNL_mass in HNL_masses:\n",
    "\n",
    "    # poi_values = np.linspace(0.001, 4, 100) pyhf.infer.intervals.upper_limits.upper_limit\n",
    "    obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upper_limits.upper_limit(\n",
    "        DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_values, level=0.1, return_results=True\n",
    "    )\n",
    "\n",
    "    print(f\"Upper limit {HNL_mass}MeV (obs):  = {obs_limit_dict[HNL_mass]:.6f}\")\n",
    "    print(f\"Upper limit {HNL_mass}MeV (exp):  = {exp_limits_dict[HNL_mass][2]:.6f}\" + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0dcf7f-0665-430c-8239-66452b8bf24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = [] #entry 2\n",
    "exp_1sig_up = [] #entry 3\n",
    "exp_1sig_down = [] #entry 1\n",
    "exp_2sig_up = [] #entry 4\n",
    "exp_2sig_down = [] #entry 0\n",
    "obs_limit = []\n",
    "\n",
    "# theta_dict_to_use = theta_dict_scaled\n",
    "theta_dict_to_use = theta_dict_scaled_r1\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in exp_limits_dict:\n",
    "    theta_squared = (theta_dict_to_use[HNL_mass])**2\n",
    "    print(theta_squared)\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_dict[HNL_mass][2])*theta_squared\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][0])*theta_squared)\n",
    "    \n",
    "    LIMIT = np.sqrt(obs_limit_dict[HNL_mass])*theta_squared\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)\n",
    "    obs_limit.append(LIMIT)\n",
    "    \n",
    "print(exp_limit)\n",
    "print(obs_limit)\n",
    "removed_2MeV = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea893895-8298-4651-940d-25e170d18912",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Plotting CL_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c402f15a-7187-43c3-804c-e671ebd82082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting CL_b for each point\n",
    "\n",
    "CL_b_dict = {}\n",
    "\n",
    "for HNL_mass in exp_limits_dict:\n",
    "\n",
    "    CLs_obs, p_values = pyhf.infer.hypotest(\n",
    "        obs_limit_dict[HNL_mass], DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], test_stat=\"qtilde\", return_tail_probs=True\n",
    "    )\n",
    "    print(f\"Observed CL_s: {CLs_obs}, CL_sb: {p_values[0]}, CL_b: {p_values[1]}\")\n",
    "    CL_b_dict[HNL_mass] = p_values[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf44a2b-3147-4e34-baad-75b859f77f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLb_plot = []\n",
    "\n",
    "if Params_pyhf[\"Load_pi0_hists\"]==True: masses=Constants.HNL_mass_pi0_samples\n",
    "elif Params_pyhf[\"Load_lepton_hists\"]==True: masses=Constants.HNL_mass_samples\n",
    "# masses = [10, 20, 50, 100, 150]\n",
    "\n",
    "for mass in masses:\n",
    "    CLb_plot.append(CL_b_dict[mass])\n",
    "    \n",
    "plt.figure(figsize=[8, 5])\n",
    "# plt.plot(masses, CLb_plot, marker=\"o\", lw=0)\n",
    "plt.plot(masses, CLb_plot, marker=\"x\", markersize=10, markeredgewidth=2, lw=0)\n",
    "plt.axhline(0.5, color=\"black\", ls=\"dashed\")\n",
    "plt.xlabel(\"HNL mass [MeV]\", fontsize=20)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylabel(r\"$\\mathrm{CL_{b}}$\", fontsize=20)\n",
    "\n",
    "plt.ylim(0, 1.0)\n",
    "\n",
    "savefig = input(\"Do you want to save the CLb plot? y/n\")\n",
    "\n",
    "if savefig==\"y\":\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"plots/CLs_plots/Edited_CL_b_{name_type}_channel.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/Edited_CL_b_{name_type}_channel.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa025c-f087-4303-882d-3b7093ed712a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Cross-checking limit from asymptotic test-stat approximations by manually plotting test-stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4234fa8-33c9-48dc-9eaa-674262afcba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Use \"Making test statistic plots\" code for this now\n",
    "# #Takes ~10 mins with 100 toys and 4 bins, for all errors in one modifier\n",
    "# for HNL_mass in HNL_masses:\n",
    "\n",
    "#     if Params_pyhf[\"Use_toys\"] == False:\n",
    "#         CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "#             1.0,  # null hypothesis\n",
    "#             DATA_OBS_dict[HNL_mass],\n",
    "#             model_dict[HNL_mass],\n",
    "#             test_stat=\"qtilde\",\n",
    "#             return_expected_set=True,\n",
    "#             calctype=\"asymptotics\",\n",
    "#             )\n",
    "#     if Params_pyhf[\"Use_toys\"] == True:\n",
    "#         CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "#             1.0,  # null hypothesis\n",
    "#             DATA_OBS_dict[HNL_mass],\n",
    "#             model_dict[HNL_mass],\n",
    "#             test_stat=\"qtilde\",\n",
    "#             return_expected_set=True,\n",
    "#             calctype=\"toybased\",\n",
    "#             ntoys=Params_pyhf[\"Num_toys\"],\n",
    "#             # track_progress=True, \n",
    "#             # track_progress=False, #Used to have as true, but this gave an error when the signal was scaled up\n",
    "#             )\n",
    "    \n",
    "#     print(f\"{HNL_mass}MeV\")\n",
    "#     for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "#         print(f\"Expected CLs({n_sigma:2d} ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22113a-be3b-404d-95b5-9c4154641beb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Removing 2MeV point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d61f76-b44a-42a5-ba62-7e5340758ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is crap, should just change what is loaded in, but leaving like this for compatibility\n",
    "#In future, 2MeV should have been taken out of the analysis right at the start.\n",
    "reload(Constants)\n",
    "if Params_pyhf[\"Load_lepton_hists\"]==True:\n",
    "    HNL_masses=Constants.HNL_mass_samples.copy()\n",
    "    if removed_2MeV == False:\n",
    "        print(\"Removing the first element in the limit and mass lists\")\n",
    "        exp_limit.pop(0)\n",
    "        obs_limit.pop(0)\n",
    "        HNL_masses.pop(0)\n",
    "        exp_1sig_up.pop(0)\n",
    "        exp_2sig_up.pop(0)\n",
    "        exp_1sig_down.pop(0)\n",
    "        exp_2sig_down.pop(0)\n",
    "        print(HNL_masses)\n",
    "        print(exp_limit)\n",
    "        print(obs_limit)\n",
    "    else: print(\"First element has already been removed\")\n",
    "    removed_2MeV = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8437737-0007-42bb-9cfa-bd8e2a45194e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving limit as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefcef15-f96e-4934-bd33-5fc7ff782499",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_limit)\n",
    "print(obs_limit)\n",
    "\n",
    "filename = name_type+BDT_name\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e73a7-2fee-4aad-9067-e3dd5d7d9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = HNL_masses\n",
    "\n",
    "if Params_pyhf[\"Stats_only\"] == True: stats =  \"Stats_only\"\n",
    "elif Params_pyhf[\"Use_flat_sys\"] == True: stats = \"Flat_sys\"\n",
    "else: stats = \"Full_sys\"\n",
    "    \n",
    "if Params_pyhf[\"Use_part_only\"] == True: part_hist = str(Params_pyhf[\"Num_bins_for_calc\"])+\"_bins\"\n",
    "else: part_hist = \"full_hist\"\n",
    "\n",
    "print(masses)\n",
    "print(exp_limit)\n",
    "\n",
    "# filename = \"EXT_full_Finished.csv\"\n",
    "filename += \"_r3_ONLY.csv\"\n",
    "\n",
    "save_limits = input(f\"Do you want to save the limits in new {filename} files? y/n \")\n",
    "if save_limits == \"y\":\n",
    "# r = zip(masses, exp_limit)\n",
    "    r = zip(masses, obs_limit)\n",
    "    q = zip(masses, exp_limit)\n",
    "    if Params_pyhf[\"Load_lepton_hists\"] == True:\n",
    "        savename = f'limit_files/My_limits/observed_{stats}_{part_hist}_{filename}'\n",
    "        with open(savename, \"w\") as s:\n",
    "            w = csv.writer(s)\n",
    "            for row in r:\n",
    "                w.writerow(row)\n",
    "\n",
    "        savename_exp = f'limit_files/My_limits/expected_{stats}_{part_hist}_{filename}'\n",
    "        with open(savename_exp, \"w\") as s:\n",
    "            w = csv.writer(s)\n",
    "            for row in q:\n",
    "                w.writerow(row)\n",
    "\n",
    "    if Params_pyhf[\"Load_pi0_hists\"] == True:\n",
    "        savename = f'limit_files/My_limits/{stats}_{part_hist}_pi0_{filename}'\n",
    "        with open(f'limit_files/My_limits/{stats}_{part_hist}_pi0_{filename}', \"w\") as s:\n",
    "            w = csv.writer(s)\n",
    "            for row in r:\n",
    "                w.writerow(row)\n",
    "                \n",
    "    if (Params_pyhf[\"Load_lepton_dirac\"]) or (Params_pyhf[\"Load_pi0_dirac\"]):\n",
    "        savename = f'limit_files/My_limits/{stats}_{part_hist}_{filename}'\n",
    "        with open(f'limit_files/My_limits/{stats}_{part_hist}_{filename}', \"w\") as s:\n",
    "            w = csv.writer(s)\n",
    "            for row in r:\n",
    "                w.writerow(row)\n",
    "\n",
    "    if Params_pyhf[\"Load_single_r1_file\"] == True:\n",
    "        savename = f'limit_files/My_limits/{stats}_{part_hist}_single_file_{filename}'\n",
    "        with open(f'limit_files/My_limits/{stats}_{part_hist}_single_file_{filename}', \"w\") as s:\n",
    "            w = csv.writer(s)\n",
    "            for row in r:\n",
    "                w.writerow(row)\n",
    "\n",
    "    print()\n",
    "    print(\"Save name is : \\'\"+savename+\"\\'\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a309c42f-47a3-4d38-a2a7-fb09f997f2ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving sigma bands for brazil plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedea031-79e6-4656-840d-a6f6a4a42c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = \"limit_files/Brazil_plot/\"\n",
    "to_save_names = [\"exp_1sig_up\",\"exp_1sig_down\",\"exp_2sig_up\",\"exp_2sig_down\",\"exp_limit\",\"obs_limit\"]\n",
    "to_save_lists = [exp_1sig_up,exp_1sig_down,exp_2sig_up,exp_2sig_down,exp_limit,obs_limit]\n",
    "\n",
    "if Params_pyhf[\"Load_lepton_hists\"] == True:decay_type = \"ee\"\n",
    "elif Params_pyhf[\"Load_pi0_hists\"]==True: decay_type = \"pi0\"\n",
    "elif Params_pyhf[\"Load_lepton_dirac\"] == True: decay_type = \"ee_dirac\"\n",
    "elif Params_pyhf[\"Load_pi0_dirac\"] == True: decay_type = \"pi0_dirac\"\n",
    "else: decay_type = \"single_file\"\n",
    "\n",
    "today = date.today()\n",
    "d1 = today.strftime(\"%d_%m\")\n",
    "\n",
    "filename=f\"_{decay_type}_{d1}.csv\"\n",
    "for i, lim in enumerate(to_save_lists):\n",
    "\n",
    "    r = zip(masses, lim)\n",
    "    savestr=to_save_names[i]\n",
    "    savename = save_loc+savestr+filename\n",
    "    with open(savename, \"w\") as s:\n",
    "        w = csv.writer(s)\n",
    "        for row in r:\n",
    "            w.writerow(row)\n",
    "\n",
    "print(\"Last saved is \" + savename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41250165-08a1-4c2b-a178-d0da18cb8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "uboone_pi0_obs = Functions.Pandafy_new('limit_files/My_limits/Full_sys_8_bins_pi0_pi0_full_Finished_10.csv')\n",
    "pi0_run3 = [2.2164498517661079e-07, 6.623595334451322e-08, 4.320543710704168e-08, 3.3977287519780566e-08, 2.5594212973429393e-08, 2.196644597790047e-08]\n",
    "\n",
    "for i, lim in enumerate(uboone_pi0_obs[\"Value\"]):\n",
    "    print(float(lim)/pi0_run3[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13b856-e47a-4b8e-b67c-740dcd02d13b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Making pull plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0a637-ac65-42fb-886e-8c66bc3ed15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_bkg_only(Total_dict, debug=False):\n",
    "    \"\"\"\n",
    "    Creating a model where the background samples are fed in individually (takes longer than summing into one total bkg hist).\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    sig_norm = {\"hi\": 1.0+Params_pyhf[\"Signal_flux_error\"], \"lo\": 1.0-Params_pyhf[\"Signal_flux_error\"]}\n",
    "    dirt_norm = {\"hi\": 1.0+Params_pyhf[\"Flat_bkg_dirt_frac\"], \"lo\": 1.0-Params_pyhf[\"Flat_bkg_dirt_frac\"]}\n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None} #This is the scaling which is to be scanned over in the hypo tests\n",
    "                # {\"name\": \"stat_siguncrt\", \"type\": \"staterror\", \"data\": sig_stat[HNL_mass]},\n",
    "                # {\"name\": \"norm_siguncrt\", \"type\": \"normsys\", \"data\": sig_norm}, #NuMI absorber KDAR rate\n",
    "                # {\"name\": \"Detvar_sig\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['SIGNAL_DETVAR']} #shapesys assumes uncorrelated\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"overlay\",\n",
    "              \"data\": Total_dict[HNL_mass]['OVERLAY_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_overlay\", \"type\": \"staterror\", \"data\": overlay_stat[HNL_mass]},\n",
    "                {\"name\": \"Multisim_overlay\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_MULTISIM']},\n",
    "                {\"name\": \"Detvar_overlay\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_DETVAR']} #shapesys assumes uncorrelated\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"dirt\",\n",
    "              \"data\": Total_dict[HNL_mass]['DIRT_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_dirt\", \"type\": \"staterror\", \"data\": dirt_stat[HNL_mass]},\n",
    "                {\"name\": \"dirt_norm\", \"type\": \"normsys\", \"data\": dirt_norm}\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"beamoff\",\n",
    "              \"data\": Total_dict[HNL_mass]['BEAMOFF_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_beamoff\", \"type\": \"staterror\", \"data\": beamoff_stat[HNL_mass]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict\n",
    "\n",
    "model_bkg_only = create_model_bkg_only(Total_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374769d-2898-4c75-957c-f321fc9fc214",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bkg_only = create_model_correlated(Total_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d1bd07-e77d-4089-a97b-65f5e521b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhf.set_backend(\"numpy\", \"minuit\")\n",
    "pyhf.set_backend(pyhf.tensorlib, pyhf.optimize.minuit_optimizer(tolerance=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88355dc9-71d0-42a3-be71-8e0f41d8e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tensor Lib: {pyhf.tensorlib}\")\n",
    "print(f\"Optimizer:  {pyhf.optimizer}\")\n",
    "print(f\"Tolerance:  {pyhf.optimizer.tolerance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f7bb1-b354-481c-8fb6-c9f4a71bc681",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 150\n",
    "print(HNL_mass)\n",
    "model = model_bkg_only[HNL_mass]\n",
    "data = Total_dict[HNL_mass][\"data\"]+model.config.auxdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b68ca-16e8-4817-b9da-45a33de1d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pyhf.infer.mle.fit(data, model, return_uncertainties=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa81a2d-2af9-496f-a2bd-b7202bbb0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfit, errors = result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395e0b8-3868-4f4a-b884-0901a3c85a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing first value! hacky, should load bkg-only somehow\n",
    "new_bestfit = np.delete(bestfit, model.config.poi_index)\n",
    "new_errors = np.delete(errors, model.config.poi_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594b8b0-7612-4b24-aa20-882ed65d450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.par_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9997e067-02c4-4fd9-bc29-1f382534cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulls = pyhf.tensorlib.concatenate(\n",
    "    [\n",
    "        (bestfit[model.config.par_slice(k)] - model.config.param_set(k).suggested_init)\n",
    "        / model.config.param_set(k).width()\n",
    "        for k in model.config.par_order\n",
    "        if model.config.param_set(k).constrained\n",
    "    ]\n",
    ")\n",
    "\n",
    "pullerr = pyhf.tensorlib.concatenate(\n",
    "    [\n",
    "        errors[model.config.par_slice(k)] / model.config.param_set(k).width()\n",
    "        for k in model.config.par_order\n",
    "        if model.config.param_set(k).constrained\n",
    "    ]\n",
    ")\n",
    "\n",
    "labels = np.asarray(\n",
    "    [\n",
    "        f\"{k}[{i}]\" if model.config.param_set(k).n_parameters > 1 else k\n",
    "        for k in model.config.par_order\n",
    "        if model.config.param_set(k).constrained\n",
    "        for i in range(model.config.param_set(k).n_parameters)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20daf0e-3942-40f8-a724-2dffaeada86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_order = np.argsort(new_errors)\n",
    "bestfit = bestfit[_order]\n",
    "errors = errors[_order]\n",
    "labels = labels[_order]\n",
    "pulls = pulls[_order]\n",
    "pullerr = pullerr[_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a1f89-ac2a-4a49-998a-3ad1d5dd0e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 8)\n",
    "\n",
    "# set up axes labeling, ranges, etc...\n",
    "ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(labels.size).tolist()))\n",
    "ax.set_xticklabels(labels, rotation=80, ha=\"right\")\n",
    "ax.set_xlim(-0.5, len(pulls) - 0.5)\n",
    "ax.set_title(\"Pull Plot\", fontsize=18)\n",
    "ax.set_ylabel(r\"$(\\theta - \\hat{\\theta})\\,/ \\Delta \\theta$\", fontsize=18)\n",
    "\n",
    "# draw the +/- 2.0 horizontal lines\n",
    "ax.hlines([-2, 2], -0.5, len(pulls) - 0.5, colors=\"black\", linestyles=\"dotted\")\n",
    "# draw the +/- 1.0 horizontal lines\n",
    "ax.hlines([-1, 1], -0.5, len(pulls) - 0.5, colors=\"black\", linestyles=\"dashdot\")\n",
    "# draw the +/- 2.0 sigma band\n",
    "ax.fill_between([-0.5, len(pulls) - 0.5], [-2, -2], [2, 2], facecolor=\"yellow\")\n",
    "# drawe the +/- 1.0 sigma band\n",
    "ax.fill_between([-0.5, len(pulls) - 0.5], [-1, -1], [1, 1], facecolor=\"green\")\n",
    "# draw a horizontal line at pull=0.0\n",
    "ax.hlines([0], -0.5, len(pulls) - 0.5, colors=\"black\", linestyles=\"dashed\")\n",
    "# finally draw the pulls\n",
    "ax.scatter(range(len(pulls)), pulls, color=\"black\")\n",
    "# and their uncertainties\n",
    "ax.errorbar(\n",
    "    range(len(pulls)),\n",
    "    pulls,\n",
    "    color=\"black\",\n",
    "    xerr=0,\n",
    "    yerr=pullerr,\n",
    "    marker=\".\",\n",
    "    fmt=\"none\",\n",
    ")\n",
    "\n",
    "# error > 1\n",
    "error_gt1 = np.argmax(errors > 1) - 0.5\n",
    "ax.axvline(x=error_gt1, color=\"red\", linestyle=\"--\")\n",
    "ax.text(\n",
    "    error_gt1 + 0.1, 1.5, r\"$\\sigma \\geq 1 \\longrightarrow$\", color=\"red\", fontsize=18\n",
    ");\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig = input(\"Do you want to save the pull plot? y/n \")\n",
    "if save_fig == 'y':\n",
    "    plt.savefig(f\"plots/CLs_plots/Fitting_tests/Pull_plot_{HNL_mass}_{name_type}.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/Fitting_tests/Pull_plot_{HNL_mass}_{name_type}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeac33b-58bc-4054-b1f4-20fe1f20cf3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing limits with adjacent BDT models applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da1e60-6a48-4060-9907-67ff7de1e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in sigma bands for Brazil plot\n",
    "#Not finished yet, will take a few hours to get this working\n",
    "\n",
    "def Get_sigma_bands(decay_type, filename):\n",
    "    \"\"\"\n",
    "    Load sigma bands for each mass point from .csv files saved above.\n",
    "    Filename is without the decay type or .csv\n",
    "    \"\"\"\n",
    "    if decay_type not in [\"ee\", \"pi0\", \"ee_dirac\", \"pi0_dirac\"]:\n",
    "        print(\"Need to choose decay type \\\"ee\\\" or \\\"pi0\\\". Exiting.\")\n",
    "        return 1\n",
    "    \n",
    "    to_load_names = [\"exp_1sig_up\",\"exp_1sig_down\",\"exp_2sig_up\",\"exp_2sig_down\",\"exp_limit\",\"obs_limit\"]\n",
    "    loaded_lists = []\n",
    "    for i, name in enumerate(to_load_names):\n",
    "        full_path = f\"limit_files/Brazil_plot/{name}_{decay_type}_{filename}.csv\"\n",
    "        if(os.path.exists(full_path)):\n",
    "            with open(full_path, \"r\") as fp:   # Unpickling\n",
    "                reader = csv.reader(fp)\n",
    "                loaded_list = list(reader)\n",
    "                loaded_lists.append(loaded_list)\n",
    "                \n",
    "    return loaded_lists\n",
    "\n",
    "def Make_dicts_for_limits(loaded_lists):\n",
    "    \"\"\"\n",
    "    Takes output of Get_sigma_bands and translates to dict with HNL masses as keys.\n",
    "    \"\"\"\n",
    "    to_load_names = [\"exp_1sig_up\",\"exp_1sig_down\",\"exp_2sig_up\",\"exp_2sig_down\",\"exp_limit\",\"obs_limit\"]\n",
    "    lims_dict = {}\n",
    "    for i, exp_lim_list in enumerate(loaded_lists[0]):\n",
    "        HNL_mass = exp_lim_list[0]\n",
    "        individual_lims = {}\n",
    "        for j, lims in enumerate(loaded_lists):\n",
    "            individual_lims[to_load_names[j]] = float(loaded_lists[j][i][1])\n",
    "        lims_dict[int(HNL_mass)] = individual_lims\n",
    "    \n",
    "    return lims_dict\n",
    "\n",
    "\n",
    "def Make_list_limits(loaded_lists):\n",
    "    \"\"\"\n",
    "    Takes output of Get_sigma_bands and translates to dict with lists of limit vals.\n",
    "    \"\"\"\n",
    "    to_load_names = [\"exp_1sig_up\",\"exp_1sig_down\",\"exp_2sig_up\",\"exp_2sig_down\",\"exp_limit\",\"obs_limit\"]\n",
    "    lims_dict = {}\n",
    "    for i, exp_lim_list in enumerate(loaded_lists):\n",
    "        lims_list, mass_list = [], []\n",
    "        for j, val in enumerate(exp_lim_list):\n",
    "            lims_list.append(float(val[1]))\n",
    "            mass_list.append(int(val[0]))\n",
    "        \n",
    "        lims_dict[to_load_names[i]] = lims_list\n",
    "    \n",
    "    return lims_dict, mass_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e774def-be03-441b-8a3f-c8280c126e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (Params_pyhf[\"Load_lepton_hists\"]):\n",
    "    ee_loaded_test = Get_sigma_bands(\"ee\", \"19_June\")\n",
    "    ee_limits_dict = Make_dicts_for_limits(ee_loaded_test)\n",
    "\n",
    "if (Params_pyhf[\"Load_pi0_hists\"]):\n",
    "    ee_loaded_test = Get_sigma_bands(\"pi0\", \"19_June\")\n",
    "    ee_limits_dict = Make_dicts_for_limits(ee_loaded_test)\n",
    "    \n",
    "ee_lims_dict, ee_mass_list = Make_list_limits(ee_loaded_test)\n",
    "\n",
    "print(ee_lims_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da226c-15c4-4050-aed5-372579a8931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_list = list(hist_dict_run1.keys())\n",
    "\n",
    "all_files_dict_r1 = {}\n",
    "all_files_dict_r3 = {}\n",
    "Run = \"run1\"\n",
    "end_string = BDT_name\n",
    "\n",
    "if (Params_pyhf[\"Load_pi0_hists\"]): pi0_save_str = \"pi0/\"\n",
    "else: pi0_save_str = \"\"\n",
    "\n",
    "for i, HNL_mass in enumerate(mass_list):\n",
    "    adj_models_dict = {}\n",
    "    model_masses = [] #for masses either side of test point (if available)\n",
    "    \n",
    "    if i > 0: model_masses.append(mass_list[i-1])\n",
    "    if i < len(mass_list)-1: model_masses.append(mass_list[i+1])\n",
    "    \n",
    "    for model in model_masses: #Looping over adjacent models\n",
    "        \n",
    "        save_name = f\"_corrected_Test_{HNL_mass}_{name_type}_model_{model}{end_string}.root\"\n",
    "        \n",
    "        file_r1 = uproot.open(f\"bdt_output/{pi0_save_str}adjacent_models/run1\"+save_name)\n",
    "        file_r3 = uproot.open(f\"bdt_output/{pi0_save_str}adjacent_models/run3\"+save_name)\n",
    "        \n",
    "        sample_str = f\"{HNL_mass}_{model}\"\n",
    "\n",
    "        # all_files_dict[sample_str] = adj_models_dict\n",
    "        all_files_dict_r1[sample_str] = file_r1\n",
    "        all_files_dict_r3[sample_str] = file_r3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da73ce37-cd37-4d05-be25-4e22716829fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_files_dict_r1.keys())\n",
    "print(all_files_dict_r3.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a6de3-9aff-45e8-adba-4cdcd03ace54",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mass = 10\n",
    "model_mass = 20\n",
    "\n",
    "adj_hist = all_files_dict_r1[f'{test_mass}_{model_mass}']['signal'].values()\n",
    "standard = hist_dict_run1[test_mass]['signal'].values()\n",
    "\n",
    "bins_test = np.arange(0,len(adj_hist)+1,1)\n",
    "bins_standard = np.arange(0,len(standard)+1,1)\n",
    "bins_cents_test=(bins_test[:-1]+bins_test[1:])/2\n",
    "bins_cents_standard=(bins_standard[:-1]+bins_standard[1:])/2\n",
    "\n",
    "plt.hist(bins_cents_test, weights=adj_hist, bins=bins_test, label=f\"{test_mass}_{model_mass}\", histtype=\"step\", lw=2)\n",
    "plt.hist(bins_cents_standard, weights=standard, bins=bins_standard, label=f\"{model_mass}\", histtype=\"step\", lw=2)\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727a100-fbbc-4cb2-abf3-2add8f53048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f\"{HNL_mass}_{model}\"\n",
    "\n",
    "reload(Functions)\n",
    "zero_bins_errors_run1_adj = Functions.make_zero_bin_unc(all_files_dict_r1, Constants.run1_POT_scaling_dict, Params_pyhf,adj_hists=True)\n",
    "zero_bins_errors_run3_adj = Functions.make_zero_bin_unc(all_files_dict_r3, Constants.run3_POT_scaling_dict, Params_pyhf,adj_hists=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16406cb5-8d64-4885-8500-8bddc5836848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_root_to_dict(hist_dict_root):\n",
    "    new_dict = {}\n",
    "    for mass in hist_dict_root:\n",
    "        placeholder_dict = {}\n",
    "        for hist in hist_dict_root[mass]:\n",
    "            # print(type(hist))\n",
    "            str_size = len(hist)\n",
    "            hist_name = hist[:str_size - 2]\n",
    "            placeholder_dict[hist_name] = hist_dict_root[mass][hist].values().copy()\n",
    "            \n",
    "        new_dict[mass] = placeholder_dict\n",
    "        \n",
    "        new_dict[mass]['bkg_overlay_stat'] = hist_dict_root[mass]['bkg_overlay'].errors()\n",
    "        new_dict[mass]['bkg_dirt_stat'] = hist_dict_root[mass]['bkg_dirt'].errors()\n",
    "        new_dict[mass]['bkg_EXT_stat'] = hist_dict_root[mass]['bkg_EXT'].errors()\n",
    "        \n",
    "        new_dict[mass]['signal_stat'] = hist_dict_root[mass]['signal'].errors()\n",
    "        \n",
    "    return new_dict\n",
    "\n",
    "r1_adj_dict = Convert_root_to_dict(all_files_dict_r1)\n",
    "r3_adj_dict = Convert_root_to_dict(all_files_dict_r3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f78ad-0d02-402f-9a47-64ad19796c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_errs = ['signal_DetVar_uncertainty_frac','overlay_DetVar_uncertainty_frac',\n",
    "            'ppfx_uncertainty_frac','Genie_uncertainty_frac','Reinteraction_uncertainty_frac']\n",
    "\n",
    "for HNL_mass in hist_dict_run1:\n",
    "    mass_label = HNL_mass\n",
    "    for adj_label in r1_adj_dict:\n",
    "        # test_mass = adj_label.split(\"_\")[1] #8uncertainties for bkg should be copied from the model, however don't have correct bins.\n",
    "        test_mass = int(adj_label.split(\"_\")[1])\n",
    "        if test_mass == mass_label:\n",
    "            for err in sys_errs:\n",
    "                r1_adj_dict[adj_label][err]=hist_dict_run1[HNL_mass][err].values().copy()\n",
    "                r3_adj_dict[adj_label][err]=hist_dict_run3[HNL_mass][err].values().copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949b624-ec11-4c14-85d7-8cad07d55b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Functions)\n",
    "TOT_R1_ERR_adj = Functions.Adj_dict_Full_uncertainty(Params_pyhf, r1_adj_dict, zero_bins_errors_run1_adj)\n",
    "TOT_R3_ERR_adj = Functions.Adj_dict_Full_uncertainty(Params_pyhf, r3_adj_dict, zero_bins_errors_run3_adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec2712-ea2f-4589-ba97-a197cb4a9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Functions)\n",
    "\n",
    "R1_BKG_adj, R1_SIGNAL_adj = Functions.Add_bkg_hists_adj(r1_adj_dict)\n",
    "R3_BKG_adj, R3_SIGNAL_adj = Functions.Add_bkg_hists_adj(r3_adj_dict)\n",
    "\n",
    "R1_output_adj = Functions.Make_into_lists(Params_pyhf, R1_BKG_adj, R1_SIGNAL_adj, TOT_R1_ERR_adj)\n",
    "R3_output_adj = Functions.Make_into_lists(Params_pyhf, R3_BKG_adj, R3_SIGNAL_adj, TOT_R3_ERR_adj)\n",
    "\n",
    "list_input_dicts = [R1_output_adj, R3_output_adj]\n",
    "\n",
    "Total_dict_both_adj = Functions.Create_final_appended_runs_dict(list_input_dicts)\n",
    "Total_dict_run1_adj = Functions.Create_final_appended_runs_dict([R1_output_adj])\n",
    "Total_dict_run3_adj = Functions.Create_final_appended_runs_dict([R3_output_adj])\n",
    "\n",
    "if Params_pyhf[\"Use_part_only\"]==True:\n",
    "    NUMBINS = Params_pyhf[\"Num_bins_for_calc\"]\n",
    "else: NUMBINS=30 #This will just give the full hist\n",
    "    \n",
    "if 'data' in r1_adj_dict['150_180']:\n",
    "    Total_dict_run1_adj=Functions.add_data_adj(Total_dict_run1_adj, r1_adj_dict, NUMBINS)\n",
    "    Total_dict_run3_adj=Functions.add_data_adj(Total_dict_run3_adj, r3_adj_dict, NUMBINS)\n",
    "    Total_dict_both_adj=Functions.add_data_appended_adj(Total_dict_both_adj, r1_adj_dict, r3_adj_dict, NUMBINS)\n",
    "    \n",
    "print(Total_dict_both_adj['150_180'].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6310722b-429b-4b77-b820-528c9dabd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_signal_adj(Total_dict, theta_dict, scaling_dict={}):\n",
    "    \"\"\"\n",
    "    Scales the number of events by the number in the scaling dict.\n",
    "    Returns the new dict of histograms and the new thetas.\n",
    "    \"\"\"\n",
    "    if(scaling_dict=={}): raise Exception(\"Specify scalings\")\n",
    "    Total_dict_scaled, new_theta_dict = copy.deepcopy(Total_dict), {}\n",
    "    for HNL_mass in Total_dict.keys():\n",
    "        test_HNL_mass = int(HNL_mass.split(\"_\")[0])\n",
    "        new_signal_hist = np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])*scaling_dict[HNL_mass]\n",
    "        new_signal_err_hist = np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR'])*scaling_dict[HNL_mass]\n",
    "        new_signal_stat_err = np.array(Total_dict[HNL_mass]['SIGNAL_STAT'])*scaling_dict[HNL_mass]\n",
    "        new_signal_shapesys = np.array(Total_dict[HNL_mass]['SIGNAL_SHAPESYS'])*scaling_dict[HNL_mass]\n",
    "        new_signal_detvar = np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR'])*scaling_dict[HNL_mass]\n",
    "        new_theta = theta_dict[test_HNL_mass]*scaling_dict[HNL_mass]**(1/4) # Number of events is proportional to theta**4\n",
    "        \n",
    "        Total_dict_scaled[HNL_mass]['TOT_SIGNAL_VALS'] = list(new_signal_hist)\n",
    "        Total_dict_scaled[HNL_mass]['TOT_SIGNAL_ERR'] = list(new_signal_err_hist)\n",
    "        Total_dict_scaled[HNL_mass]['SIGNAL_STAT'] = list(new_signal_stat_err)\n",
    "        Total_dict_scaled[HNL_mass]['SIGNAL_SHAPESYS'] = list(new_signal_shapesys)\n",
    "        Total_dict_scaled[HNL_mass]['SIGNAL_DETVAR'] = list(new_signal_detvar)\n",
    "        \n",
    "        new_theta_dict[HNL_mass] = new_theta\n",
    "        \n",
    "    return Total_dict_scaled, new_theta_dict\n",
    "\n",
    "single_SF = 2000\n",
    "if (Params_pyhf[\"Load_lepton_hists\"]) or (Params_pyhf[\"Load_lepton_dirac\"]):\n",
    "    scaling_dict = {2:5000,10:2000,20:5e9,50:2e7,100:2e5,150:2e4} #Scaling for both r1 and r3 combined\n",
    "    scaling_dict_r1 = {2:5000,10:10000,20:2e9,50:5e6,100:1e6,150:5e4}\n",
    "elif Params_pyhf[\"Load_pi0_hists\"]==True or Params_pyhf[\"Load_pi0_dirac\"]==True:\n",
    "    # scaling_dict = {150:100,180:10,200:5,220:2,240:2,245:2}\n",
    "    scaling_dict = {150:500,180:50,200:25,220:10,240:10,245:10}\n",
    "    scaling_dict_r1 = {150:500,180:50,200:25,220:10,240:10,245:10}\n",
    "elif Params_pyhf[\"Load_single_r1_file\"]==True: #Currently using this for pi0 Dirac samples\n",
    "    scaling_dict = {150:1000,180:50,200:50,220:102,240:20,245:20}\n",
    "    \n",
    "adj_scale_dict = {}\n",
    "for mass_label in Total_dict_both_adj:\n",
    "    test_mass = mass_label.split(\"_\")[0]\n",
    "    adj_scale_dict[mass_label] = scaling_dict[int(test_mass)]\n",
    "    \n",
    "print(adj_scale_dict)\n",
    "    \n",
    "scaled=True\n",
    "\n",
    "Total_dict_adj, theta_dict_scaled_adj  = scale_signal_adj(Total_dict_both_adj, theta_dict, adj_scale_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53970a6d-2fbf-4dd2-8c47-f3f9d6d4731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scaled==False:\n",
    "    print(\"Using unscaled hists\")\n",
    "    Total_dict, theta_dict_scaled = Total_dict_both, theta_dict\n",
    "\n",
    "model_dict_both_adj = create_model_dict_same(Total_dict_adj)\n",
    "\n",
    "# model_dict_run1 = create_model_dict_same(Total_dict_run1_scaled)\n",
    "# model_dict_run3 = create_model_dict_same(Total_dict_run3)\n",
    "print(\"Created models \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7494ca-092a-4167-98b3-5561a3d08b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = model_dict_both_adj #Quicker, uncertainties are entered as one uncertainty which is the quadsum of components.\n",
    "\n",
    "HNL_masses = Total_dict_adj.keys()\n",
    "    \n",
    "# HNL_masses = HNL_masses_list\n",
    "\n",
    "# list_test = Constants.HNL_mass_samples\n",
    "print(\"Running hypothesis tests for these masses: \" + str(HNL_masses))\n",
    "\n",
    "DATA_OBS_dict = {}\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "    model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "    bkg_pars = init_pars.copy()\n",
    "    bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "    model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "    list_keys = list(Total_dict_adj[HNL_mass].keys())\n",
    "    if \"data\" in list_keys: #haven't made this yet, need to test\n",
    "        DATA_OBS_dict[HNL_mass] = Total_dict_adj[HNL_mass][\"data\"]+model_dict[HNL_mass].config.auxdata\n",
    "    else:\n",
    "        DATA_OBS_dict[HNL_mass] = Total_dict_adj[HNL_mass]['TOT_BKG_VALS']+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "    model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed4f91-20b4-4aac-99eb-4d19bf7309cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = '150_180'\n",
    "theta = theta_dict_scaled_adj[HNL_mass]\n",
    "\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.figure(figsize=(7,4),facecolor='white',dpi=100)\n",
    "\n",
    "num_bins = len(Total_dict_adj[HNL_mass]['TOT_BKG_VALS'])\n",
    "\n",
    "bins = np.arange(0,num_bins+1,1)\n",
    "bins_cents=(bins[:-1]+bins[1:])/2\n",
    "plt.hist(bins_cents, weights=Total_dict_adj[HNL_mass]['TOT_SIGNAL_VALS'], bins=bins, histtype=\"step\", lw=2, label=f\"Signal \\n\" + fr\"{HNL_mass}MeV $\\theta$={theta:.5f}\")\n",
    "plt.hist(bins_cents, weights=Total_dict_adj[HNL_mass]['TOT_BKG_VALS'], bins=bins, histtype=\"step\",lw=2, label=\"Background\")\n",
    "\n",
    "bkg_up=np.append((Total_dict_adj[HNL_mass]['TOT_BKG_VALS']+np.array(Total_dict_adj[HNL_mass]['TOT_BKG_ERR'])),(Total_dict_adj[HNL_mass]['TOT_BKG_VALS']+np.array(Total_dict_adj[HNL_mass]['TOT_BKG_ERR']))[-1])\n",
    "bkg_down=np.append((Total_dict_adj[HNL_mass]['TOT_BKG_VALS']-np.array(Total_dict_adj[HNL_mass]['TOT_BKG_ERR'])),(Total_dict_adj[HNL_mass]['TOT_BKG_VALS']-np.array(Total_dict_adj[HNL_mass]['TOT_BKG_ERR']))[-1])\n",
    "sig_up=np.append((Total_dict_adj[HNL_mass]['TOT_SIGNAL_VALS']+np.array(Total_dict_adj[HNL_mass]['TOT_SIGNAL_ERR'])),(Total_dict_adj[HNL_mass]['TOT_SIGNAL_VALS']+np.array(Total_dict_adj[HNL_mass]['TOT_SIGNAL_ERR']))[-1])\n",
    "sig_down=np.append((Total_dict_adj[HNL_mass]['TOT_SIGNAL_VALS']-np.array(Total_dict_adj[HNL_mass]['TOT_SIGNAL_ERR'])),(Total_dict_adj[HNL_mass]['TOT_SIGNAL_VALS']-np.array(Total_dict_adj[HNL_mass]['TOT_SIGNAL_ERR']))[-1])\n",
    "\n",
    "plt.fill_between(bins, bkg_down, bkg_up,step=\"post\",hatch='///',alpha=0,zorder=2)\n",
    "plt.fill_between(bins, sig_down, sig_up,step=\"post\",hatch='///',alpha=0,zorder=2)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"BDT score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698245e4-8247-41b0-ba0b-f32ec0d3d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "print(\"If the output of an upper limit calculation is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "poi_values = np.linspace(0.001, 5, 100) #Values of mu which are scanned over in hypo tests\n",
    "print(\"Max value is \" + str(max(poi_values)))\n",
    "print(\"Min value is \" + str(min(poi_values)))\n",
    "print(\"Next value is \" + str(poi_values[1]))\n",
    "print(\"Next value is \" + str(poi_values[2]) + \"\\n\")\n",
    "\n",
    "print(\"-----Starting Hypothesis tests-----\" + \"\\n\")\n",
    "for HNL_mass in HNL_masses:\n",
    "\n",
    "    # poi_values = np.linspace(0.001, 4, 100) pyhf.infer.intervals.upper_limits.upper_limit\n",
    "    obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upper_limits.upper_limit(\n",
    "        DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_values, level=0.1, return_results=True\n",
    "    )\n",
    "\n",
    "    print(f\"Upper limit {HNL_mass}MeV (obs):  = {obs_limit_dict[HNL_mass]:.6f}\")\n",
    "    print(f\"Upper limit {HNL_mass}MeV (exp):  = {exp_limits_dict[HNL_mass][2]:.6f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f4b58-9ba2-4c18-b69d-4494895ca8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = [] #entry 2\n",
    "exp_1sig_up = [] #entry 3\n",
    "exp_1sig_down = [] #entry 1\n",
    "exp_2sig_up = [] #entry 4\n",
    "exp_2sig_down = [] #entry 0\n",
    "obs_limit = []\n",
    "\n",
    "prev_model_exp, next_model_exp = [], []\n",
    "prev_model_obs, next_model_obs = [], []\n",
    "\n",
    "theta_dict_to_use = theta_dict_scaled_adj\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for mass_label in exp_limits_dict:\n",
    "    HNL_mass = int(mass_label.split(\"_\")[0])\n",
    "    model_mass = int(mass_label.split(\"_\")[1])\n",
    "    theta_squared = (theta_dict_to_use[mass_label])**2\n",
    "    print(theta_squared)\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_dict[mass_label][2])*theta_squared\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_dict[mass_label][3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_dict[mass_label][4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_dict[mass_label][1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_dict[mass_label][0])*theta_squared)\n",
    "    \n",
    "    LIMIT = np.sqrt(obs_limit_dict[mass_label])*theta_squared\n",
    "    print(f\"Expected {HNL_mass}MeV {model_mass} model limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV {model_mass} model limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)\n",
    "    obs_limit.append(LIMIT)\n",
    "    \n",
    "    if model_mass < HNL_mass: \n",
    "        prev_model_exp.append(EXP_LIMIT)\n",
    "        prev_model_obs.append(LIMIT)\n",
    "    if model_mass > HNL_mass: \n",
    "        next_model_exp.append(EXP_LIMIT)\n",
    "        next_model_obs.append(LIMIT)\n",
    "    \n",
    "print(exp_limit)\n",
    "print(obs_limit)\n",
    "removed_2MeV = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9f0ee2-c52f-42f9-8a51-2105fe8f15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (Params_pyhf[\"Load_lepton_hists\"]):\n",
    "\n",
    "    print(prev_model_obs)\n",
    "    print(next_model_obs)\n",
    "\n",
    "    prev_model_obs.pop(0)\n",
    "    next_model_obs.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b1e9b-cde4-4171-8b6a-ffba6ce574e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (Params_pyhf[\"Load_lepton_hists\"]):\n",
    "\n",
    "    print(prev_model_exp)\n",
    "    print(next_model_exp)\n",
    "\n",
    "    prev_model_exp.pop(0)\n",
    "    next_model_exp.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea760b4-6ae2-4f08-bcfb-647154890116",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prev_model_obs)\n",
    "print(next_model_obs)\n",
    "\n",
    "if (Params_pyhf[\"Load_lepton_hists\"]):\n",
    "    prev_model_masses = [20, 50, 100, 150]\n",
    "    next_model_masses = [10, 20, 50, 100]\n",
    "    \n",
    "if (Params_pyhf[\"Load_pi0_hists\"]):\n",
    "    prev_model_masses = [180, 200, 220, 240, 245]\n",
    "    next_model_masses = [150, 180, 200, 220, 240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b6830-9912-4af8-8333-246621fa0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting BOTH on one plot\n",
    "plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "savefig = True\n",
    "\n",
    "\n",
    "plt.plot(np.array(ee_mass_list),np.array(ee_lims_dict[\"obs_limit\"]),lw=5,ls='-',color='black')\n",
    "plt.plot(np.array(ee_mass_list),np.array(ee_lims_dict[\"exp_limit\"]),lw=2,ls='--',color='red')\n",
    "plt.fill_between(ee_mass_list,np.array(ee_lims_dict[\"exp_2sig_down\"]),np.array(ee_lims_dict[\"exp_2sig_up\"]),color='yellow')\n",
    "plt.fill_between(ee_mass_list,np.array(ee_lims_dict[\"exp_1sig_down\"]),np.array(ee_lims_dict[\"exp_1sig_up\"]),color='lightgreen')\n",
    "\n",
    "plt.scatter(prev_model_masses, prev_model_obs, marker=\"x\", color=\"blue\", lw=2, label=\"Adjacent model below\")\n",
    "plt.scatter(next_model_masses, next_model_obs, marker=\"x\", color=\"orange\", lw=2, label=\"Adjacent model above\")\n",
    "\n",
    "# plt.scatter(prev_model_masses, prev_model_exp, marker=\"x\", color=\"blue\", lw=2,  label=\"Expected model below\")\n",
    "# plt.scatter(next_model_masses, next_model_exp, marker=\"x\", color=\"orange\", lw=2, label=\"Expected model above\")\n",
    "# plt.plot(next_model_masses, next_model_exp, marker=\"x\", color=\"orange\", lw=0, markersize=12, label=\"Expected model above\")\n",
    "\n",
    "plt.ylabel(r'$|U_{\\mu 4}|^2$ Limit at 90% CL',fontsize=24)\n",
    "plt.xlabel('HNL Mass [MeV]',fontsize=22)\n",
    "\n",
    "if (Params_pyhf[\"Load_lepton_hists\"]):\n",
    "    plt.ylim(5e-7,6e-3)\n",
    "if (Params_pyhf[\"Load_pi0_hists\"]):\n",
    "    plt.ylim(1e-8,4e-7)\n",
    "    \n",
    "plt.legend(loc=\"lower left\",ncol=1,frameon=False,fontsize=22)\n",
    "# plt.text(0.99,0.95,r'MicroBooNE NuMI POT:$7.01\\times 10^{20}$',fontsize=20,color='black',alpha=1,verticalalignment='top',horizontalalignment='right',transform=plt.gca().transAxes)\n",
    "plt.tick_params(axis='x', labelsize=20)\n",
    "plt.tick_params(axis='y', labelsize=20)\n",
    "if (Params_pyhf[\"Load_lepton_hists\"]):\n",
    "    plt.xlim(0,155)\n",
    "if (Params_pyhf[\"Load_pi0_hists\"]):\n",
    "    plt.xlim(140,255)\n",
    "# plt.xlim(0,250)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "savename = f\"{name_type}_adj_test\"\n",
    "\n",
    "# plt.grid(True,lw=1, alpha=0.3)\n",
    "\n",
    "if savefig == True:\n",
    "    plt.savefig('plots/Limits/'+savename+'.pdf',bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.savefig('plots/Limits/'+savename+'.png',bbox_inches='tight', pad_inches=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d614692-ebf7-4ef7-b970-5b8ebcbe4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting ee limit\n",
    "if Params_pyhf[\"Load_lepton_hists\"]==True:\n",
    "    plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "    savefig = False\n",
    "\n",
    "    plt.plot(HNL_masses,np.array(obs_limit),lw=4,ls='-',marker='o',color='black',label='Observed')\n",
    "    plt.plot(HNL_masses,np.array(exp_limit),lw=2,ls='--',color='red',label='Expected')\n",
    "    plt.fill_between(HNL_masses,np.array(exp_2sig_down),np.array(exp_2sig_up),color='yellow',label=r'Exp. 2$\\sigma$')\n",
    "    plt.fill_between(HNL_masses,np.array(exp_1sig_down),np.array(exp_1sig_up),color='lightgreen',label=r'Exp. 1$\\sigma$')\n",
    "\n",
    "    plt.ylabel(r'$|U_{\\mu 4}|^2$ Limit at 90% CL',fontsize=22)\n",
    "    plt.xlabel('HNL Mass [MeV]',fontsize=22)\n",
    "\n",
    "    plt.ylim(5e-7,6e-3)\n",
    "    plt.legend(loc=\"lower left\",ncol=2,frameon=False,fontsize=22)\n",
    "    # plt.text(0.99,0.95,r'MicroBooNE NuMI POT:$7.01\\times 10^{20}$',fontsize=20,color='black',alpha=1,verticalalignment='top',horizontalalignment='right',transform=plt.gca().transAxes)\n",
    "    plt.tick_params(axis='x', labelsize=20)\n",
    "    plt.tick_params(axis='y', labelsize=20)\n",
    "    plt.xlim(0,155)\n",
    "    # plt.xlim(0,250)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    savename = f\"{decay_type}_observed\"\n",
    "\n",
    "    if savefig == True:\n",
    "        plt.savefig('plots/Limits/'+savename+'.pdf',bbox_inches='tight', pad_inches=0.3)\n",
    "        plt.savefig('plots/Limits/'+savename+'.png',bbox_inches='tight', pad_inches=0.3)\n",
    "\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9519eb-217c-4502-90cb-3903bcbad60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eeb77d-75dc-4454-9988-2c852640c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test = Total_dict.keys()\n",
    "DATA_OBS_dict = {}\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples: #removing the 240MeV point\n",
    "for HNL_mass in list_test:\n",
    "    init_pars = model_dict_both[HNL_mass].config.suggested_init()\n",
    "    model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "    bkg_pars = init_pars.copy()\n",
    "    bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "    model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "    DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass]['TOT_BKG_VALS']+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "    model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639d271-7e7c-4cff-9ed6-d20f1f7095cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for HNL_mass in list_test:\n",
    "\n",
    "    if Params_pyhf[\"Use_toys\"] == False:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"asymptotics\",\n",
    "            )\n",
    "    if Params_pyhf[\"Use_toys\"] == True:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"toybased\",\n",
    "            ntoys=Params_pyhf[\"Num_toys\"],\n",
    "            track_progress=True,\n",
    "            )\n",
    "    \n",
    "    print(f\"{HNL_mass}MeV\")\n",
    "    for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "        print(f\"Expected CLs({n_sigma:2d} ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ab734-f7dd-40bb-8b3a-312eda608e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "print(\"If the output of the following is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "poi_values = np.linspace(0.001, 2, 100) #I could make a dict of this and have different pois for different mass points\n",
    "print(\"Max value is \" + str(max(poi_values)))\n",
    "print(\"Min value is \" + str(min(poi_values)))\n",
    "print(\"Next value is \" + str(poi_values[1]))\n",
    "print(\"Next value is \" + str(poi_values[2]) + \"\\n\")\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "# list_test = [180, 200, 220, 240, 245]\n",
    "for HNL_mass in list_test:\n",
    "\n",
    "    # poi_values = np.linspace(0.001, 4, 100)\n",
    "    obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "        DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_values, level=0.1, return_results=True\n",
    "    )\n",
    "\n",
    "    print(f\"Upper limit {HNL_mass}MeV (obs):  = {obs_limit_dict[HNL_mass]:.6f}\")\n",
    "    print(f\"Upper limit {HNL_mass}MeV (exp):  = {exp_limits_dict[HNL_mass][2]:.6f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb324293-40db-4be3-94b0-4498860ed94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = [] #entry 2\n",
    "exp_1sig_up = [] #entry 3\n",
    "exp_1sig_down = [] #entry 1\n",
    "exp_2sig_up = [] #entry 4\n",
    "exp_2sig_down = [] #entry 0\n",
    "obs_limit = []\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in exp_limits_dict:\n",
    "    theta_squared = (theta_dict_scaled[mass_point])**2\n",
    "    print(theta_squared)\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_dict[HNL_mass][2])*theta_squared\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][0])*theta_squared)\n",
    "    \n",
    "    LIMIT = np.sqrt(obs_limit_dict[HNL_mass])*theta_squared\n",
    "    # if HNL_mass in Constants.Old_generator_mass_points:\n",
    "    #     EXP_LIMIT = EXP_LIMIT*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "    #     LIMIT = LIMIT*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c1c24-58ae-4433-8b1c-30f24a80e5b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Brazil plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb80c7-a950-4bba-97a8-df6decfcec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HNL_masses = [10, 20, 50, 100, 150]\n",
    "\n",
    "print(ee_lims_dict)\n",
    "\n",
    "if Params_pyhf[\"Load_lepton_hists\"] == True:decay_type = \"ee\"\n",
    "elif Params_pyhf[\"Load_pi0_hists\"]==True: decay_type = \"pi0\"\n",
    "elif Params_pyhf[\"Load_lepton_dirac\"] == True: decay_type = \"ee_dirac\"\n",
    "elif Params_pyhf[\"Load_pi0_dirac\"] == True: decay_type = \"pi0_dirac\"\n",
    "else: decay_type = \"single_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2d76f-c16f-47a0-8c41-3889c5844e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting ee limit\n",
    "titlename = r\"MicroBooNE Data $7.01\\times10^{20}$ POT\"\n",
    "\n",
    "if Params_pyhf[\"Load_lepton_hists\"]==True:\n",
    "    plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "    savefig = True\n",
    "\n",
    "    plt.plot(HNL_masses,np.array(ee_lims_dict['obs_limit']),lw=4,ls='-',marker='o',color='black',label='Observed')\n",
    "    plt.plot(HNL_masses,np.array(ee_lims_dict['exp_limit']),lw=2,ls='--',color='red',label='Expected')\n",
    "    plt.fill_between(HNL_masses,np.array(ee_lims_dict['exp_2sig_down']),np.array(ee_lims_dict['exp_2sig_up']),color='yellow',label=r'Exp. 2$\\sigma$')\n",
    "    plt.fill_between(HNL_masses,np.array(ee_lims_dict['exp_1sig_down']),np.array(ee_lims_dict['exp_1sig_up']),color='lightgreen',label=r'Exp. 1$\\sigma$')\n",
    "\n",
    "    plt.ylabel(r'$|U_{\\mu 4}|^2$ Limit at 90% CL',fontsize=22)\n",
    "    plt.xlabel('HNL Mass [MeV]',fontsize=22)\n",
    "\n",
    "    plt.ylim(5e-7,6e-3)\n",
    "    plt.legend(loc=\"lower left\",ncol=2,frameon=False,fontsize=22)\n",
    "    if titlename != \"\": plt.legend(loc=\"lower left\", ncol=2, frameon=False, fontsize=22, prop={'size': 22}, title=titlename, title_fontsize=22)\n",
    "    # plt.text(0.99,0.95,r'MicroBooNE NuMI POT:$7.01\\times 10^{20}$',fontsize=20,color='black',alpha=1,verticalalignment='top',horizontalalignment='right',transform=plt.gca().transAxes)\n",
    "    plt.tick_params(axis='x', labelsize=20)\n",
    "    plt.tick_params(axis='y', labelsize=20)\n",
    "    plt.xlim(0,155)\n",
    "    # plt.xlim(0,250)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    savename = f\"{decay_type}_observed\"\n",
    "\n",
    "    if savefig == True:\n",
    "        plt.savefig('plots/Limits/'+savename+'.pdf',bbox_inches='tight', pad_inches=0.3)\n",
    "        plt.savefig('plots/Limits/'+savename+'.png',bbox_inches='tight', pad_inches=0.3)\n",
    "\n",
    "    plt.grid(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba5564-16c6-49ef-9f5e-57528b7a4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_masses = [150, 180, 200, 220, 240, 245]\n",
    "\n",
    "print(ee_lims_dict)\n",
    "\n",
    "if Params_pyhf[\"Load_lepton_hists\"] == True:decay_type = \"ee\"\n",
    "elif Params_pyhf[\"Load_pi0_hists\"]==True: decay_type = \"pi0\"\n",
    "elif Params_pyhf[\"Load_lepton_dirac\"] == True: decay_type = \"ee_dirac\"\n",
    "elif Params_pyhf[\"Load_pi0_dirac\"] == True: decay_type = \"pi0_dirac\"\n",
    "else: decay_type = \"single_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ce420-c92b-488f-9d87-1b2bac39dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting pi0 limit\n",
    "titlename = r\"MicroBooNE Data $7.01\\times10^{20}$ POT\"\n",
    "\n",
    "if Params_pyhf[\"Load_pi0_hists\"]==True:\n",
    "    plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "    savefig = True\n",
    "\n",
    "    plt.plot(HNL_masses,np.array(ee_lims_dict['obs_limit']),lw=5,ls='-',marker='o',markersize=10,color='black',label='Observed')\n",
    "    plt.plot(HNL_masses,np.array(ee_lims_dict['exp_limit']),lw=2,ls='--',color='red',label='Expected')\n",
    "    plt.fill_between(HNL_masses,np.array(ee_lims_dict['exp_2sig_down']),np.array(ee_lims_dict['exp_2sig_up']),color='yellow',label=r'Exp. 2$\\sigma$')\n",
    "    plt.fill_between(HNL_masses,np.array(ee_lims_dict['exp_1sig_down']),np.array(ee_lims_dict['exp_1sig_up']),color='lightgreen',label=r'Exp. 1$\\sigma$')\n",
    "\n",
    "    plt.ylabel(r'$|U_{\\mu 4}|^2$ Limit at 90% CL',fontsize=22)\n",
    "    plt.xlabel('HNL Mass [MeV]',fontsize=22)\n",
    "\n",
    "    plt.ylim(1e-8,4e-7)\n",
    "    plt.legend(loc=\"lower left\",ncol=2,frameon=False,fontsize=22)\n",
    "    if titlename != \"\": plt.legend(loc=\"lower left\", ncol=2, frameon=False, fontsize=22, prop={'size': 22}, title=titlename, title_fontsize=22)\n",
    "    # plt.text(0.99,0.95,r'MicroBooNE NuMI POT:$7.01\\times 10^{20}$',fontsize=20,color='black',alpha=1,verticalalignment='top',horizontalalignment='right',transform=plt.gca().transAxes)\n",
    "    plt.tick_params(axis='x', labelsize=20)\n",
    "    plt.tick_params(axis='y', labelsize=20)\n",
    "    plt.xlim(140,255)\n",
    "    # plt.xlim(0,250)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    savename = f\"{decay_type}_observed\"\n",
    "\n",
    "    if savefig == True:\n",
    "        plt.savefig('plots/Limits/'+savename+'.pdf',bbox_inches='tight', pad_inches=0.3)\n",
    "        plt.savefig('plots/Limits/'+savename+'.png',bbox_inches='tight', pad_inches=0.3)\n",
    "\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aabef30-4da9-4e20-94ae-f99218373d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting BOTH on one plot\n",
    "plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "savefig = False\n",
    "\n",
    "save_loc = \"limit_files/Brazil_plot/\"\n",
    "to_save_names = [\"exp_1sig_up\",\"exp_1sig_down\",\"exp_2sig_up\",\"exp_2sig_down\",\"exp_limit\",\"obs_limit\"]\n",
    "to_save_lists = [exp_1sig_up,exp_1sig_down,exp_2sig_up,exp_2sig_down,exp_limit,obs_limit]\n",
    "\n",
    "#ee curves\n",
    "decay_type=\"ee\"\n",
    "filename=f\"_{decay_type}_21_April.csv\"\n",
    "# for i, lim in enumerate(to_save_lists):\n",
    "\n",
    "ee_exp_1sig_up = Functions.Pandafy_new(save_loc+\"exp_1sig_up\"+filename)\n",
    "ee_exp_1sig_down = Functions.Pandafy_new(save_loc+\"exp_1sig_down\"+filename)\n",
    "ee_exp_2sig_up = Functions.Pandafy_new(save_loc+\"exp_2sig_up\"+filename)\n",
    "ee_exp_2sig_down = Functions.Pandafy_new(save_loc+\"exp_2sig_down\"+filename)\n",
    "ee_exp_limit = Functions.Pandafy_new(save_loc+\"exp_limit\"+filename)\n",
    "ee_obs_limit = Functions.Pandafy_new(save_loc+\"obs_limit\"+filename)\n",
    "HNL_masses_ee = [10, 20, 50, 100, 150]\n",
    "\n",
    "plt.plot(HNL_masses_ee,np.array(ee_obs_limit[\"Value\"]),lw=5,ls='-',color='black')\n",
    "plt.plot(HNL_masses_ee,np.array(ee_exp_limit[\"Value\"]),lw=2,ls='--',color='red')\n",
    "plt.fill_between(HNL_masses_ee,np.array(ee_exp_2sig_down[\"Value\"]),np.array(ee_exp_2sig_up[\"Value\"]),color='yellow')\n",
    "plt.fill_between(HNL_masses_ee,np.array(ee_exp_1sig_down[\"Value\"]),np.array(ee_exp_1sig_up[\"Value\"]),color='lightgreen')\n",
    "\n",
    "#pi0 curves\n",
    "decay_type=\"pi0\"\n",
    "filename=f\"_{decay_type}_21_April.csv\"\n",
    "HNL_masses_pi0 = [150, 180, 200, 220, 240, 245]\n",
    "\n",
    "pi0_exp_1sig_up = Functions.Pandafy_new(save_loc+\"exp_1sig_up\"+filename)\n",
    "pi0_exp_1sig_down = Functions.Pandafy_new(save_loc+\"exp_1sig_down\"+filename)\n",
    "pi0_exp_2sig_up = Functions.Pandafy_new(save_loc+\"exp_2sig_up\"+filename)\n",
    "pi0_exp_2sig_down = Functions.Pandafy_new(save_loc+\"exp_2sig_down\"+filename)\n",
    "pi0_exp_limit = Functions.Pandafy_new(save_loc+\"exp_limit\"+filename)\n",
    "pi0_obs_limit = Functions.Pandafy_new(save_loc+\"obs_limit\"+filename)\n",
    "\n",
    "\n",
    "plt.plot(HNL_masses_pi0,np.array(pi0_obs_limit[\"Value\"]),lw=5,ls='-',color='black',label='Observed')\n",
    "plt.plot(HNL_masses_pi0,np.array(pi0_exp_limit[\"Value\"]),lw=2,ls='--',color='red',label='Expected')\n",
    "plt.fill_between(HNL_masses_pi0,np.array(pi0_exp_2sig_down[\"Value\"]),np.array(pi0_exp_2sig_up[\"Value\"]),color='yellow',label=r'Exp. 2$\\sigma$')\n",
    "plt.fill_between(HNL_masses_pi0,np.array(pi0_exp_1sig_down[\"Value\"]),np.array(pi0_exp_1sig_up[\"Value\"]),color='lightgreen',label=r'Exp. 1$\\sigma$')\n",
    "\n",
    "\n",
    "plt.ylabel(r'$|U_{\\mu 4}|^2$ Limit at 90% CL',fontsize=24)\n",
    "plt.xlabel('HNL Mass [MeV]',fontsize=22)\n",
    "\n",
    "plt.ylim(1e-8,6e-3)\n",
    "plt.legend(loc=\"lower left\",ncol=2,frameon=False,fontsize=24)\n",
    "plt.text(0.99,0.95,r'MicroBooNE NuMI POT:$7.01\\times 10^{20}$',fontsize=24,color='black',alpha=1,\n",
    "         verticalalignment='top',horizontalalignment='right',transform=plt.gca().transAxes)\n",
    "plt.tick_params(axis='x', labelsize=20)\n",
    "plt.tick_params(axis='y', labelsize=20)\n",
    "plt.xlim(0,255)\n",
    "# plt.xlim(0,250)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "savename = \"Both_channels_observed\"\n",
    "\n",
    "# plt.grid(True,lw=1, alpha=0.3)\n",
    "\n",
    "if savefig == True:\n",
    "    plt.savefig('plots/Limits/'+savename+'.pdf',bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.savefig('plots/Limits/'+savename+'.png',bbox_inches='tight', pad_inches=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879feaf-e89c-42e8-9138-1e150f4d4360",
   "metadata": {},
   "source": [
    "## Comparing Dirac and Majorana limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e28f2-77b4-46fe-9423-ae9d048cc74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_sigma_bands(decay_type, filename):\n",
    "    \"\"\"\n",
    "    Load sigma bands for each mass point from .csv files saved above.\n",
    "    Filename is without the decay type or .csv\n",
    "    \"\"\"\n",
    "    if decay_type not in [\"ee\", \"pi0\", \"ee_dirac\", \"pi0_dirac\"]:\n",
    "        print(\"Need to choose decay type \\\"ee\\\" or \\\"pi0\\\". Exiting.\")\n",
    "        return 1\n",
    "    \n",
    "    to_load_names = [\"exp_1sig_up\",\"exp_1sig_down\",\"exp_2sig_up\",\"exp_2sig_down\",\"exp_limit\",\"obs_limit\"]\n",
    "    loaded_lists = []\n",
    "    for i, name in enumerate(to_load_names):\n",
    "        full_path = f\"limit_files/Brazil_plot/{name}_{decay_type}_{filename}.csv\"\n",
    "        if(os.path.exists(full_path)):\n",
    "            with open(full_path, \"r\") as fp:   # Unpickling\n",
    "                reader = csv.reader(fp)\n",
    "                loaded_list = list(reader)\n",
    "                loaded_lists.append(loaded_list)\n",
    "                \n",
    "    return loaded_lists\n",
    "\n",
    "def Make_dicts_for_limits(loaded_lists):\n",
    "    \"\"\"\n",
    "    Takes output of Get_sigma_bands and translates to dict with HNL masses as keys.\n",
    "    \"\"\"\n",
    "    to_load_names = [\"exp_1sig_up\",\"exp_1sig_down\",\"exp_2sig_up\",\"exp_2sig_down\",\"exp_limit\",\"obs_limit\"]\n",
    "    lims_dict = {}\n",
    "    for i, exp_lim_list in enumerate(loaded_lists[0]):\n",
    "        HNL_mass = exp_lim_list[0]\n",
    "        individual_lims = {}\n",
    "        for j, lims in enumerate(loaded_lists):\n",
    "            individual_lims[to_load_names[j]] = float(loaded_lists[j][i][1])\n",
    "        lims_dict[int(HNL_mass)] = individual_lims\n",
    "    \n",
    "    return lims_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f14c4-71c7-43bc-8d98-bb1802bd6e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_HNL_masses = Constants.HNL_ee_dirac_mass_samples\n",
    "pi0_HNL_masses = Constants.HNL_pi0_dirac_mass_samples\n",
    "\n",
    "ee_Dirac_limits = Get_sigma_bands(\"ee_dirac\", \"12_07\")\n",
    "ee_Majorana_limits = Get_sigma_bands(\"ee\", \"12_07\")\n",
    "\n",
    "pi0_Dirac_limits = Get_sigma_bands(\"pi0_dirac\", \"12_07\")\n",
    "pi0_Majorana_limits = Get_sigma_bands(\"pi0\", \"12_07\")\n",
    "\n",
    "ee_Dirac_limits_dict = Make_dicts_for_limits(ee_Dirac_limits)\n",
    "ee_Majorana_limits_dict = Make_dicts_for_limits(ee_Majorana_limits)\n",
    "pi0_Dirac_limits_dict = Make_dicts_for_limits(pi0_Dirac_limits)\n",
    "pi0_Majorana_limits_dict = Make_dicts_for_limits(pi0_Majorana_limits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b88d3-e595-42ad-9904-e0ca6cbafe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ee_Dirac_limits_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e20e2eb-cece-4b2f-9d1c-f2336a4076b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_ee_obs, ratio_pi0_obs = {}, {}\n",
    "ratio_ee_exp, ratio_pi0_exp = {}, {}\n",
    "for HNL_mass in ee_HNL_masses:\n",
    "    ratio_ee_obs[HNL_mass] = ee_Dirac_limits_dict[HNL_mass]['obs_limit']/ee_Majorana_limits_dict[HNL_mass]['obs_limit']\n",
    "    ratio_ee_exp[HNL_mass] = ee_Dirac_limits_dict[HNL_mass]['exp_limit']/ee_Majorana_limits_dict[HNL_mass]['exp_limit']\n",
    "    \n",
    "for HNL_mass in pi0_HNL_masses:\n",
    "    ratio_pi0_obs[HNL_mass] = pi0_Dirac_limits_dict[HNL_mass]['obs_limit']/pi0_Majorana_limits_dict[HNL_mass]['obs_limit']\n",
    "    ratio_pi0_exp[HNL_mass] = pi0_Dirac_limits_dict[HNL_mass]['exp_limit']/pi0_Majorana_limits_dict[HNL_mass]['exp_limit']\n",
    "    \n",
    "print(\"ee observed limits ratios:\")\n",
    "print(ratio_ee_obs)\n",
    "print(\"ee expected limits ratios:\")\n",
    "print(ratio_ee_exp)\n",
    "\n",
    "print(\"pi0 observed limits ratios:\")\n",
    "print(ratio_pi0_obs)\n",
    "print(\"pi0 expected limits ratios:\")\n",
    "print(ratio_pi0_exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bbd1f7-e785-46b3-b6a3-c0727b464e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee_scalings_sum = 0\n",
    "for HNL_mass in ee_HNL_masses:\n",
    "    ee_scalings_sum += ratio_ee_obs[HNL_mass]\n",
    "    \n",
    "num_points_ee = len(ee_HNL_masses)\n",
    "average_ee_scaling = ee_scalings_sum/num_points_ee\n",
    "\n",
    "pi0_scalings_sum = 0\n",
    "for HNL_mass in pi0_HNL_masses:\n",
    "    pi0_scalings_sum += ratio_pi0_obs[HNL_mass]\n",
    "    \n",
    "num_points_pi0 = len(pi0_HNL_masses)\n",
    "average_pi0_scaling = pi0_scalings_sum/num_points_pi0\n",
    "\n",
    "print(f\"ee average scaling: {average_ee_scaling}\")\n",
    "print(f\"pi0 average scaling: {average_pi0_scaling}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917fdd5-6480-4802-87f6-35fb140421aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Changing poi values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cae526-6444-454f-9c77-08280027d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CL_range(observations, model,unbounded_bounds,cl):\n",
    "\n",
    "    alpha = round(1-0.01*cl,2)\n",
    "    low_end  = 1000\n",
    "    high_end = 2000\n",
    "    obs_limit = 2000\n",
    "    iterations = 0\n",
    "    delta_iter = 1000\n",
    "    while obs_limit >= high_end:\n",
    "        iterations += 1\n",
    "        if iterations > 1:\n",
    "            delta_iter = iterations*10000\n",
    "            high_end   += delta_iter\n",
    "        poi_values = np.linspace(low_end,high_end,3)\n",
    "        obs_limit, exp_limits = pyhf.infer.intervals.upper_limits.upper_limit(\n",
    "            observations,model,poi_values,level=alpha,par_bounds=unbounded_bounds\n",
    "        )\n",
    "        print(iterations,\"iterations\")\n",
    "    #return np.linspace(low_end,2*high_end,10)\n",
    "    #lo_range = max(1000,obs_limit-obs_limit/2.)\n",
    "    lo_range = 1000.\n",
    "    if obs_limit > 10000:\n",
    "        lo_range = obs_limit-obs_limit/2.\n",
    "    up_range = obs_limit+obs_limit/2.\n",
    "    return np.linspace(lo_range,up_range,50)\n",
    "\n",
    "\n",
    "nbins = model.config.channel_nbins['singlechannel']\n",
    "print(\"\\nNumber of bins:\",nbins)\n",
    "print(type(nbins))\n",
    "\n",
    "# suggested initial parameters\n",
    "init_pars = model.config.suggested_init()\n",
    "print(\"\\nInitial parameters:\",init_pars)\n",
    "print(\"\\nSuggested bounds:\",model.config.suggested_bounds())\n",
    "\n",
    "unbounded_bounds = model.config.suggested_bounds()\n",
    "unbounded_bounds[model.config.poi_index] = (0, 160000000) #Something very large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830b945-1556-4c50-8ce1-86681c781f8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dumping model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc7172-6236-477f-9d5c-1a50e81519ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 100\n",
    "print(json.dumps(model_dict[HNL_mass].spec, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416cdf2-0f11-4ae6-ac1b-9260325f2396",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Comparing to Collie output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608e383-7ca2-43da-99f1-4b5660318ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing to Collie output:\n",
    "\n",
    "# collie_exp = 17010\n",
    "# collie_obs = 20522\n",
    "\n",
    "collie_exp = 16956.64\n",
    "collie_obs = 20029.17\n",
    "\n",
    "SF = scaling_dict[150]\n",
    "\n",
    "pyhf_obs = 0.5027*SF\n",
    "pyhf_exp = 0.4109*SF\n",
    "\n",
    "print(\"collie obs mu is \" + str(collie_obs))\n",
    "print(\"collie exp mu is \" + str(collie_exp))\n",
    "print()\n",
    "\n",
    "print(\"pyhf obs mu is \" + str(pyhf_obs))\n",
    "print(\"pyhf exp mu is \" + str(pyhf_exp))\n",
    "print()\n",
    "\n",
    "print(\"collie divided by pyhf obs is \" + str(collie_obs/pyhf_obs))\n",
    "print(\"collie divided by pyhf exp is \" + str(collie_exp/pyhf_exp))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc1f65-4dfd-45cc-9351-ecdc937f7fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for HNL_mass in [HNL_mass]:\n",
    "    theta_squared = (theta_dict_scaled[HNL_mass])**2\n",
    "    \n",
    "    EXP_LIMIT = np.sqrt(exp_limits_single[2])*theta_squared\n",
    "    LIMIT = np.sqrt(obs_limit_single)*theta_squared\n",
    "    print(\"-----pyhf-----\")\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "print(\"-----Collie-----\")\n",
    "collie_exp_limit = np.sqrt(collie_exp)*theta_dict[HNL_mass]**2 #Collie input is NOT scaled\n",
    "collie_obs_limit = np.sqrt(collie_obs)*theta_dict[HNL_mass]**2 #Collie input is NOT scaled\n",
    "print(f\"Expected {HNL_mass}MeV limit is \" + str(collie_exp_limit))\n",
    "print(f\"Observed {HNL_mass}MeV limit is \" + str(collie_obs_limit)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80bf9e-cc1a-4d6a-807e-ec8653445b9d",
   "metadata": {},
   "source": [
    "# End of code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
