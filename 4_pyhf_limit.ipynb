{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe0cab0-578b-42ea-86af-07342852e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful!\n"
     ]
    }
   ],
   "source": [
    "#Loading libraries\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyhf\n",
    "from pyhf.contrib.viz import brazil\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import uproot\n",
    "import math\n",
    "import awkward as ak\n",
    "import pickle\n",
    "import csv\n",
    "from importlib import reload\n",
    "\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print(\"Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6939111e-d766-4112-ab60-1fd7c65239ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fully evaluated systematic uncertainty for background. Dirt will still be 100%.\n",
      "Using fully evaluated systematic uncertainty for signal. Using 30.0% flux error.\n"
     ]
    }
   ],
   "source": [
    "Params_pyhf = {\"Stats_only\":False,\n",
    "               \"Use_flat_sys\":False,\n",
    "               \"Use_part_only\":True,\n",
    "               \"Num_bins_for_calc\":4,\n",
    "               \"Use_toys\":True,\n",
    "               \"Num_toys\":100,\n",
    "               \"Load_lepton_hists\":True,\n",
    "               \"Load_pi0_hists\":False,\n",
    "               \"Flat_bkg_overlay_frac\":0.5,\n",
    "               \"Flat_bkg_dirt_frac\":1.0,\n",
    "               \"Flat_bkg_EXT_frac\":0.0,\n",
    "               \"Flat_sig_detvar\":0.2, #This is very conservative, could be fed in per mass point from signal detvar script\n",
    "               \"Signal_flux_error\":0.3, #This comes from the KDAR flux uncertainty.\n",
    "               \"Overlay_detvar_frac\":0.5}\n",
    "\n",
    "Functions.pyhf_params(Params_pyhf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431c728d-46f7-48d0-8cf8-e76c5fa5738f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing hists for Run1 are: \n",
      "['data;1']\n",
      "thetas are:\n",
      "{2: 0.5, 10: 0.05, 20: 0.02, 50: 0.005, 100: 0.001, 150: 0.0005, 180: 0.0004, 200: 0.0002, 220: 0.0002, 240: 0.0002, 245: 0.0002}\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "hist_dict_run1, hist_dict_run3, theta_dict = Functions.Load_pyhf_files(\"logit_FINAL_2.root\", Params_pyhf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2e45e-7b0c-45cc-a21a-65bfd2fafdc3",
   "metadata": {},
   "source": [
    "## Loading in Uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466c18c5-63d4-4237-96ec-e8ac7a64789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_total_uncertainty(Params, hist_dict): #Takes the dictionary of all root files\n",
    "    BKG_ERR_dict, SIGNAL_ERR_dict = {}, {}\n",
    "    bkg_sample_names = ['bkg_overlay','bkg_EXT','bkg_dirt']\n",
    "    overlay_sys_names = [\"ppfx_uncertainty\",\"Genie_uncertainty\",\"Reinteraction_uncertainty\",\"overlay_DetVar_uncertainty\"]\n",
    "    for HNL_mass in hist_dict:\n",
    "        bkg_stat_err_dict, bkg_sys_err_dict = {}, {} #Clean for each mass point\n",
    "        for name in bkg_sample_names:\n",
    "            bkg_stat_err_dict[name]=hist_dict[HNL_mass][name].errors() #Load in stat error from error saved in hist\n",
    "        sig_stat_err = hist_dict[HNL_mass]['signal'].errors()\n",
    "        if Params[\"Stats_only\"] == True: #Set all systematic errors to zero\n",
    "            for name in bkg_sample_names:\n",
    "                bkg_sys_err_dict[name] = np.zeros_like(hist_dict[HNL_mass][name].errors())\n",
    "            sig_sys_err =  np.zeros_like(hist_dict[HNL_mass]['signal'].errors())\n",
    "        elif Params[\"Use_flat_sys\"] == True:\n",
    "            for name in bkg_sample_names:\n",
    "                bkg_sys_err_dict[name] = hist_dict[HNL_mass][name].values()*Params[\"Flat_\"+name+\"_frac\"]\n",
    "            sig_flux_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Signal_flux_error\"]\n",
    "            sig_detvar_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Flat_sig_detvar\"]\n",
    "            sig_sys_err = np.sqrt(sig_flux_err**2 + sig_detvar_err**2)\n",
    "        elif Params[\"Use_flat_sys\"] == False:\n",
    "            overlay_sys_dict = {}\n",
    "            for sys in overlay_sys_names:\n",
    "                overlay_sys_dict[sys] = hist_dict[HNL_mass][sys].values()\n",
    "            bkg_sys_err_dict['bkg_overlay'] = Functions.add_all_errors_dict(overlay_sys_dict)\n",
    "            bkg_sys_err_dict['bkg_EXT'] = np.zeros_like(hist_dict[HNL_mass]['bkg_EXT'].errors())\n",
    "            bkg_sys_err_dict['bkg_dirt'] = hist_dict[HNL_mass]['bkg_dirt'].values()*Params[\"Flat_bkg_dirt_frac\"]\n",
    "            \n",
    "            sig_detvar_err = hist_dict[HNL_mass][\"signal_DetVar_uncertainty\"].values()\n",
    "            sig_flux_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Signal_flux_error\"]\n",
    "            sig_sys_err = Functions.add_all_errors([sig_detvar_err,sig_flux_err])\n",
    "            \n",
    "        #Evaluating final stat+sys errors    \n",
    "        bkg_stat_plus_sys_dict={}\n",
    "        for name in bkg_sample_names:\n",
    "            bkg_stat_plus_sys_dict[name]=Functions.add_all_errors([bkg_stat_err_dict[name],bkg_sys_err_dict[name]]) #WRONG\n",
    "        \n",
    "        total_bkg_err = Functions.add_all_errors_dict(bkg_stat_plus_sys_dict) #Now adding the errors of overlay, EXT and dirt in quadrature\n",
    "        total_sig_err = Functions.add_all_errors([sig_stat_err,sig_sys_err])\n",
    "        \n",
    "        BKG_ERR_dict[HNL_mass] = total_bkg_err\n",
    "        SIGNAL_ERR_dict[HNL_mass] = total_sig_err\n",
    "    return BKG_ERR_dict, SIGNAL_ERR_dict\n",
    "\n",
    "def Uncertainty_breakdown(Params, hist_dict, bkg_reweight_err_dict=None, bkg_detvar_dict=None, sig_detvar_dict=None): #Takes the dictionary of all root files\n",
    "    BKG_ERR_dict, SIGNAL_ERR_dict = {}, {}\n",
    "    for HNL_mass in hist_dict:\n",
    "        bkg_stat_err_list = [hist_dict[HNL_mass]['bkg_overlay'].errors(), \n",
    "                             hist_dict[HNL_mass]['bkg_EXT'].errors(), \n",
    "                             hist_dict[HNL_mass]['bkg_dirt'].errors()]\n",
    "        sig_stat_err = hist_dict[HNL_mass]['signal'].errors()\n",
    "        print(\"Signal stat error:\")\n",
    "        print(sig_stat_err)\n",
    "        if Params[\"Stats_only\"] == True:\n",
    "        #As default the errors saved in the files are stat errors, this will change once I properly calculate them\n",
    "            bkg_err_list = bkg_stat_err_list\n",
    "            sig_err = sig_stat_err\n",
    "        elif Params[\"Use_flat_sys_bkg\"] == True:\n",
    "            zero_bins = []\n",
    "            for i,val in enumerate(hist_dict[HNL_mass]['bkg_overlay'].values()):\n",
    "                if val == 0:\n",
    "                    zero_bins.append(i)\n",
    "                    print(f\"{HNL_mass} last bin 0, setting error to 2.0\")\n",
    "            if len(zero_bins) != 0:\n",
    "                bkg_sys_err_list = [hist_dict[HNL_mass]['bkg_overlay'].values()*Params[\"Flat_overlay_bkg_frac\"] + np.ones_like(hist_dict[HNL_mass]['bkg_overlay'].values())*2.0, #This is horrible need to rewrite \n",
    "                                    np.zeros_like(hist_dict[HNL_mass]['bkg_EXT'].errors()), #No systematic error on the EXT sample\n",
    "                                    hist_dict[HNL_mass]['bkg_dirt'].values()*Params[\"Flat_dirt_bkg_frac\"]]\n",
    "            else:    \n",
    "                bkg_sys_err_list = [hist_dict[HNL_mass]['bkg_overlay'].values()*Params[\"Flat_overlay_bkg_frac\"], \n",
    "                                    np.zeros_like(hist_dict[HNL_mass]['bkg_EXT'].errors()), #No systematic error on the EXT sample\n",
    "                                    hist_dict[HNL_mass]['bkg_dirt'].values()*Params[\"Flat_dirt_bkg_frac\"]]\n",
    "            bkg_err_list = [Functions.add_all_errors([bkg_stat_err_list[0],bkg_sys_err_list[0]]), #adding the sys and stat error in quadrature for each bkg type\n",
    "                            Functions.add_all_errors([bkg_stat_err_list[1],bkg_sys_err_list[1]]),\n",
    "                            Functions.add_all_errors([bkg_stat_err_list[2],bkg_sys_err_list[2]])]\n",
    "        elif Params[\"Use_flat_sys_bkg\"] == False:\n",
    "            ppfx_unc = hist_dict[HNL_mass][\"ppfx_uncertainty\"].values()\n",
    "            genie_unc = hist_dict[HNL_mass][\"Genie_uncertainty\"].values()\n",
    "            reint_unc = hist_dict[HNL_mass][\"Reinteraction_uncertainty\"].values()\n",
    "            # detvar_unc = bkg_detvar_dict[HNL_mass][\"Total_DetVar_uncertainty\"].values() #Don't know what this looks like yet, as I haven't made\n",
    "            detvar_unc = hist_dict[HNL_mass]['bkg_overlay'].values()*Params[\"Overlay_detvar_frac\"] #Just setting as flat. Too much variation in samples\n",
    "            tot_overlay_sys = Functions.add_all_errors([ppfx_unc, genie_unc, reint_unc, detvar_unc])\n",
    "            bkg_sys_err_list = [tot_overlay_sys, \n",
    "                                np.zeros_like(hist_dict[HNL_mass]['bkg_EXT'].errors()), #No systematic error on the EXT sample\n",
    "                                hist_dict[HNL_mass]['bkg_dirt'].values()*Params[\"Flat_dirt_bkg_frac\"]] #Don't have reweight or DetVar samples for dirt\n",
    "            bkg_err_list = [Functions.add_all_errors([bkg_stat_err_list[0],bkg_sys_err_list[0]]), #adding the sys and stat error in quadrature for each bkg type\n",
    "                            Functions.add_all_errors([bkg_stat_err_list[1],bkg_sys_err_list[1]]),\n",
    "                            Functions.add_all_errors([bkg_stat_err_list[2],bkg_sys_err_list[2]])]\n",
    "            bkg_stat_err_total = Functions.add_all_errors([bkg_stat_err_list[0],bkg_stat_err_list[1],bkg_stat_err_list[2]])\n",
    "            print(\"bkg stat error:\")\n",
    "            print(bkg_stat_err_total)\n",
    "            print(\"bkg flux error:\")\n",
    "            print(ppfx_unc)\n",
    "            print(\"bkg genie error:\")\n",
    "            print(genie_unc)\n",
    "            print(\"bkg reint error:\")\n",
    "            print(reint_unc)\n",
    "            \n",
    "        if (Params[\"Stats_only\"] == False) and (Params[\"Use_flat_sys_signal\"] == True):\n",
    "            zero_bins = []\n",
    "            for i,val in enumerate(hist_dict[HNL_mass]['signal'].values()):\n",
    "                if val == 0:\n",
    "                    zero_bins.append(i)\n",
    "                    print(f\"{HNL_mass} signal last bin 0, setting error to 2.0\")\n",
    "            if len(zero_bins) != 0:\n",
    "                sig_sys_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Flat_sig_frac\"]+2.0\n",
    "            else:\n",
    "                sig_sys_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Flat_sig_frac\"]\n",
    "            sig_err = Functions.add_all_errors([sig_stat_err,sig_sys_err])\n",
    "        if (Params[\"Stats_only\"] == False) and (Params[\"Use_flat_sys_signal\"] == False):\n",
    "            sig_detvar_err = sig_detvar_dict[HNL_mass][\"Total_DetVar_uncertainty\"].values()\n",
    "            sig_flux_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Signal_flux_error\"]\n",
    "            sig_err = Functions.add_all_errors([sig_stat_err,sig_detvar_err,sig_flux_err]) #Adding stat, detvar and flux errors in quadrature\n",
    "        total_bkg_err = Functions.add_all_errors(bkg_err_list) #Now adding the errors of overlay, EXT and dirt in quadrature\n",
    "        BKG_ERR_dict[HNL_mass] = total_bkg_err\n",
    "        SIGNAL_ERR_dict[HNL_mass] = sig_err\n",
    "        print(\"Total bkg error:\")\n",
    "        print(total_bkg_err)\n",
    "        print(\"Total signal error:\")\n",
    "        print(sig_err)\n",
    "    return BKG_ERR_dict, SIGNAL_ERR_dict\n",
    "    \n",
    "def Add_bkg_hists_make_signal(hist_dict):\n",
    "    BKG_dict, SIGNAL_dict = {}, {}\n",
    "    for HNL_mass in hist_dict:\n",
    "        bkg_hists = [hist_dict[HNL_mass]['bkg_EXT'], hist_dict[HNL_mass]['bkg_overlay'], hist_dict[HNL_mass]['bkg_dirt']]\n",
    "        \n",
    "        total_bkg = Functions.add_hists_vals(bkg_hists)\n",
    "        BKG_dict[HNL_mass] = total_bkg\n",
    "        SIGNAL_dict[HNL_mass] = hist_dict[HNL_mass]['signal'].values()\n",
    " \n",
    "    return BKG_dict, SIGNAL_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d595061-c00a-43c5-845f-16e9d176918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dict(Total_dict, debug=False):\n",
    "    model_dict = {}\n",
    "    \n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass][\"SIGNAL_dict\"],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None},\n",
    "                {\"name\": \"uncorr_siguncrt\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass][\"SIGNAL_ERR_dict\"]}\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"background\",\n",
    "              \"data\": Total_dict[HNL_mass][\"BKG_dict\"],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass][\"BKG_ERR_dict\"]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e370570b-20ae-4a1f-aeda-8a8b2e95ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([2, 10, 20, 50, 100, 150, 180, 200, 220, 240, 245])\n",
      "dict_keys([2, 10, 20, 50, 100, 150, 180, 200, 220, 240, 245])\n",
      "dict_keys([2, 10, 20, 50, 100, 150, 180, 200, 220, 240, 245])\n"
     ]
    }
   ],
   "source": [
    "R1_BKG_ERR_dict, R1_SIGNAL_ERR_dict = Calculate_total_uncertainty(Params_pyhf, hist_dict_run1)\n",
    "R3_BKG_ERR_dict, R3_SIGNAL_ERR_dict = Calculate_total_uncertainty(Params_pyhf, hist_dict_run3)\n",
    "\n",
    "R1_BKG, R1_SIGNAL = Add_bkg_hists_make_signal(hist_dict_run1)\n",
    "R3_BKG, R3_SIGNAL = Add_bkg_hists_make_signal(hist_dict_run3)\n",
    "\n",
    "R1_output = Functions.Make_into_lists(Params_pyhf, R1_BKG, R1_SIGNAL, R1_BKG_ERR_dict, R1_SIGNAL_ERR_dict)\n",
    "R3_output = Functions.Make_into_lists(Params_pyhf, R3_BKG, R3_SIGNAL, R3_BKG_ERR_dict, R3_SIGNAL_ERR_dict)\n",
    "\n",
    "list_input_dicts = [R1_output, R3_output]\n",
    "\n",
    "Total_dict = Functions.Create_final_appended_runs_dict(list_input_dicts)\n",
    "Total_dict_run1 = Functions.Create_final_appended_runs_dict([R1_output])\n",
    "Total_dict_run3 = Functions.Create_final_appended_runs_dict([R3_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd6b13-1435-4cdb-a37a-6ccddd5722be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scaling signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c1437b-4f00-499a-afb3-c0e2cc5bd296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_signal(Total_dict, theta_dict, scaling_dict={}):\n",
    "    if(scaling_dict=={}): raise Exception(\"Specify scalings\")\n",
    "    new_theta_dict = {}\n",
    "    for HNL_mass in Total_dict.keys():\n",
    "        new_signal_hist = np.array(Total_dict[HNL_mass]['SIGNAL_dict'])*scaling_dict[HNL_mass]\n",
    "        new_signal_err_hist = np.array(Total_dict[HNL_mass]['SIGNAL_ERR_dict'])*scaling_dict[HNL_mass]\n",
    "        new_theta = theta_dict[HNL_mass]*scaling_dict[HNL_mass]**(1/4) # Number of events is proportional to theta**4\n",
    "        \n",
    "        Total_dict[HNL_mass]['SIGNAL_dict'] = list(new_signal_hist)\n",
    "        Total_dict[HNL_mass]['SIGNAL_ERR_dict'] = list(new_signal_err_hist)\n",
    "        new_theta_dict[HNL_mass] = new_theta\n",
    "    return Total_dict, new_theta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad4601-b177-49a6-93d0-a7e4387d6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_SF = 2000\n",
    "# scaling_dict = {2:2000,10:10,20:10,50:10,100:10,150:10,180:10,200:10,220:10,240:10,245:10}\n",
    "scaling_dict = {2:single_SF,10:single_SF,20:single_SF,50:single_SF,100:single_SF,150:single_SF,180:single_SF,200:single_SF,220:single_SF,240:single_SF,245:single_SF}\n",
    "\n",
    "Total_dict, theta_dict  = scale_signal(Total_dict, theta_dict, scaling_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d44019-e751-4595-b5c9-6166719fa92f",
   "metadata": {},
   "source": [
    "## Creating model (only do once happy with scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "447c26da-ff4f-430b-9b9f-cec8641caabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created models\n"
     ]
    }
   ],
   "source": [
    "model_dict_both = create_model_dict(Total_dict)\n",
    "model_dict_run1 = create_model_dict(Total_dict_run1)\n",
    "model_dict_run3 = create_model_dict(Total_dict_run3)\n",
    "print(\"Created models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b092e9e5-a61e-448a-97f5-2a5bee784aeb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Making uncertainty breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba893d6-5821-489d-ba0a-30c6ab879cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Total_dict[2].keys())\n",
    "# print(Total_dict[2]['BKG_dict'])\n",
    "# print(Total_dict[2]['BKG_ERR_dict'])\n",
    "\n",
    "HNL_mass = 100\n",
    "test_dict = {}\n",
    "test_dict[HNL_mass] = hist_dict_run3[HNL_mass]\n",
    "\n",
    "print(\"Signal:\")\n",
    "print(test_dict[HNL_mass]['signal'].values())\n",
    "print(\"Background:\")\n",
    "print(test_dict[HNL_mass]['bkg_overlay'].values() + test_dict[HNL_mass]['bkg_EXT'].values() + test_dict[HNL_mass]['bkg_dirt'].values())\n",
    "print()\n",
    "print(\"Bkg overlay:\")\n",
    "print(test_dict[HNL_mass]['bkg_overlay'].values())\n",
    "print(\"Bkg EXT:\")\n",
    "print(test_dict[HNL_mass]['bkg_EXT'].values())\n",
    "print(\"Bkg dirt:\")\n",
    "print(test_dict[HNL_mass]['bkg_dirt'].values())\n",
    "print()\n",
    "\n",
    "TEST_BKG_ERR_dict, TEST_SIGNAL_ERR_dict = Uncertainty_breakdown(Params_pyhf, test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68fc82f-8497-4465-88d7-ea486bdb2f3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Making test statistic plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9ff10-074e-47fa-b9b8-e1b706a8c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 2 #Mass point to test\n",
    "DATA_OBS_dict = {}\n",
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "bkg_pars = init_pars.copy()\n",
    "bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "print(list(zip(model_dict[HNL_mass].config.parameters, init_pars)))\n",
    "print(list(zip(model_dict[HNL_mass].config.parameters, bkg_pars)))\n",
    "\n",
    "pdf_bkg = model_dict[HNL_mass].make_pdf(pyhf.tensorlib.astensor(bkg_pars)) #Making the pdfs\n",
    "pdf_sig = model_dict[HNL_mass].make_pdf(pyhf.tensorlib.astensor(init_pars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a12a35-c8b3-422d-b8a6-efd8ce89fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "\n",
    "# mu' = 0\n",
    "mc_bkg = pdf_bkg.sample((n_samples,))\n",
    "# mu' = 1\n",
    "mc_sig = pdf_sig.sample((n_samples,))\n",
    "\n",
    "print(mc_bkg.shape)\n",
    "print(mc_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e5770-2881-4cda-856d-0159004cf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_calculator_qtilde = pyhf.infer.utils.create_calculator( #only seems to support q-like test statistics\n",
    "    \"toybased\",\n",
    "    model_dict[HNL_mass].expected_data(init_pars),\n",
    "    model_dict[HNL_mass],\n",
    "    ntoys=n_samples,\n",
    "    test_stat=\"qtilde\",\n",
    ")\n",
    "qtilde_sig, qtilde_bkg = toy_calculator_qtilde.distributions(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97268c1e-74e7-4c6d-a8c5-ffa08eb621c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmu_bounds = model_dict[HNL_mass].config.suggested_bounds()\n",
    "print(f\"Old bounds: {qmu_bounds}\")\n",
    "qmu_bounds[model_dict[HNL_mass].config.poi_index] = (-10, 10)\n",
    "print(f\"New bounds: {qmu_bounds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1247f-80ae-490f-9799-98bf2f5fda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_calculator_qmu = pyhf.infer.utils.create_calculator(\n",
    "    \"toybased\",\n",
    "    model_dict[HNL_mass].expected_data(model_dict[HNL_mass].config.suggested_init()),\n",
    "    model_dict[HNL_mass],\n",
    "    par_bounds=qmu_bounds,\n",
    "    ntoys=n_samples,\n",
    "    test_stat=\"q\",\n",
    ")\n",
    "qmu_sig, qmu_bkg = toy_calculator_qmu.distributions(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cd24f-3977-4835-8406-7b20bad19c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(qmu_sig.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bd896-544c-42f1-9d23-80c783ef6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "nbins = 10 #Should make an array for the bin edges\n",
    "bins = np.linspace(0,0.35,nbins)\n",
    "plt.hist(\n",
    "    qmu_sig.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=\"$f(q_1|1)$ signal-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.hist(\n",
    "    qmu_bkg.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=\"$f(q_1|0)$ background-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.xlabel(r\"$q_1$\", fontsize=18)\n",
    "plt.ylabel(r\"$f\\,(q_1|\\mu')$\", fontsize=18)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad355c4-c5e1-4840-8f8c-61ca0b68ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(qtilde_sig.samples))\n",
    "print(max(qtilde_bkg.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195623a-0f61-454a-a7ef-88bd9e3a0574",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "nbins = 30 #Should make an array for the bin edges\n",
    "bins = np.linspace(0,45,nbins)\n",
    "plt.hist(\n",
    "    qtilde_sig.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_1|1)$ signal-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.hist(\n",
    "    qtilde_bkg.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_1|0)$ background-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "value_for_prob = 2.16\n",
    "\n",
    "plt.axvline(x = value_for_prob, color = 'red', label = '90% exclusion')\n",
    "\n",
    "plt.xlabel(r\"$\\tilde{q}_1$\", fontsize=22)\n",
    "plt.ylabel(r\"$f\\,(\\tilde{q}_1|\\mu')$\", fontsize=22)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd3b009-a332-44f5-ae5e-ebc7e68ce988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding probability for one value of test statistic\n",
    "ordered_test_stat = np.sort(qtilde_sig.samples)\n",
    "length =len(ordered_test_stat)\n",
    "\n",
    "prob = 0.9\n",
    "slice_at = 0.9*length\n",
    "print(np.floor(slice_at))\n",
    "print(ordered_test_stat[int(slice_at)])\n",
    "value_for_prob = ordered_test_stat[int(slice_at)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1ef3c-b55b-4830-8f63-436da533686c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Testing single point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c533c-cc65-4f3c-8be0-dee7b236807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 10 #Mass point to test\n",
    "DATA_OBS_dict = {}\n",
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "bkg_pars = init_pars.copy()\n",
    "bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass][\"BKG_dict\"]+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])\n",
    "\n",
    "CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"asymptotics\",\n",
    "            )\n",
    "for expected_value, n_sigma in zip(CLs_exp, np.arange(-1, 2)):\n",
    "    print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")\n",
    "    \n",
    "poi_values = np.linspace(0.001, 10, 50)\n",
    "obs_limit_single, exp_limits_single, (scan, results) = pyhf.infer.intervals.upperlimit(DATA_OBS_dict[HNL_mass], \n",
    "                                                                                       model_dict[HNL_mass], poi_values, \n",
    "                                                                                       level=0.1, return_results=True)\n",
    "print(f\"Upper limit {HNL_mass}MeV (obs): μ = {obs_limit_single:.4f}\")\n",
    "print(f\"Upper limit {HNL_mass}MeV (exp): μ = {exp_limits_single[2]:.4f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30a6bc-7a6c-47b4-89df-7688667c8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = []\n",
    "obs_limit = []\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in [10]:\n",
    "    theta_squared = (theta_dict[HNL_mass])**2\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_single[2])*theta_squared\n",
    "    LIMIT = np.sqrt(obs_limit_single)*theta_squared\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d29e6-f7f4-4088-b9c5-0ccc2fd1f419",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running through all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ace42d-de08-48dd-8c1a-ed51a6f79b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for these masses: [2, 10, 20, 50, 100, 150, 180, 200, 220, 240, 245]\n"
     ]
    }
   ],
   "source": [
    "model_dict = model_dict_both\n",
    "list_test = Constants.HNL_mass_samples\n",
    "print(\"Running for these masses: \" + str(list_test))\n",
    "\n",
    "DATA_OBS_dict = {}\n",
    "\n",
    "for HNL_mass in list_test:\n",
    "    init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "    model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "    bkg_pars = init_pars.copy()\n",
    "    bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "    model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "    list_keys = list(Total_dict[HNL_mass].keys())\n",
    "    if \"data\" in list_keys: #haven't made this yet, need to test\n",
    "        Total_dict[HNL_mass][\"data\"]+model_dict[HNL_mass].config.auxdata\n",
    "    else:\n",
    "        DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass][\"BKG_dict\"]+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "    model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4234fa8-33c9-48dc-9eaa-674262afcba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for HNL_mass in list_test:\n",
    "\n",
    "    if Params_pyhf[\"Use_toys\"] == False:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"asymptotics\",\n",
    "            )\n",
    "    if Params_pyhf[\"Use_toys\"] == True:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"toybased\",\n",
    "            ntoys=Params_pyhf[\"Num_toys\"],\n",
    "            track_progress=True,\n",
    "            )\n",
    "    \n",
    "    print(f\"{HNL_mass}MeV\")\n",
    "    for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "        print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727ad3d-56ab-42bf-ae92-e24e5b50f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "print(\"If the output of the following is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "poi_values = np.linspace(0.001, 2, 100) #I could make a dict of this and have different pois for different mass points\n",
    "print(\"Max value is \" + str(max(poi_values)))\n",
    "print(\"Min value is \" + str(min(poi_values)))\n",
    "print(\"Next value is \" + str(poi_values[1]))\n",
    "print(\"Next value is \" + str(poi_values[2]) + \"\\n\")\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "# list_test = [180, 200, 220, 240, 245]\n",
    "for HNL_mass in list_test:\n",
    "\n",
    "    # poi_values = np.linspace(0.001, 4, 100)\n",
    "    obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "        DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_values, level=0.1, return_results=True\n",
    "    )\n",
    "\n",
    "    print(f\"Upper limit {HNL_mass}MeV (obs): μ = {obs_limit_dict[HNL_mass]:.6f}\")\n",
    "    print(f\"Upper limit {HNL_mass}MeV (exp): μ = {exp_limits_dict[HNL_mass][2]:.6f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ea839-501a-4cfd-9b20-0ba1fa544900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((scan, results))\n",
    "# print(scan)\n",
    "print(results[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0dcf7f-0665-430c-8239-66452b8bf24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = [] #entry 2\n",
    "exp_1sig_up = [] #entry 3\n",
    "exp_1sig_down = [] #entry 1\n",
    "exp_2sig_up = [] #entry 4\n",
    "exp_2sig_down = [] #entry 0\n",
    "obs_limit = []\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in exp_limits_dict:\n",
    "    theta_squared = (theta_dict[HNL_mass])**2\n",
    "    print(theta_squared)\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_dict[HNL_mass][2])*theta_squared\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][0])*theta_squared)\n",
    "    \n",
    "    LIMIT = np.sqrt(obs_limit_dict[HNL_mass])*theta_squared\n",
    "    # if HNL_mass in Constants.Old_generator_mass_points:\n",
    "    #     EXP_LIMIT = EXP_LIMIT*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "    #     LIMIT = LIMIT*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8437737-0007-42bb-9cfa-bd8e2a45194e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving limit as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e73a7-2fee-4aad-9067-e3dd5d7d9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = list_test\n",
    "\n",
    "if Params_pyhf[\"Stats_only\"] == True:\n",
    "    stats =  \"Stats_only\"\n",
    "else:\n",
    "    stats = \"Flat_sys\"\n",
    "    \n",
    "if Params_pyhf[\"Use_second_half_only\"] == True:\n",
    "    half_hist = \"havled\"\n",
    "else:\n",
    "    half_hist = \"full_hist\"\n",
    "\n",
    "print(masses)\n",
    "print(exp_limit)\n",
    "\n",
    "r = zip(masses, exp_limit)\n",
    "if Params_pyhf[\"Load_pi0_hists\"] == False:\n",
    "    with open(f'limit_files/My_limits/{stats}_{half_hist}_expected_mu_FINAL.csv', \"w\") as s:\n",
    "        w = csv.writer(s)\n",
    "        for row in r:\n",
    "            w.writerow(row)\n",
    "            \n",
    "if Params_pyhf[\"Load_pi0_hists\"] == True:\n",
    "    with open(f'limit_files/My_limits/{stats}_{half_hist}_expected_pi0_mu_FINAL.csv', \"w\") as s:\n",
    "        w = csv.writer(s)\n",
    "        for row in r:\n",
    "            w.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeac33b-58bc-4054-b1f4-20fe1f20cf3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Testing adjacent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e7cd9-cf28-49f0-a3a0-bef0f63c1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacent_hist_dict_run1, adjacent_hist_dict_run3 = {}, {}\n",
    "R1_BKG_ERR_dict_ADJ, R3_BKG_ERR_dict_ADJ, R1_SIGNAL_ERR_dict_ADJ, R3_SIGNAL_ERR_dict_ADJ = {}, {}, {}, {}\n",
    "mass_point = 50\n",
    "test_models = [20, 100]\n",
    "for mass_model in test_models:\n",
    "    adjacent_hist_dict_run1[mass_model] = uproot.open(f'bdt_output/adjacent_models/run1_{mass_point}MeV_{mass_model}MeV_model.root')\n",
    "    adjacent_hist_dict_run3[mass_model] = uproot.open(f'bdt_output/adjacent_models/run3_{mass_point}MeV_{mass_model}MeV_model.root')\n",
    "    R1_BKG_ERR_dict_ADJ, R1_SIGNAL_ERR_dict_ADJ = Calculate_total_uncertainty(Params_pyhf, adjacent_hist_dict_run1)\n",
    "    R3_BKG_ERR_dict_ADJ, R3_SIGNAL_ERR_dict_ADJ = Calculate_total_uncertainty(Params_pyhf, adjacent_hist_dict_run3)\n",
    "    # R1_BKG_ERR_dict_ADJ[mass_model]=R1_BKG_ERR_dict[mass_point]\n",
    "    # R3_BKG_ERR_dict_ADJ[mass_model]=R3_BKG_ERR_dict[mass_point]\n",
    "    # R1_SIGNAL_ERR_dict_ADJ[mass_model]=R1_SIGNAL_ERR_dict[mass_point]\n",
    "    # R3_SIGNAL_ERR_dict_ADJ[mass_model]=R3_SIGNAL_ERR_dict[mass_point]\n",
    "adjacent_hist_dict_run1[mass_point] = hist_dict_run1[mass_point]\n",
    "adjacent_hist_dict_run3[mass_point] = hist_dict_run3[mass_point]\n",
    "R1_BKG_ERR_dict_ADJ[mass_point]=R1_BKG_ERR_dict[mass_point]\n",
    "R3_BKG_ERR_dict_ADJ[mass_point]=R3_BKG_ERR_dict[mass_point]\n",
    "R1_SIGNAL_ERR_dict_ADJ[mass_point]=R1_SIGNAL_ERR_dict[mass_point]\n",
    "R3_SIGNAL_ERR_dict_ADJ[mass_point]=R3_SIGNAL_ERR_dict[mass_point]\n",
    "\n",
    "R1_BKG_ADJ, R1_SIGNAL_ADJ = Add_bkg_hists_make_signal(adjacent_hist_dict_run1)\n",
    "R3_BKG_ADJ, R3_SIGNAL_ADJ = Add_bkg_hists_make_signal(adjacent_hist_dict_run3)\n",
    "\n",
    "R1_output_ADJ = Functions.Make_into_lists(Params_pyhf, R1_BKG_ADJ, R1_SIGNAL_ADJ, R1_BKG_ERR_dict_ADJ, R1_SIGNAL_ERR_dict_ADJ)\n",
    "R3_output_ADJ = Functions.Make_into_lists(Params_pyhf, R3_BKG_ADJ, R3_SIGNAL_ADJ, R3_BKG_ERR_dict_ADJ, R3_SIGNAL_ERR_dict_ADJ)\n",
    "\n",
    "list_input_dicts_ADJ = [R1_output_ADJ, R3_output_ADJ]\n",
    "# list_input_dicts = [R1_output, R1_output] #Used when I didn't have Run3\n",
    "\n",
    "Total_dict_ADJ = Functions.Create_final_appended_runs_dict(list_input_dicts_ADJ)\n",
    "print(Total_dict_ADJ[100].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af73b4a8-0dad-4d60-a962-c7155024db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Total_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7688a367-1e06-44a8-95df-464d27a37a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_ADJ = create_model_dict(Total_dict_ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eeb77d-75dc-4454-9988-2c852640c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test = adjacent_hist_dict_run1.keys()\n",
    "DATA_OBS_dict = {}\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples: #removing the 240MeV point\n",
    "for HNL_mass in list_test:\n",
    "    init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "    model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "    bkg_pars = init_pars.copy()\n",
    "    bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "    model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "    DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass][\"BKG_dict\"]+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "    model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639d271-7e7c-4cff-9ed6-d20f1f7095cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for HNL_mass in list_test:\n",
    "\n",
    "    if Params_pyhf[\"Use_toys\"] == False:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"asymptotics\",\n",
    "            )\n",
    "    if Params_pyhf[\"Use_toys\"] == True:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"toybased\",\n",
    "            ntoys=Params_pyhf[\"Num_toys\"],\n",
    "            track_progress=True,\n",
    "            )\n",
    "    \n",
    "    print(f\"{HNL_mass}MeV\")\n",
    "    for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "        print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ab734-f7dd-40bb-8b3a-312eda608e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "print(\"If the output of the following is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "poi_values = np.linspace(0.001, 2, 100) #I could make a dict of this and have different pois for different mass points\n",
    "print(\"Max value is \" + str(max(poi_values)))\n",
    "print(\"Min value is \" + str(min(poi_values)))\n",
    "print(\"Next value is \" + str(poi_values[1]))\n",
    "print(\"Next value is \" + str(poi_values[2]) + \"\\n\")\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "# list_test = [180, 200, 220, 240, 245]\n",
    "for HNL_mass in list_test:\n",
    "\n",
    "    # poi_values = np.linspace(0.001, 4, 100)\n",
    "    obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "        DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_values, level=0.1, return_results=True\n",
    "    )\n",
    "\n",
    "    print(f\"Upper limit {HNL_mass}MeV (obs): μ = {obs_limit_dict[HNL_mass]:.6f}\")\n",
    "    print(f\"Upper limit {HNL_mass}MeV (exp): μ = {exp_limits_dict[HNL_mass][2]:.6f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb324293-40db-4be3-94b0-4498860ed94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = [] #entry 2\n",
    "exp_1sig_up = [] #entry 3\n",
    "exp_1sig_down = [] #entry 1\n",
    "exp_2sig_up = [] #entry 4\n",
    "exp_2sig_down = [] #entry 0\n",
    "obs_limit = []\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in exp_limits_dict:\n",
    "    theta_squared = (theta_dict[mass_point])**2\n",
    "    print(theta_squared)\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_dict[HNL_mass][2])*theta_squared\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][0])*theta_squared)\n",
    "    \n",
    "    LIMIT = np.sqrt(obs_limit_dict[HNL_mass])*theta_squared\n",
    "    # if HNL_mass in Constants.Old_generator_mass_points:\n",
    "    #     EXP_LIMIT = EXP_LIMIT*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "    #     LIMIT = LIMIT*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a5ecc-57ca-4f56-bc74-cc00bdadfdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [6.912810521978472e-06, 7.3827958770328454e-06, 1.9840221581842897e-05]\n",
    "x_points = [50, 50, 50]\n",
    "labels = [\"correct\", \"20MeV\", \"50MeV\"]\n",
    "for i, point in enumerate(points):\n",
    "    plt.plot(x_points[i],point,label=labels[i],marker=\"o\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c38a19-4c02-4f26-8aef-5947b26cf4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_20 = 6.912810521978472e-06/7.3827958770328454e-06\n",
    "rat_100 = 6.912810521978472e-06/1.9840221581842897e-05\n",
    "print(\"20MeV model is \" + str(rat_20))\n",
    "print(\"100MeV model is \" + str(rat_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c1c24-58ae-4433-8b1c-30f24a80e5b1",
   "metadata": {},
   "source": [
    "## Brazil plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2d76f-c16f-47a0-8c41-3889c5844e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "savefig = True\n",
    "# exp_limit = [] #entry 2\n",
    "# exp_1sig_up = [] #entry 3\n",
    "# exp_1sig_down = [] #entry 1\n",
    "# exp_2sig_up = [] #entry 4\n",
    "# exp_2sig_down = [] #entry 0\n",
    "\n",
    "# plt.plot(masses_hnl_val,np.sqrt(np.array(obs_limit)),lw=5,ls='-',color='black',label='Observed')\n",
    "plt.plot(Constants.HNL_mass_samples,np.array(exp_limit),lw=2,ls='--',color='red',label='Expected')\n",
    "plt.fill_between(Constants.HNL_mass_samples,np.array(exp_2sig_down),np.array(exp_2sig_up),color='yellow',label=r'Exp. 2$\\sigma$')\n",
    "plt.fill_between(Constants.HNL_mass_samples,np.array(exp_1sig_down),np.array(exp_1sig_up),color='lightgreen',label=r'Exp. 1$\\sigma$')\n",
    "# plt.fill_between(masses,m2s,p2s,color='lightgreen',label=r'Exp. 2$\\sigma$')\n",
    "# plt.grid(ls='--',color='C7',alpha=0.1)\n",
    "\n",
    "plt.ylabel(r'$|U_{\\mu 4}|^2$ Limit at 90% CL',fontsize=20)\n",
    "plt.xlabel('HNL Mass [MeV]',fontsize=20)\n",
    "# plt.title(\"HNL Mixing Upper Limit\")\n",
    "plt.ylim(1e-8,1e-3)\n",
    "plt.legend(loc=\"lower left\",ncol=2,frameon=False,fontsize=20)\n",
    "# plt.text(0.99,0.95,r'MicroBooNE NuMI POT:$7.01\\times 10^{20}$',fontsize=20,color='black',alpha=1,verticalalignment='top',horizontalalignment='right',transform=plt.gca().transAxes)\n",
    "plt.tick_params(axis='x', labelsize=20)\n",
    "plt.tick_params(axis='y', labelsize=20)\n",
    "plt.xlim(0,250)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "\n",
    "if savefig == True:\n",
    "    plt.savefig('plots/Limits/Brazil_preliminary.pdf',bbox_inches='tight', pad_inches=0.3)\n",
    "\n",
    "plt.grid(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf87921-c9cd-4488-95f0-b359183879ca",
   "metadata": {},
   "source": [
    "## End of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814efb9e-a208-4b77-87ec-8e9b203f45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_vals = input(\"Do you want to run with new pois? y/n \")\n",
    "if print_vals == \"y\":\n",
    "    poi_new_dict = {}\n",
    "    for HNL_mass in list_test:\n",
    "        best_fit_value = exp_limits_dict[HNL_mass][2]\n",
    "        new_poi_array = np.linspace(0.001, best_fit_value*2, 100)\n",
    "        poi_new_dict[HNL_mass] = new_poi_array\n",
    "\n",
    "    obs_limit_dict = {}\n",
    "    exp_limits_dict = {}\n",
    "    print(\"If the output of the following is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "    # for HNL_mass in Constants.HNL_mass_samples:\n",
    "    for HNL_mass in list_test:\n",
    "\n",
    "        obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "            DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_new_dict[HNL_mass], level=0.1, return_results=True\n",
    "        )\n",
    "\n",
    "        print(f\"Upper limit {HNL_mass}MeV (obs): μ = {obs_limit_dict[HNL_mass]:.4f}\")\n",
    "        print(f\"Upper limit {HNL_mass}MeV (exp): μ = {exp_limits_dict[HNL_mass][2]:.4f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a259396-713f-4435-99e8-870067d7c520",
   "metadata": {},
   "source": [
    "## Finished code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1787c6f6-155b-472d-8587-627baeaa4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def New_BR_limit(Old_theta_squared, HNL_mass):\n",
    "#     New_limit = Old_theta_squared*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "#     print(\"Old limit was \" + str(Old_theta_squared))\n",
    "#     print(\"New limit is \" + str(New_limit))\n",
    "#     return New_limit\n",
    "\n",
    "# Old_limits = {20:0.00015681119188699395,\n",
    "#               50:9.92373190724973e-06,\n",
    "#               100:9.84200063077575e-07,\n",
    "#               150:1.2830647341135436e-07,\n",
    "#               180:7.604092480424186e-08}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15b7ad-a343-44ab-8045-dbd8ee626630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New_limits = []\n",
    "# for HNL_mass in list_test:\n",
    "#     New_limits.append(New_BR_limit(Old_limits[HNL_mass], HNL_mass))\n",
    "    \n",
    "# r = zip([20,50,100,150,180], New_limits)\n",
    "\n",
    "# with open(f'limit_files/New_BR_expected_mu.csv', \"w\") as s:\n",
    "#     w = csv.writer(s)\n",
    "#     for row in r:\n",
    "#         w.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656fea3-8b53-45fc-8cc6-914f6b9f8575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "#     new_scaling = np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "#     print(\"New scaling is \" + str(new_scaling))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d35cf-1a71-42a7-ba8a-06a8fe0c30b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Total_dict[20][\"SIGNAL_ERR_dict\"])\n",
    "\n",
    "#print(Total_dict[20][\"SIGNAL_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5fe72-6da3-4db9-af90-38ee337aec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SIGNAL_dict = {}\n",
    "TOTAL_SIGNAL_ERR_dict = {}\n",
    "TOTAL_BKG_dict = {}\n",
    "TOTAL_BKG_ERR_dict = {}\n",
    "TOTAL_DATA_dict = {}\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    \n",
    "    r1_signal = hist_dict_run1[HNL_mass]['signal']\n",
    "    r1_EXT = hist_dict_run1[HNL_mass]['bkg_EXT']\n",
    "    r1_nu = hist_dict_run1[HNL_mass]['bkg_overlay']\n",
    "    r1_dirt = hist_dict_run1[HNL_mass]['bkg_dirt']\n",
    "    r1_data = hist_dict_run1[HNL_mass]['data']\n",
    "    \n",
    "    r3_signal = hist_dict_run3[HNL_mass]['signal']\n",
    "    r3_EXT = hist_dict_run3[HNL_mass]['bkg_EXT']\n",
    "    r3_nu = hist_dict_run3[HNL_mass]['bkg_overlay']\n",
    "    r3_dirt = hist_dict_run3[HNL_mass]['bkg_dirt']\n",
    "    r3_data = hist_dict_run3[HNL_mass]['data']\n",
    "    \n",
    "    r1_bkg_hists = [r1_EXT, r1_nu, r1_dirt]\n",
    "    r1_total_bkg = add_hists_vals(r1_bkg_hists)\n",
    "    \n",
    "    r3_bkg_hists = [r3_EXT, r3_nu, r3_dirt]\n",
    "    r3_total_bkg = add_hists_vals(r3_bkg_hists)\n",
    "    \n",
    "    if Params_pyhf[\"Stats_only\"] == True:\n",
    "        overlay_r1_err = get_stat_errors(r1_nu)\n",
    "        dirt_r1_err = get_stat_errors(r1_dirt)\n",
    "        \n",
    "        overlay_r3_err = get_stat_errors(r3_nu)\n",
    "        dirt_r3_err = get_stat_errors(r3_dirt)\n",
    "        \n",
    "        r1_sig_err = get_stat_errors(r1_signal)\n",
    "        r3_sig_err = get_stat_errors(r3_signal)\n",
    "        \n",
    "    elif Params_pyhf[\"Stats_only\"] == False:\n",
    "        overlay_r1_err = get_full_errors_nu_FLAT_INPUTS(r1_nu)\n",
    "        dirt_r1_err = get_full_errors_dirt(r1_dirt)\n",
    "      \n",
    "        overlay_r3_err = get_full_errors_nu_FLAT_INPUTS(r3_nu)\n",
    "        dirt_r3_err = get_full_errors_dirt(r3_dirt)\n",
    "        \n",
    "        r1_sig_err = get_full_errors_signal(r1_signal)\n",
    "        r3_sig_err = get_full_errors_signal(r3_signal)\n",
    "    \n",
    "    \n",
    "    r1_bkg_err_list = [overlay_r1_err, r1_EXT.errors(), dirt_r1_err]\n",
    "    r1_total_bkg_err = add_all_errors(r1_bkg_err_list)\n",
    "\n",
    "    r3_bkg_err_list = [overlay_r3_err, r3_EXT.errors(), dirt_r3_err]\n",
    "    r3_total_bkg_err = add_all_errors(r3_bkg_err_list)\n",
    "    \n",
    "    #Converting np.ndarrays to lists\n",
    "    SIGNAL_R1 = np.ndarray.tolist(r1_signal.values())\n",
    "    SIGNAL_ERR_R1 = np.ndarray.tolist(r1_sig_err)\n",
    "    BKG_R1 = np.ndarray.tolist(r1_total_bkg)\n",
    "    BKG_ERR_R1 = np.ndarray.tolist(r1_total_bkg_err)\n",
    "    DATA_R1 = np.ndarray.tolist(r1_data.values())\n",
    "    \n",
    "    SIGNAL_R3 = np.ndarray.tolist(r3_signal.values())\n",
    "    SIGNAL_ERR_R3 = np.ndarray.tolist(r3_sig_err)\n",
    "    BKG_R3 = np.ndarray.tolist(r3_total_bkg)\n",
    "    BKG_ERR_R3 = np.ndarray.tolist(r3_total_bkg_err)\n",
    "    DATA_R3 = np.ndarray.tolist(r3_data.values())\n",
    "    \n",
    "    list_of_lists = [SIGNAL_R1, SIGNAL_ERR_R1, BKG_R1, BKG_ERR_R1, DATA_R1, SIGNAL_R3, SIGNAL_ERR_R3, BKG_R3, BKG_ERR_R3, DATA_R3]\n",
    "    if Params_pyhf[\"Use_second_half_only\"] == True:\n",
    "        for n in range(len(list_of_lists)):\n",
    "            list_of_lists[n]=remove_first_half_hist(list_of_lists[n])\n",
    "    \n",
    "    TOTAL_SIGNAL_dict[HNL_mass] = append_r3_to_r1(list_of_lists[0], list_of_lists[5])\n",
    "    TOTAL_SIGNAL_ERR_dict[HNL_mass] = append_r3_to_r1(list_of_lists[1], list_of_lists[6])\n",
    "    TOTAL_BKG_dict[HNL_mass] = append_r3_to_r1(list_of_lists[2], list_of_lists[7])\n",
    "    TOTAL_BKG_ERR_dict[HNL_mass] = append_r3_to_r1(list_of_lists[3], list_of_lists[8])\n",
    "    TOTAL_DATA_dict[HNL_mass] = append_r3_to_r1(list_of_lists[4], list_of_lists[9])\n",
    "    \n",
    "    # print(f\"Total {HNL_mass}MeV signal is \")\n",
    "    # print(SIGNAL_R1)\n",
    "\n",
    "# print()\n",
    "# print(\"Total bkg is \")\n",
    "# print(r1_total_bkg)\n",
    "\n",
    "# print()\n",
    "# print(\"Total bkg error is  \")\n",
    "# print(r1_total_bkg_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce615d-8780-424f-9372-21a3c27a84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "# for HNL_mass in HNL_masses:\n",
    "#     model_dict[HNL_mass] = pyhf.simplemodels.uncorrelated_background(signal=TOTAL_SIGNAL_dict[HNL_mass],\n",
    "#                                                                      bkg=TOTAL_BKG_dict[HNL_mass], \n",
    "#                                                                      bkg_uncertainty=TOTAL_BKG_ERR_dict[HNL_mass])\n",
    "    \n",
    "for HNL_mass in HNL_masses:\n",
    "    model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "  \"channels\": [\n",
    "    {\n",
    "      \"name\": \"singlechannel\",\n",
    "      \"samples\": [\n",
    "        {\n",
    "          \"name\": \"signal\",\n",
    "          \"data\": TOTAL_SIGNAL_dict[HNL_mass],\n",
    "          \"modifiers\": [\n",
    "            {\n",
    "              \"name\": \"mu\",\n",
    "              \"type\": \"normfactor\",\n",
    "              \"data\": None\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"uncorr_siguncrt\",\n",
    "              \"type\": \"shapesys\",\n",
    "              \"data\": TOTAL_SIGNAL_ERR_dict[HNL_mass]  \n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"background\",\n",
    "          \"data\": TOTAL_BKG_dict[HNL_mass],\n",
    "          \"modifiers\": [\n",
    "            {\n",
    "              \"name\": \"uncorr_bkguncrt\",\n",
    "              \"type\": \"shapesys\",\n",
    "              \"data\": TOTAL_BKG_ERR_dict[HNL_mass]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e2625-7dc1-486d-9d33-a3c051f0584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_full = pyhf.model.uncorrelated_background(signal=TOTAL_SIGNAL, signal_uncertainty=TOTAL_SIGNAL_ERR, bkg=TOTAL_BKG, bkg_uncertainty=TOTAL_BKG_ERR)\n",
    "# model_full\n",
    "# print(json.dumps(model.spec, indent=2))\n",
    "# model.config.param_set(\"uncorr_bkguncrt\").n_parameters\n",
    "#model.config.param_set(\"uncorr_siguncrt\").n_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3cf9e-299d-42f0-afca-e541978bb7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(json.dumps(model.spec, indent=2))\n",
    "DATA_OBS_dict = {}\n",
    "for HNL_mass in HNL_masses:\n",
    "    init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "    model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "    bkg_pars = init_pars.copy()\n",
    "    bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "    model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "    DATA_OBS_dict[HNL_mass] = TOTAL_DATA_dict[HNL_mass]+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "    model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a5f893-b271-4da1-bf61-f16bd3fb3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyhf.infer.mle.fit(data=DATA_OBS, pdf=model)\n",
    "for HNL_mass in HNL_masses:\n",
    "    CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "        1.0,  # null hypothesis\n",
    "        DATA_OBS_dict[HNL_mass],\n",
    "        model_dict[HNL_mass],\n",
    "        test_stat=\"qtilde\",\n",
    "        return_expected_set=True,\n",
    "        )\n",
    "    \n",
    "#print(f\"      Observed CLs: {CLs_obs:.4f}\")\n",
    "    print(f\"{HNL_mass}MeV\")\n",
    "    for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "        print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433df84e-f04e-45cf-9a8f-84f7cf889b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "print(\"If the output of the following is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "\n",
    "    poi_values = np.linspace(0.001, 10, 100)\n",
    "    obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "        DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_values, level=0.1, return_results=True\n",
    "    )\n",
    "    print(f\"Upper limit {HNL_mass}MeV (obs): μ = {obs_limit_dict[HNL_mass]:.4f}\")\n",
    "    print(f\"Upper limit {HNL_mass}MeV (exp): μ = {exp_limits_dict[HNL_mass][2]:.4f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5451d-aad4-4dfb-87e6-fd111ea3bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bdt_output/new_theta_dict.pkl', 'rb') as handle:\n",
    "    new_theta_dict = pickle.load(handle)\n",
    "print(new_theta_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e89ee7-9741-419d-8dfa-faee7b318a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mass point\n",
    "with open('bdt_output/new_theta_dict.pkl', 'rb') as handle:\n",
    "    new_theta_dict = pickle.load(handle)\n",
    "#print(new_theta_dict)\n",
    "scaled_thetas = new_theta_dict #Saved in 3.5_BDT_Result\n",
    "\n",
    "exp_limit = []\n",
    "obs_limit = []\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    theta_squared = (scaled_thetas[HNL_mass])**2\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_dict[HNL_mass][2])*theta_squared\n",
    "    LIMIT = np.sqrt(obs_limit_dict[HNL_mass])*theta_squared\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)\n",
    "# print()\n",
    "# print(\"Owen's expected limit is \" + str(Owen_exp_limit))\n",
    "# print(\"Owen's observed limit is \" + str(Owen_obs_limit))\n",
    "\n",
    "# print()\n",
    "# perc_diff_exp = (1-(EXP_LIMIT/Owen_exp_limit))*100\n",
    "# perc_diff_obs = (1-(LIMIT/Owen_obs_limit))*100\n",
    "\n",
    "# print(\"pyhf expected limit is \" + str(perc_diff_exp) + \" different from Owen's limit.\")\n",
    "# print(\"pyhf observed limit is \" + str(perc_diff_obs) + \" different from Owen's limit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69557119-e9c6-430c-a6b5-b6ddaea4613f",
   "metadata": {},
   "source": [
    "## Saving Limits as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197c589-b82b-4105-be66-ec902f4ffafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = HNL_masses\n",
    "\n",
    "if Params_pyhf[\"Stats_only\"] == True:\n",
    "    stats =  \"Stats_only\"\n",
    "else:\n",
    "    stats = \"Owen_sys\"\n",
    "    \n",
    "if Params_pyhf[\"Use_second_half_only\"] == True:\n",
    "    half_hist = \"havled\"\n",
    "else:\n",
    "    half_hist = \"full_hist\"\n",
    "\n",
    "print(masses)\n",
    "print(exp_limit)\n",
    "\n",
    "r = zip(masses, exp_limit)\n",
    "\n",
    "with open(f'limit_files/{stats}_{half_hist}_expected_mu_COMBINED_highest_E.csv', \"w\") as s:\n",
    "    w = csv.writer(s)\n",
    "    for row in r:\n",
    "        w.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1405e88b-4862-4f4f-9aae-df0dc1924ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc7172-6236-477f-9d5c-1a50e81519ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 100\n",
    "print(json.dumps(model_dict[HNL_mass].spec, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01974e7f-3d17-4287-ba74-5328c603a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_full_sys = {}\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    model_dict_full_sys[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "  \"channels\": [\n",
    "    {\n",
    "      \"name\": \"singlechannel\",\n",
    "      \"samples\": [\n",
    "        {\n",
    "          \"name\": \"signal\",\n",
    "          \"data\": TOTAL_SIGNAL_dict[HNL_mass],\n",
    "          \"modifiers\": [\n",
    "            {\n",
    "              \"name\": \"mu\",\n",
    "              \"type\": \"normfactor\",\n",
    "              \"data\": None\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"uncorr_siguncrt\",\n",
    "              \"type\": \"shapesys\",\n",
    "              \"data\": TOTAL_SIGNAL_ERR_dict[HNL_mass]  \n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"background\",\n",
    "          \"data\": TOTAL_BKG_dict[HNL_mass],\n",
    "          \"modifiers\": [\n",
    "            {\n",
    "              \"name\": \"uncorr_bkguncrt\",\n",
    "              \"type\": \"shapesys\",\n",
    "              \"data\": TOTAL_BKG_ERR_dict[HNL_mass]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416cdf2-0f11-4ae6-ac1b-9260325f2396",
   "metadata": {},
   "source": [
    "# Adding in signal systematic uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d3408-b6f7-4cc9-bad2-86f8bb48220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Messing around with model\n",
    "\n",
    "full_model = pyhf.Model(\n",
    "    {\n",
    "  \"channels\": [\n",
    "    {\n",
    "      \"name\": \"singlechannel\",\n",
    "      \"samples\": [\n",
    "        {\n",
    "          \"name\": \"signal\",\n",
    "          \"data\": [\n",
    "            0.4354482889175415,\n",
    "            0.6531724333763123,\n",
    "            0.9367626905441284,\n",
    "            1.1947383880615234,\n",
    "            1.3539148569107056,\n",
    "            1.192908763885498,\n",
    "            0.6879351139068604,\n",
    "            0.2671237289905548,\n",
    "            0.8156560063362122,\n",
    "            1.649437665939331,\n",
    "            2.6418192386627197,\n",
    "            3.511852264404297,\n",
    "            3.4166924953460693,\n",
    "            2.6418192386627197,\n",
    "            1.114729881286621,\n",
    "            0.842844545841217\n",
    "          ],\n",
    "          \"modifiers\": [\n",
    "            {\n",
    "              \"name\": \"mu\",\n",
    "              \"type\": \"normfactor\",\n",
    "              \"data\": None\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"uncorr_siguncrt\",\n",
    "              \"type\": \"shapesys\",\n",
    "              \"data\": TOTAL_SIGNAL_ERR  \n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"background\",\n",
    "          \"data\": [\n",
    "            227.98190307617188,\n",
    "            185.65267944335938,\n",
    "            141.53671264648438,\n",
    "            83.10063171386719,\n",
    "            39.49835968017578,\n",
    "            20.065095901489258,\n",
    "            5.26054573059082,\n",
    "            0.7651026844978333,\n",
    "            385.53765869140625,\n",
    "            330.3393249511719,\n",
    "            241.39376831054688,\n",
    "            143.0430908203125,\n",
    "            55.337371826171875,\n",
    "            20.656126022338867,\n",
    "            7.634726524353027,\n",
    "            3.049088954925537\n",
    "          ],\n",
    "          \"modifiers\": [\n",
    "            {\n",
    "              \"name\": \"uncorr_bkguncrt\",\n",
    "              \"type\": \"shapesys\",\n",
    "              \"data\": [\n",
    "                34.158817291259766,\n",
    "                27.066844940185547,\n",
    "                22.60236358642578,\n",
    "                14.79345417022705,\n",
    "                6.955612659454346,\n",
    "                4.61644983291626,\n",
    "                1.6257153749465942,\n",
    "                0.48608535528182983,\n",
    "                73.31805419921875,\n",
    "                65.45207214355469,\n",
    "                51.50766372680664,\n",
    "                34.320030212402344,\n",
    "                10.886519432067871,\n",
    "                5.264797210693359,\n",
    "                2.1698012351989746,\n",
    "                1.1060731410980225\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6192c9b-f9ee-4407-be07-9f98aa29e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(json.dumps(full_model.spec, indent=2))\n",
    "init_pars = full_model.config.suggested_init()\n",
    "full_model.expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "bkg_pars = init_pars.copy()\n",
    "bkg_pars[model.config.poi_index] = 0\n",
    "full_model.expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "DATA_OBS = TOTAL_DATA+full_model.config.auxdata\n",
    "\n",
    "full_model.logpdf(pars=bkg_pars, data=DATA_OBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01a2b1-3aa2-4348-9804-04c1f5937e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "    1.0,  # null hypothesis\n",
    "    DATA_OBS,\n",
    "    full_model,\n",
    "    test_stat=\"qtilde\",\n",
    "    return_expected_set=True,\n",
    ")\n",
    "print(f\"      Observed CLs: {CLs_obs:.4f}\")\n",
    "for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "    print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eddbe7e-3de0-4d2a-8729-49308c8ec4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_values = np.linspace(0.1, 10, 50)\n",
    "obs_limit, exp_limits, (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "    DATA_OBS, full_model, poi_values, level=0.1, return_results=True\n",
    ")\n",
    "print(f\"Upper limit (obs): μ = {obs_limit:.4f}\")\n",
    "print(f\"Upper limit (exp): μ = {exp_limits[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb56c7e-87ee-4eb0-8390-0ef5904cddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mass point\n",
    "EXP_LIMIT = np.sqrt(exp_limits[2])*theta_squared\n",
    "LIMIT = np.sqrt(obs_limit)*theta_squared\n",
    "print(\"Expected limit is \" + str(EXP_LIMIT))\n",
    "print(\"Observed limit is \" + str(LIMIT))\n",
    "print()\n",
    "print(\"Owen's expected limit is \" + str(Owen_exp_limit))\n",
    "print(\"Owen's observed limit is \" + str(Owen_obs_limit))\n",
    "\n",
    "print()\n",
    "perc_diff_exp = (1-(EXP_LIMIT/Owen_exp_limit))*100\n",
    "perc_diff_obs = (1-(LIMIT/Owen_obs_limit))*100\n",
    "\n",
    "# perc_diff_exp = (1-(Owen_exp_limit/EXP_LIMIT))*100\n",
    "# perc_diff_obs = (1-(Owen_obs_limit/LIMIT))*100\n",
    "\n",
    "print(\"pyhf expected limit is \" + str(perc_diff_exp) + \"% different from Owen's limit.\")\n",
    "print(\"pyhf observed limit is \" + str(perc_diff_obs) + \"% different from Owen's limit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608e383-7ca2-43da-99f1-4b5660318ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
