{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe0cab0-578b-42ea-86af-07342852e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful!\n"
     ]
    }
   ],
   "source": [
    "#Loading libraries\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import uproot\n",
    "import math\n",
    "import awkward as ak\n",
    "import pickle\n",
    "import csv\n",
    "from importlib import reload\n",
    "import copy\n",
    "import pyhf\n",
    "# from pyhf.contrib.viz import brazil\n",
    "\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print(\"Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e24fb95-377b-4c1e-8cd6-e07ad499975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6939111e-d766-4112-ab60-1fd7c65239ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using fully evaluated systematic uncertainty for background. Dirt will still be 100%.\n",
      "Using fully evaluated systematic uncertainty for signal. Using 30.0% flux error.\n"
     ]
    }
   ],
   "source": [
    "Params_pyhf = {\"Stats_only\":False,\n",
    "               \"Use_flat_sys\":False,\n",
    "               \"Num_bins_for_calc\":8,\n",
    "               \"Use_part_only\":True,\n",
    "               \"Use_toys\":True,\n",
    "               \"Num_toys\":1000,\n",
    "               \"Load_lepton_hists\":True,\n",
    "               \"Load_pi0_hists\":False,\n",
    "               \"Flat_bkg_overlay_frac\":0.5,\n",
    "               \"Flat_bkg_dirt_frac\":1.0,\n",
    "               \"Flat_bkg_EXT_frac\":0.0,\n",
    "               \"Flat_sig_detvar\":0.2, #This is very conservative, could be fed in per mass point from signal detvar script\n",
    "               \"Signal_flux_error\":0.3, #This comes from the KDAR flux uncertainty.\n",
    "               \"Overlay_detvar_frac\":0.5,\n",
    "               \"Load_single_r1_file\": True}\n",
    "\n",
    "scaled = False #Just to keep track of if the histograms have been scaled\n",
    "\n",
    "Functions.pyhf_params(Params_pyhf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "431c728d-46f7-48d0-8cf8-e76c5fa5738f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing histograms in Run1\n",
      "thetas are:\n",
      "{2: 0.1, 10: 0.01, 20: 1e-04, 50: 1e-04, 100: 1e-04, 150: 1e-04}\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# HNL_masses_list = [150,200,245]\n",
    "hist_dict_run1, hist_dict_run3, theta_dict = Functions.Load_pyhf_files(\"FINAL_3.root\",\n",
    "                                                                       Params_pyhf)#, HNL_masses = HNL_masses_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb7693b-0e8b-445c-b507-016b15bc6607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyhf.tensor.numpy_backend.numpy_backend at 0x7f2da0f706c0>,\n",
       " <pyhf.optimize.scipy_optimizer at 0x7f2d9f0ee480>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyhf.get_backend()\n",
    "\n",
    "# nbins = model.config.channel_nbins['singlechannel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1cc99c9-05b7-4e3c-a601-459bc04b2b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pyhf version: 0.7.1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\npyhf version:\",pyhf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2e45e-7b0c-45cc-a21a-65bfd2fafdc3",
   "metadata": {},
   "source": [
    "## Loading in Uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "466c18c5-63d4-4237-96ec-e8ac7a64789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_total_uncertainty(Params, hist_dict, zero_bins_errs): #Takes the dictionary of all root files\n",
    "    TOT_BKG_ERR_dict, TOT_SIGNAL_ERR_dict = {}, {}\n",
    "    BKG_STAT_ERR_dict, SIGNAL_STAT_ERR_dict = {}, {}\n",
    "    BKG_SHAPESYS_ERR_dict, SIGNAL_SHAPESYS_ERR_dict = {}, {}\n",
    "    SIGNAL_NORMSYS_ERR_dict = {} #No normsys for this because currently background contributions are added together\n",
    "    bkg_sample_names = ['bkg_overlay','bkg_EXT','bkg_dirt']\n",
    "    overlay_sys_names = [\"ppfx_uncertainty\",\"Genie_uncertainty\",\"Reinteraction_uncertainty\",\"overlay_DetVar_uncertainty\"]\n",
    "    for HNL_mass in hist_dict:\n",
    "        bkg_stat_err_dict, bkg_sys_err_dict = {}, {} #Clean for each mass point\n",
    "        for name in bkg_sample_names:\n",
    "            bkg_stat_err_dict[name]=hist_dict[HNL_mass][name].errors() #Load in stat error from error saved in hist\n",
    "        sig_stat_err = hist_dict[HNL_mass]['signal'].errors()\n",
    "        if Params[\"Stats_only\"] == True: #Set all systematic errors to zero\n",
    "            for name in bkg_sample_names:\n",
    "                bkg_sys_err_dict[name] = np.zeros_like(hist_dict[HNL_mass][name].errors())\n",
    "            sig_sys_err =  np.zeros_like(hist_dict[HNL_mass]['signal'].errors())\n",
    "        elif Params[\"Use_flat_sys\"] == True:\n",
    "            for name in bkg_sample_names:\n",
    "                bkg_sys_err_dict[name] = hist_dict[HNL_mass][name].values()*Params[\"Flat_\"+name+\"_frac\"]\n",
    "            sig_flux_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Signal_flux_error\"]\n",
    "            sig_detvar_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Flat_sig_detvar\"]\n",
    "            sig_sys_err = np.sqrt(sig_flux_err**2 + sig_detvar_err**2)\n",
    "        elif Params[\"Use_flat_sys\"] == False: #This is using the fully evaluated uncertainties\n",
    "            overlay_sys_dict = {}\n",
    "            for sys in overlay_sys_names:\n",
    "                overlay_sys_dict[sys] = hist_dict[HNL_mass][sys].values()\n",
    "            bkg_sys_err_dict['bkg_overlay'] = Functions.add_all_errors_dict(overlay_sys_dict)\n",
    "            bkg_sys_err_dict['bkg_EXT'] = np.zeros_like(hist_dict[HNL_mass]['bkg_EXT'].errors())\n",
    "            bkg_sys_err_dict['bkg_dirt'] = hist_dict[HNL_mass]['bkg_dirt'].values()*Params[\"Flat_bkg_dirt_frac\"]\n",
    "            \n",
    "            sig_detvar_err = hist_dict[HNL_mass][\"signal_DetVar_uncertainty\"].values()\n",
    "            sig_flux_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Signal_flux_error\"]\n",
    "            sig_sys_err = Functions.add_all_errors([sig_detvar_err,sig_flux_err])\n",
    "            \n",
    "        #Evaluating final stat+sys errors    \n",
    "        bkg_stat_plus_sys_dict={}\n",
    "        for name in bkg_sample_names:\n",
    "            bkg_stat_plus_sys_dict[name]=Functions.add_all_errors([bkg_stat_err_dict[name],bkg_sys_err_dict[name],zero_bins_errs[HNL_mass][name]]) \n",
    "        \n",
    "        total_bkg_err = Functions.add_all_errors_dict(bkg_stat_plus_sys_dict) #Now adding the errors of overlay, EXT and dirt in quadrature\n",
    "        total_sig_err = Functions.add_all_errors([sig_stat_err,sig_sys_err])\n",
    "        \n",
    "        TOT_BKG_ERR_dict[HNL_mass] = total_bkg_err\n",
    "        TOT_SIGNAL_ERR_dict[HNL_mass] = total_sig_err\n",
    "        \n",
    "        BKG_STAT_ERR_dict[HNL_mass] = Functions.add_all_errors_dict(bkg_stat_err_dict)\n",
    "        BKG_SHAPESYS_ERR_dict[HNL_mass] = Functions.add_all_errors_dict(bkg_sys_err_dict)\n",
    "        SIGNAL_STAT_ERR_dict[HNL_mass] = sig_stat_err\n",
    "        SIGNAL_SHAPESYS_ERR_dict[HNL_mass] = sig_detvar_err\n",
    "    TOT_ERR_DICT = {}\n",
    "    TOT_ERR_DICT[\"TOT_BKG_ERR\"], TOT_ERR_DICT[\"TOT_SIGNAL_ERR\"] = TOT_BKG_ERR_dict, TOT_SIGNAL_ERR_dict\n",
    "    TOT_ERR_DICT[\"BKG_STAT\"], TOT_ERR_DICT[\"BKG_SHAPESYS\"] = BKG_STAT_ERR_dict, BKG_SHAPESYS_ERR_dict\n",
    "    TOT_ERR_DICT[\"SIGNAL_STAT\"], TOT_ERR_DICT[\"SIGNAL_SHAPESYS\"] = SIGNAL_STAT_ERR_dict, SIGNAL_SHAPESYS_ERR_dict\n",
    "    # return TOT_BKG_ERR_dict, TOT_SIGNAL_ERR_dict\n",
    "    return TOT_ERR_DICT\n",
    "    \n",
    "\n",
    "#Want TOTAL_BKG_ERR_DICT, STAT_BKG_ERR_DICT, SHAPESYS_BKG_ERR_DICT, where the shapesys also contains the dirt and overlay flat norm errors.\n",
    "# TOTAL_SIGNAL_ERR_DICT, STAT_SIGNAL_ERR_DICT, SHAPESYS_SIGNAL_ERR_DICT, NORM_SIGNAL_ERR_DICT\n",
    "    \n",
    "def Add_bkg_hists_make_signal(hist_dict):\n",
    "    BKG_dict, SIGNAL_dict = {}, {}\n",
    "    for HNL_mass in hist_dict:\n",
    "        bkg_hists = [hist_dict[HNL_mass]['bkg_EXT'], hist_dict[HNL_mass]['bkg_overlay'], hist_dict[HNL_mass]['bkg_dirt']]\n",
    "        \n",
    "        total_bkg = Functions.add_hists_vals(bkg_hists)\n",
    "        BKG_dict[HNL_mass] = total_bkg\n",
    "        SIGNAL_dict[HNL_mass] = hist_dict[HNL_mass]['signal'].values()\n",
    " \n",
    "    return BKG_dict, SIGNAL_dict\n",
    "\n",
    "def add_data(Total_dict, hists, numbins):\n",
    "    for HNL_mass in Total_dict:\n",
    "        hist_placeholder = list(hists[HNL_mass][\"data\"].values())\n",
    "        hist_data = Functions.remove_part_hist(hist_placeholder, numbins)\n",
    "        Total_dict[HNL_mass][\"data\"]=hist_data\n",
    "    return Total_dict\n",
    "\n",
    "def add_data_appended(Total_dict, hists_r1, hists_r3, numbins):\n",
    "    for HNL_mass in Total_dict:\n",
    "        r1_hist_placeholder = list(hists_r1[HNL_mass][\"data\"].values())\n",
    "        r3_hist_placeholder = list(hists_r3[HNL_mass][\"data\"].values())\n",
    "        r1_hist = Functions.remove_part_hist(r1_hist_placeholder, numbins)\n",
    "        r3_hist = Functions.remove_part_hist(r3_hist_placeholder, numbins)\n",
    "        appended = r1_hist+r3_hist\n",
    "        Total_dict[HNL_mass][\"data\"]=appended\n",
    "    return Total_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e370570b-20ae-4a1f-aeda-8a8b2e95ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([2, 10, 20, 50, 100, 150])\n",
      "dict_keys([2, 10, 20, 50, 100, 150])\n",
      "dict_keys([2, 10, 20, 50, 100, 150])\n"
     ]
    }
   ],
   "source": [
    "reload(Functions)\n",
    "zero_bins_errors_run1 = Functions.make_zero_bin_unc(hist_dict_run1, Constants.run1_POT_scaling_dict)\n",
    "zero_bins_errors_run3 = Functions.make_zero_bin_unc(hist_dict_run3, Constants.run3_POT_scaling_dict)\n",
    "\n",
    "TOT_R1_ERR = Calculate_total_uncertainty(Params_pyhf, hist_dict_run1, zero_bins_errors_run1)\n",
    "TOT_R3_ERR = Calculate_total_uncertainty(Params_pyhf, hist_dict_run3, zero_bins_errors_run3)\n",
    "\n",
    "R1_BKG, R1_SIGNAL = Add_bkg_hists_make_signal(hist_dict_run1)\n",
    "R3_BKG, R3_SIGNAL = Add_bkg_hists_make_signal(hist_dict_run3)\n",
    "\n",
    "R1_output = Functions.Make_into_lists(Params_pyhf, R1_BKG, R1_SIGNAL, TOT_R1_ERR)\n",
    "R3_output = Functions.Make_into_lists(Params_pyhf, R3_BKG, R3_SIGNAL, TOT_R3_ERR)\n",
    "\n",
    "list_input_dicts = [R1_output, R3_output]\n",
    "\n",
    "Total_dict_both = Functions.Create_final_appended_runs_dict(list_input_dicts)\n",
    "Total_dict_run1 = Functions.Create_final_appended_runs_dict([R1_output])\n",
    "Total_dict_run3 = Functions.Create_final_appended_runs_dict([R3_output])\n",
    "\n",
    "if Params_pyhf[\"Use_part_only\"]==True:\n",
    "    NUMBINS = Params_pyhf[\"Num_bins_for_calc\"]\n",
    "else: NUMBINS=30 #This will just give the full hist\n",
    "    \n",
    "if 'data;1' in hist_dict_run1[150]:\n",
    "    Total_dict_run1=add_data(Total_dict_run1, hist_dict_run1, NUMBINS)\n",
    "    Total_dict_run3=add_data(Total_dict_run3, hist_dict_run3, NUMBINS)\n",
    "    Total_dict_both=add_data_appended(Total_dict_both, hist_dict_run1, hist_dict_run3, NUMBINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3c0802a-1a75-49f3-9c99-56ed034130c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stat_unc(Total_dict):\n",
    "    sig_stat, bkg_stat = {}, {}\n",
    "    for HNL_mass in Total_dict:\n",
    "        sig_stat[HNL_mass] = list(np.divide(np.array(Total_dict[HNL_mass]['SIGNAL_STAT']),np.array(Total_dict[HNL_mass]['SIGNAL_dict'])))\n",
    "        bkg_stat[HNL_mass] = list(np.divide(np.array(Total_dict[HNL_mass]['BKG_STAT']),np.array(Total_dict[HNL_mass]['BKG_dict'])))\n",
    "    return sig_stat, bkg_stat\n",
    "\n",
    "sig_stat, bkg_stat = create_stat_unc(Total_dict_both)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d595061-c00a-43c5-845f-16e9d176918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dict_same(Total_dict, debug=False): #Creating a model where the uncertainties are all enveloped in one shapesys modifier\n",
    "    model_dict = {}\n",
    "    \n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass][\"SIGNAL_dict\"],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                {\"name\": \"uncorr_siguncrt\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass][\"TOT_SIGNAL_ERR\"]}\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"background\",\n",
    "              \"data\": Total_dict[HNL_mass][\"BKG_dict\"],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass][\"TOT_BKG_ERR\"]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict\n",
    "\n",
    "def create_model_dict_split(Total_dict, debug=False): #Creating a model where the different types of uncertainties are split into different modifiers\n",
    "    model_dict = {}\n",
    "    sig_norm = {\"hi\": 1.0+Params_pyhf[\"Signal_flux_error\"], \"lo\": 1.0-Params_pyhf[\"Signal_flux_error\"]}\n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        \n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass][\"SIGNAL_dict\"],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                {\"name\": \"uncorr_siguncrt\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['SIGNAL_SHAPESYS']},\n",
    "                {\"name\": \"stat_siguncrt\", \"type\": \"staterror\", \"data\": sig_stat[HNL_mass]},  \n",
    "                {\"name\": \"norm_siguncrt\", \"type\": \"normsys\", \"data\": sig_norm}  \n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"background\",\n",
    "              \"data\": Total_dict[HNL_mass][\"BKG_dict\"],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_SHAPESYS']},\n",
    "                {\"name\": \"stat_bkguncrt\", \"type\": \"staterror\", \"data\": bkg_stat[HNL_mass]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd6b13-1435-4cdb-a37a-6ccddd5722be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scaling signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c1437b-4f00-499a-afb3-c0e2cc5bd296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_signal(Total_dict, theta_dict, scaling_dict={}):\n",
    "    \"\"\"\n",
    "    Scales the number of events by the number in the scaling dict.\n",
    "    Returns the new dict of histograms and the new thetas.\n",
    "    \"\"\"\n",
    "    if(scaling_dict=={}): raise Exception(\"Specify scalings\")\n",
    "    Total_dict_scaled, new_theta_dict = copy.deepcopy(Total_dict), {}\n",
    "    for HNL_mass in Total_dict.keys():\n",
    "        new_signal_hist = np.array(Total_dict[HNL_mass]['SIGNAL_dict'])*scaling_dict[HNL_mass]\n",
    "        new_signal_err_hist = np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR'])*scaling_dict[HNL_mass]\n",
    "        new_theta = theta_dict[HNL_mass]*scaling_dict[HNL_mass]**(1/4) # Number of events is proportional to theta**4\n",
    "        \n",
    "        Total_dict_scaled[HNL_mass]['SIGNAL_dict'] = list(new_signal_hist)\n",
    "        Total_dict_scaled[HNL_mass]['TOT_SIGNAL_ERR'] = list(new_signal_err_hist)\n",
    "        new_theta_dict[HNL_mass] = new_theta\n",
    "        \n",
    "    return Total_dict_scaled, new_theta_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fad4601-b177-49a6-93d0-a7e4387d6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_SF = 2000\n",
    "if Params_pyhf[\"Load_lepton_hists\"]==True:\n",
    "    scaling_dict = {2:5000,10:1000,20:2e9,50:5e6,100:1e5,150:5e3} #Scaling for both r1 and r3 combined\n",
    "    # scaling_dict = {2:5000,10:10000,20:2e9,50:5e6,100:1e6,150:5e4} #Scaling for just r1\n",
    "elif Params_pyhf[\"Load_pi0_hists\"]==True:\n",
    "    scaling_dict = {150:100,180:10,200:5,220:2,240:2,245:2}\n",
    "    # scaling_dict = {150:500,180:50,200:25,220:102,240:10,245:10}\n",
    "elif Params_pyhf[\"Load_single_r1_file\"]==True: #Currently using this for pi0 Dirac samples\n",
    "    scaling_dict = {150:1000,180:50,200:50,220:102,240:20,245:20}\n",
    "scaled=True\n",
    "\n",
    "Total_dict, theta_dict_scaled  = scale_signal(Total_dict_both, theta_dict, scaling_dict)\n",
    "Total_dict_run1_scaled, theta_dict_scaled  = scale_signal(Total_dict_run1, theta_dict, scaling_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "166e1e7a-7f27-4d2a-b1e1-efde4c7f5745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BKG_dict', 'SIGNAL_dict', 'TOT_BKG_ERR', 'TOT_SIGNAL_ERR', 'BKG_STAT', 'BKG_SHAPESYS', 'SIGNAL_STAT', 'SIGNAL_SHAPESYS', 'data'])\n"
     ]
    }
   ],
   "source": [
    "print(Total_dict[150].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae12345-a8e7-471b-926a-bea2bc7a2c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plotting example hist after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9e985-2c90-457e-bde4-7b60889d0a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 150\n",
    "theta = theta_dict_scaled[HNL_mass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d6513-ed99-432d-90e7-79f05c62fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16\n",
    "plt.figure(figsize=(7,4),facecolor='white',dpi=100)\n",
    "\n",
    "num_bins = Params_pyhf[\"Num_bins_for_calc\"]*2\n",
    "\n",
    "bins = np.arange(0,num_bins+1,1)\n",
    "bins_cents=(bins[:-1]+bins[1:])/2\n",
    "plt.hist(bins_cents, weights=Total_dict[HNL_mass]['SIGNAL_dict'], bins=bins, histtype=\"step\", lw=2, label=f\"Signal \\n\" + fr\"{HNL_mass}MeV $\\theta$={theta:.5f}\")\n",
    "plt.hist(bins_cents, weights=Total_dict[HNL_mass]['BKG_dict'],bins=bins, histtype=\"step\",lw=2, label=\"Background\")\n",
    "\n",
    "bkg_up=np.append((Total_dict[HNL_mass]['BKG_dict']+np.array(Total_dict[HNL_mass]['TOT_BKG_ERR'])),(Total_dict[HNL_mass]['BKG_dict']+np.array(Total_dict[HNL_mass]['TOT_BKG_ERR']))[-1])\n",
    "bkg_down=np.append((Total_dict[HNL_mass]['BKG_dict']-np.array(Total_dict[HNL_mass]['TOT_BKG_ERR'])),(Total_dict[HNL_mass]['BKG_dict']-np.array(Total_dict[HNL_mass]['TOT_BKG_ERR']))[-1])\n",
    "sig_up=np.append((Total_dict[HNL_mass]['SIGNAL_dict']+np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR'])),(Total_dict[HNL_mass]['SIGNAL_dict']+np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR']))[-1])\n",
    "sig_down=np.append((Total_dict[HNL_mass]['SIGNAL_dict']-np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR'])),(Total_dict[HNL_mass]['SIGNAL_dict']-np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR']))[-1])\n",
    "\n",
    "plt.fill_between(bins, bkg_down, bkg_up,step=\"post\",hatch='///',alpha=0,zorder=2)\n",
    "plt.fill_between(bins, sig_down, sig_up,step=\"post\",hatch='///',alpha=0,zorder=2)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"BDT score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ed0542-37d5-4eaf-a140-f97f5caa4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16\n",
    "plt.figure(figsize=(7,4),facecolor='white',dpi=100)\n",
    "\n",
    "num_bins = Params_pyhf[\"Num_bins_for_calc\"]\n",
    "\n",
    "bins = np.arange(0,num_bins+1,1)\n",
    "bins_cents=(bins[:-1]+bins[1:])/2\n",
    "plt.hist(bins_cents, weights=Total_dict_run1_scaled[HNL_mass]['SIGNAL_dict'], bins=bins, histtype=\"step\", lw=2, label=f\"Signal \\n\" + fr\"{HNL_mass}MeV $\\theta$={theta:.5f}\")\n",
    "plt.hist(bins_cents, weights=Total_dict_run1_scaled[HNL_mass]['BKG_dict'],bins=bins, histtype=\"step\",lw=2, label=\"Background\")\n",
    "\n",
    "bkg_up=np.append((Total_dict_run1_scaled[HNL_mass]['BKG_dict']+np.array(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_ERR'])),(Total_dict_run1_scaled[HNL_mass]['BKG_dict']+np.array(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_ERR']))[-1])\n",
    "bkg_down=np.append((Total_dict_run1_scaled[HNL_mass]['BKG_dict']-np.array(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_ERR'])),(Total_dict_run1_scaled[HNL_mass]['BKG_dict']-np.array(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_ERR']))[-1])\n",
    "sig_up=np.append((Total_dict_run1_scaled[HNL_mass]['SIGNAL_dict']+np.array(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_ERR'])),(Total_dict_run1_scaled[HNL_mass]['SIGNAL_dict']+np.array(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_ERR']))[-1])\n",
    "sig_down=np.append((Total_dict_run1_scaled[HNL_mass]['SIGNAL_dict']-np.array(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_ERR'])),(Total_dict_run1_scaled[HNL_mass]['SIGNAL_dict']-np.array(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_ERR']))[-1])\n",
    "\n",
    "plt.fill_between(bins, bkg_down, bkg_up,step=\"post\",hatch='///',alpha=0,zorder=2)\n",
    "plt.fill_between(bins, sig_down, sig_up,step=\"post\",hatch='///',alpha=0,zorder=2)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"BDT score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d44019-e751-4595-b5c9-6166719fa92f",
   "metadata": {},
   "source": [
    "## Creating model (only do once happy with scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447c26da-ff4f-430b-9b9f-cec8641caabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scaled==False:\n",
    "    print(\"Using unscaled hists\")\n",
    "    Total_dict, theta_dict_scaled = Total_dict_both, theta_dict\n",
    "\n",
    "model_dict_both = create_model_dict_same(Total_dict)\n",
    "model_dict_split = create_model_dict_split(Total_dict_both)\n",
    "model_dict_run1 = create_model_dict_same(Total_dict_run1_scaled)\n",
    "# model_dict_run3 = create_model_dict(Total_dict_run3)\n",
    "print(\"Created models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de95dd-88b6-484c-988f-014528bb70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta_dict_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b092e9e5-a61e-448a-97f5-2a5bee784aeb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Making uncertainty breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c7c8d-4915-4c9c-abf9-bcdc206025cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 10\n",
    "hist_dict_run3[HNL_mass].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba893d6-5821-489d-ba0a-30c6ab879cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Total_dict[2].keys())\n",
    "# print(Total_dict[2]['BKG_dict'])\n",
    "# print(Total_dict[2]['BKG_ERR_dict'])\n",
    "\n",
    "HNL_mass = 100\n",
    "test_dict = {}\n",
    "test_dict[HNL_mass] = hist_dict_run3[HNL_mass]\n",
    "\n",
    "print(\"Signal:\")\n",
    "print(test_dict[HNL_mass]['signal'].values())\n",
    "print(\"Background:\")\n",
    "print(test_dict[HNL_mass]['bkg_overlay'].values() + test_dict[HNL_mass]['bkg_EXT'].values() + test_dict[HNL_mass]['bkg_dirt'].values())\n",
    "print()\n",
    "print(\"Bkg overlay:\")\n",
    "print(test_dict[HNL_mass]['bkg_overlay'].values())\n",
    "print(\"Bkg EXT:\")\n",
    "print(test_dict[HNL_mass]['bkg_EXT'].values())\n",
    "print(\"Bkg dirt:\")\n",
    "print(test_dict[HNL_mass]['bkg_dirt'].values())\n",
    "print()\n",
    "\n",
    "TEST_BKG_ERR_dict, TEST_SIGNAL_ERR_dict = Functions.Uncertainty_breakdown(Params_pyhf, test_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68fc82f-8497-4465-88d7-ea486bdb2f3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Making test statistic plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9ff10-074e-47fa-b9b8-e1b706a8c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Use_full_modifiers = True\n",
    "\n",
    "if Use_full_modifiers == True: model_dict = model_dict_split\n",
    "if Use_full_modifiers == False: model_dict = model_dict_both\n",
    "\n",
    "HNL_mass = 10 #Mass point to test\n",
    "DATA_OBS_dict = {}\n",
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "bkg_pars = init_pars.copy()\n",
    "bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "print(list(zip(model_dict[HNL_mass].config.parameters, init_pars)))\n",
    "print(list(zip(model_dict[HNL_mass].config.parameters, bkg_pars)))\n",
    "\n",
    "pdf_bkg = model_dict[HNL_mass].make_pdf(pyhf.tensorlib.astensor(bkg_pars)) #Making the pdfs\n",
    "pdf_sig = model_dict[HNL_mass].make_pdf(pyhf.tensorlib.astensor(init_pars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a12a35-c8b3-422d-b8a6-efd8ce89fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "\n",
    "# mu' = 0\n",
    "mc_bkg = pdf_bkg.sample((n_samples,))\n",
    "# mu' = 1\n",
    "mc_sig = pdf_sig.sample((n_samples,))\n",
    "\n",
    "print(mc_bkg.shape)\n",
    "print(mc_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e5770-2881-4cda-856d-0159004cf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_calculator_qtilde = pyhf.infer.utils.create_calculator( #only seems to support q-like test statistics\n",
    "    \"toybased\",\n",
    "    model_dict[HNL_mass].expected_data(init_pars),\n",
    "    model_dict[HNL_mass],\n",
    "    ntoys=n_samples,\n",
    "    test_stat=\"qtilde\",\n",
    ")\n",
    "qtilde_sig, qtilde_bkg = toy_calculator_qtilde.distributions(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad355c4-c5e1-4840-8f8c-61ca0b68ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max qtilde signal value: \" + str(max(qtilde_sig.samples)))\n",
    "print(\"Max qtilde background value: \" + str(max(qtilde_bkg.samples)))\n",
    "\n",
    "#Finding probability for one value of test statistic\n",
    "ordered_test_stat = np.sort(qtilde_sig.samples)\n",
    "length =len(ordered_test_stat)\n",
    "\n",
    "prob = 0.9\n",
    "slice_at = 0.9*length\n",
    "print(np.floor(slice_at))\n",
    "print(ordered_test_stat[int(slice_at)])\n",
    "value_for_prob = ordered_test_stat[int(slice_at)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195623a-0f61-454a-a7ef-88bd9e3a0574",
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig=True\n",
    "\n",
    "x_max = max(qtilde_sig.samples)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "nbins = 30 #Should make an array for the bin edges\n",
    "bins = np.linspace(0,x_max*1.2,nbins) #was 0, 45 before\n",
    "plt.hist(\n",
    "    qtilde_sig.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_1|1)$ signal-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.hist(\n",
    "    qtilde_bkg.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_1|0)$ background-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "# value_for_prob = 2.16\n",
    "if Use_full_modifiers == True: model_name = \"full_unc\"\n",
    "if Use_full_modifiers == False: model_name = \"quad_sum_unc\"\n",
    "\n",
    "plt.axvline(x = value_for_prob, color = 'red', label = '90% exclusion',linestyle=\"dashed\", lw=3)\n",
    "\n",
    "plt.xlabel(r\"$\\tilde{q}_1$\", fontsize=22)\n",
    "plt.ylabel(r\"$f\\,(\\tilde{q}_1|\\mu')$\", fontsize=22)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=22)\n",
    "\n",
    "if savefig==True:\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_qtilde_{model_name}_{HNL_mass}MeV.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_qtilde_{model_name}_{HNL_mass}MeV.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97268c1e-74e7-4c6d-a8c5-ffa08eb621c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmu_bounds = model_dict[HNL_mass].config.suggested_bounds()\n",
    "print(f\"Old bounds: {qmu_bounds}\")\n",
    "# qmu_bounds[model_dict[HNL_mass].config.poi_index] = (-10, 10) #Made these larger and didn't get minimization error in followin cell\n",
    "qmu_bounds[model_dict[HNL_mass].config.poi_index] = (-50, 50)\n",
    "print(f\"New bounds: {qmu_bounds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1247f-80ae-490f-9799-98bf2f5fda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_calculator_qmu = pyhf.infer.utils.create_calculator(\n",
    "    \"toybased\",\n",
    "    model_dict[HNL_mass].expected_data(model_dict[HNL_mass].config.suggested_init()),\n",
    "    model_dict[HNL_mass],\n",
    "    par_bounds=qmu_bounds,\n",
    "    ntoys=n_samples,\n",
    "    test_stat=\"q\",\n",
    ")\n",
    "qmu_sig, qmu_bkg = toy_calculator_qmu.distributions(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cd24f-3977-4835-8406-7b20bad19c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(qmu_sig.samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bd896-544c-42f1-9d23-80c783ef6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "nbins = 10 #Should make an array for the bin edges\n",
    "bins = np.linspace(0,0.2,nbins)\n",
    "plt.hist(\n",
    "    qmu_sig.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=\"$f(q_1|1)$ signal-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.hist(\n",
    "    qmu_bkg.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=\"$f(q_1|0)$ background-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.xlabel(r\"$q_1$\", fontsize=18)\n",
    "plt.ylabel(r\"$f\\,(q_1|\\mu')$\", fontsize=18)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917fdd5-6480-4802-87f6-35fb140421aa",
   "metadata": {},
   "source": [
    "## Changing poi values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cae526-6444-454f-9c77-08280027d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CL_range(observations, model,unbounded_bounds,cl):\n",
    "\n",
    "    alpha = round(1-0.01*cl,2)\n",
    "    low_end  = 1000\n",
    "    high_end = 2000\n",
    "    obs_limit = 2000\n",
    "    iterations = 0\n",
    "    delta_iter = 1000\n",
    "    while obs_limit >= high_end:\n",
    "        iterations += 1\n",
    "        if iterations > 1:\n",
    "            delta_iter = iterations*10000\n",
    "            high_end   += delta_iter\n",
    "        poi_values = np.linspace(low_end,high_end,3)\n",
    "        obs_limit, exp_limits = pyhf.infer.intervals.upper_limits.upper_limit(\n",
    "            observations,model,poi_values,level=alpha,par_bounds=unbounded_bounds\n",
    "        )\n",
    "        print(iterations,\"iterations\")\n",
    "    #return np.linspace(low_end,2*high_end,10)\n",
    "    #lo_range = max(1000,obs_limit-obs_limit/2.)\n",
    "    lo_range = 1000.\n",
    "    if obs_limit > 10000:\n",
    "        lo_range = obs_limit-obs_limit/2.\n",
    "    up_range = obs_limit+obs_limit/2.\n",
    "    return np.linspace(lo_range,up_range,50)\n",
    "\n",
    "\n",
    "nbins = model.config.channel_nbins['singlechannel']\n",
    "print(\"\\nNumber of bins:\",nbins)\n",
    "print(type(nbins))\n",
    "\n",
    "# suggested initial parameters\n",
    "init_pars = model.config.suggested_init()\n",
    "print(\"\\nInitial parameters:\",init_pars)\n",
    "print(\"\\nSuggested bounds:\",model.config.suggested_bounds())\n",
    "\n",
    "unbounded_bounds = model.config.suggested_bounds()\n",
    "unbounded_bounds[model.config.poi_index] = (0, 160000000) #Something very large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1ef3c-b55b-4830-8f63-436da533686c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Testing single point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a54aa2-dd60-4c73-96da-abdd7b907fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Total_dict_to_use[HNL_mass].keys())\n",
    "# print(Total_dict_to_use[HNL_mass][\"BKG_dict\"])\n",
    "# print(Total_dict_to_use[HNL_mass][\"data\"])\n",
    "# print()\n",
    "\n",
    "# print(Total_dict_to_use[HNL_mass][\"BKG_dict\"]+model_dict_to_use[HNL_mass].config.auxdata)\n",
    "# print(Total_dict_to_use[HNL_mass][\"data\"]+model_dict_to_use[HNL_mass].config.auxdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c533c-cc65-4f3c-8be0-dee7b236807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 10 #The mass point to test\n",
    "n_sigmas = 2\n",
    "if n_sigmas==1: sigma_list = np.arange(-1, 2)\n",
    "if n_sigmas==2: sigma_list = np.arange(-2, 3)\n",
    "\n",
    "# model_dict_to_use = model_dict_both\n",
    "model_dict_to_use = model_dict_run1\n",
    "Total_dict_to_use = Total_dict_run1_scaled #Total_dict_both, Total_dict_run1, Total_dict_run3\n",
    "\n",
    "DATA_OBS_dict = {}\n",
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "init_pars = model_dict_to_use[HNL_mass].config.suggested_init()\n",
    "model_dict_to_use[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "bkg_pars = init_pars.copy()\n",
    "bkg_pars[model_dict_to_use[HNL_mass].config.poi_index] = 0\n",
    "model_dict_to_use[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "if 'data' not in Total_dict_to_use[HNL_mass].keys(): \n",
    "    print(\"No data in this sample, using bkg as data\")\n",
    "    DATA_OBS_dict[HNL_mass] = Total_dict_to_use[HNL_mass][\"BKG_dict\"]+model_dict_to_use[HNL_mass].config.auxdata\n",
    "else: \n",
    "    print(\"Using real data for observed limit\")\n",
    "    DATA_OBS_dict[HNL_mass] = Total_dict_to_use[HNL_mass][\"data\"]+model_dict_to_use[HNL_mass].config.auxdata    \n",
    "\n",
    "model_dict_to_use[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass]) #Is this only for the test stat distribution stuff?\n",
    "\n",
    "if Params_pyhf[\"Use_toys\"] == False:\n",
    "    CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "                1.0,  # null hypothesis\n",
    "                DATA_OBS_dict[HNL_mass],\n",
    "                model_dict_to_use[HNL_mass],\n",
    "                test_stat=\"qtilde\",\n",
    "                return_expected_set=True,\n",
    "                calctype=\"asymptotics\",\n",
    "                )\n",
    "if Params_pyhf[\"Use_toys\"] == True:\n",
    "    CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict_to_use[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"toybased\",\n",
    "            ntoys=Params_pyhf[\"Num_toys\"],\n",
    "            # track_progress=True, \n",
    "            # track_progress=False, #Used to have as true, but this gave an error when the signal was scaled up\n",
    "            )\n",
    "\n",
    "for expected_value, n_sigma in zip(CLs_exp, sigma_list): #-2, 3\n",
    "    print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")\n",
    "    \n",
    "poi_values = np.linspace(0.001, 10, 500) #I could make a dict of this and have different pois for different mass points\n",
    "print(\"Max value is \" + str(max(poi_values)))\n",
    "print(\"Min value is \" + str(min(poi_values)))\n",
    "print(\"Next value is \" + str(poi_values[1]))\n",
    "print(\"Next value is \" + str(poi_values[2]) + \"\\n\")\n",
    "\n",
    "obs_limit_single, exp_limits_single, (scan, results) = pyhf.infer.intervals.upperlimit(DATA_OBS_dict[HNL_mass], \n",
    "                                                                                       model_dict_to_use[HNL_mass], poi_values, \n",
    "                                                                                       level=0.1, return_results=True)\n",
    "print(f\"Upper limit {HNL_mass}MeV (obs): μ = {obs_limit_single:.4f}\")\n",
    "print(f\"Upper limit {HNL_mass}MeV (exp): μ = {exp_limits_single[2]:.4f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30a6bc-7a6c-47b4-89df-7688667c8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = []\n",
    "obs_limit = []\n",
    "exp_1sig_up = [] #entry 3\n",
    "exp_1sig_down = [] #entry 1\n",
    "exp_2sig_up = [] #entry 4\n",
    "exp_2sig_down = [] #entry 0\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in [HNL_mass]:\n",
    "    theta_squared = (theta_dict_scaled[HNL_mass])**2\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_single[2])*theta_squared\n",
    "    LIMIT = np.sqrt(obs_limit_single)*theta_squared\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_single[3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_single[4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_single[1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_single[0])*theta_squared)\n",
    "    obs_limit.append(LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39599ac7-a757-4b36-a2e6-ca778c1e377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for HNL_mass in [HNL_mass]:\n",
    "    theta_squared = (theta_dict_scaled[HNL_mass])**2\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_single[2])*theta_squared\n",
    "    LIMIT = np.sqrt(obs_limit_single)*theta_squared\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_single[3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_single[4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_single[1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_single[0])*theta_squared)\n",
    "    obs_limit.append(LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910cc934-a481-4719-a82d-087aa712e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs_limit)\n",
    "print(\"Expected limit\")\n",
    "print(exp_limit)\n",
    "print(\"1 sigma up\")\n",
    "print(exp_1sig_up)\n",
    "print(\"2 sigma up\")\n",
    "print(exp_2sig_up)\n",
    "print(\"1 sigma down\")\n",
    "print(exp_1sig_down)\n",
    "print(\"2 sigma down\")\n",
    "print(exp_2sig_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a459eea-84c1-4045-90aa-e1af4d11c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_limits_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d29e6-f7f4-4088-b9c5-0ccc2fd1f419",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running through all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ace42d-de08-48dd-8c1a-ed51a6f79b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dict = model_dict_split #Uncertainties are split into different modifiers, takes several times longer to run.\n",
    "# model_dict = model_dict_both #Quicker, uncertainties are entered as one uncertainty which is the quadsum of components.\n",
    "# Total_dict = Total_dict_both\n",
    "\n",
    "model_dict = model_dict_run1\n",
    "Total_dict = Total_dict_run1_scaled\n",
    "\n",
    "if Params_pyhf[\"Load_pi0_hists\"]==True:HNL_masses=Constants.HNL_mass_pi0_samples\n",
    "elif Params_pyhf[\"Load_lepton_hists\"]==True:HNL_masses=Constants.HNL_mass_samples\n",
    "elif Params_pyhf[\"Load_single_r1_file\"]==True: HNL_masses = HNL_masses_list\n",
    "    \n",
    "# HNL_masses = HNL_masses_list\n",
    "\n",
    "# list_test = Constants.HNL_mass_samples\n",
    "print(\"Running for these masses: \" + str(HNL_masses))\n",
    "\n",
    "DATA_OBS_dict = {}\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "    model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "    bkg_pars = init_pars.copy()\n",
    "    bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "    model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "    list_keys = list(Total_dict[HNL_mass].keys())\n",
    "    if \"data\" in list_keys: #haven't made this yet, need to test\n",
    "        DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass][\"data\"]+model_dict[HNL_mass].config.auxdata\n",
    "    else:\n",
    "        DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass][\"BKG_dict\"]+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "    model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4234fa8-33c9-48dc-9eaa-674262afcba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes ~10 mins with 100 toys and 4 bins, for all errors in one modifier\n",
    "for HNL_mass in HNL_masses:\n",
    "\n",
    "    if Params_pyhf[\"Use_toys\"] == False:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"asymptotics\",\n",
    "            )\n",
    "    if Params_pyhf[\"Use_toys\"] == True:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"toybased\",\n",
    "            ntoys=Params_pyhf[\"Num_toys\"],\n",
    "            # track_progress=True, \n",
    "            # track_progress=False, #Used to have as true, but this gave an error when the signal was scaled up\n",
    "            )\n",
    "    \n",
    "    print(f\"{HNL_mass}MeV\")\n",
    "    for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "        print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727ad3d-56ab-42bf-ae92-e24e5b50f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Should take ~10 mins with 100 toys and 4 bins\n",
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "print(\"If the output of the following is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "poi_values = np.linspace(0.001, 5, 100) #I could make a dict of this and have different pois for different mass points\n",
    "print(\"Max value is \" + str(max(poi_values)))\n",
    "print(\"Min value is \" + str(min(poi_values)))\n",
    "print(\"Next value is \" + str(poi_values[1]))\n",
    "print(\"Next value is \" + str(poi_values[2]) + \"\\n\")\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "# list_test = [180, 200, 220, 240, 245]\n",
    "for HNL_mass in HNL_masses:\n",
    "\n",
    "    # poi_values = np.linspace(0.001, 4, 100)\n",
    "    obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "        DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_values, level=0.1, return_results=True\n",
    "    )\n",
    "\n",
    "    print(f\"Upper limit {HNL_mass}MeV (obs): μ = {obs_limit_dict[HNL_mass]:.6f}\")\n",
    "    print(f\"Upper limit {HNL_mass}MeV (exp): μ = {exp_limits_dict[HNL_mass][2]:.6f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ea839-501a-4cfd-9b20-0ba1fa544900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((scan, results))\n",
    "# print(scan)\n",
    "# print(results[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0dcf7f-0665-430c-8239-66452b8bf24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = [] #entry 2\n",
    "exp_1sig_up = [] #entry 3\n",
    "exp_1sig_down = [] #entry 1\n",
    "exp_2sig_up = [] #entry 4\n",
    "exp_2sig_down = [] #entry 0\n",
    "obs_limit = []\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in exp_limits_dict:\n",
    "    theta_squared = (theta_dict_scaled[HNL_mass])**2\n",
    "    print(theta_squared)\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_dict[HNL_mass][2])*theta_squared\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][0])*theta_squared)\n",
    "    \n",
    "    LIMIT = np.sqrt(obs_limit_dict[HNL_mass])*theta_squared\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)\n",
    "    obs_limit.append(LIMIT)\n",
    "    \n",
    "print(exp_limit)\n",
    "print(obs_limit)\n",
    "removed_2MeV = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc576b2-2f17-425c-a861-5b65c13e9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "POT_norms = Constants.run1_POT_scaling_dict\n",
    "\n",
    "print(POT_norms['245_pi0_dirac'])\n",
    "print(POT_norms['245_pi0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ae591-6907-40f0-88fe-08ad10831664",
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = HNL_masses\n",
    "# limts_Maj = [0.005413019157172526,5.447736619270424e-06,1.585234277739009e-06] #ee\n",
    "limts_Maj = [1.8571539668620982e-07,4.214921434135996e-08,2.217237268632437e-08] #pi0\n",
    "limits_Dir = [5.378917430264621e-07, 1.2004619124764445e-07, 6.640407306572216e-08]\n",
    "plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "plt.plot(masses, limts_Maj, label=\"Maj.\")\n",
    "plt.plot(masses, limits_Dir, label=\"Dir.\")\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167745e-cd79-4559-b68f-b93151b57ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_Dirac = {}\n",
    "ratio_sum = 0\n",
    "for i, HNL_mass in enumerate(HNL_masses):\n",
    "    ratio_Dirac[HNL_mass] = limts_Maj[i]/limits_Dir[i]\n",
    "    ratio_sum += ratio_Dirac[HNL_mass]\n",
    "    \n",
    "print(ratio_Dirac)\n",
    "print(ratio_sum/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22113a-be3b-404d-95b5-9c4154641beb",
   "metadata": {},
   "source": [
    "## Removing 2MeV point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d61f76-b44a-42a5-ba62-7e5340758ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Constants)\n",
    "if Params_pyhf[\"Load_lepton_hists\"]==True:\n",
    "    HNL_masses=Constants.HNL_mass_samples.copy()\n",
    "    if removed_2MeV == False:\n",
    "        print(\"Removing the first element in the limit and mass lists\")\n",
    "        exp_limit.pop(0)\n",
    "        obs_limit.pop(0)\n",
    "        HNL_masses.pop(0)\n",
    "        exp_1sig_up.pop(0)\n",
    "        exp_2sig_up.pop(0)\n",
    "        exp_1sig_down.pop(0)\n",
    "        exp_2sig_down.pop(0)\n",
    "        print(HNL_masses)\n",
    "        print(exp_limit)\n",
    "        print(obs_limit)\n",
    "    else: print(\"First element has already been removed\")\n",
    "    removed_2MeV = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8437737-0007-42bb-9cfa-bd8e2a45194e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving limit as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e73a7-2fee-4aad-9067-e3dd5d7d9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = HNL_masses\n",
    "\n",
    "if Params_pyhf[\"Stats_only\"] == True: stats =  \"Stats_only\"\n",
    "elif Params_pyhf[\"Use_flat_sys\"] == True: stats = \"Flat_sys\"\n",
    "else: stats = \"Full_sys\"\n",
    "    \n",
    "if Params_pyhf[\"Use_part_only\"] == True: part_hist = str(Params_pyhf[\"Num_bins_for_calc\"])+\"_bins\"\n",
    "else: part_hist = \"full_hist\"\n",
    "\n",
    "print(masses)\n",
    "print(exp_limit)\n",
    "\n",
    "filename = \"Dirac_ee_21_April.csv\"\n",
    "\n",
    "# r = zip(masses, exp_limit)\n",
    "r = zip(masses, obs_limit)\n",
    "if Params_pyhf[\"Load_lepton_hists\"] == True:\n",
    "    savename = f'limit_files/My_limits/{stats}_{part_hist}_{filename}'\n",
    "    with open(f'limit_files/My_limits/{stats}_{part_hist}_{filename}', \"w\") as s:\n",
    "        w = csv.writer(s)\n",
    "        for row in r:\n",
    "            w.writerow(row)\n",
    "            \n",
    "if Params_pyhf[\"Load_pi0_hists\"] == True:\n",
    "    savename = f'limit_files/My_limits/{stats}_{part_hist}_pi0_{filename}'\n",
    "    with open(f'limit_files/My_limits/{stats}_{part_hist}_pi0_{filename}', \"w\") as s:\n",
    "        w = csv.writer(s)\n",
    "        for row in r:\n",
    "            w.writerow(row)\n",
    "            \n",
    "if Params_pyhf[\"Load_single_r1_file\"] == True:\n",
    "    savename = f'limit_files/My_limits/{stats}_{part_hist}_single_file_{filename}'\n",
    "    with open(f'limit_files/My_limits/{stats}_{part_hist}_single_file_{filename}', \"w\") as s:\n",
    "        w = csv.writer(s)\n",
    "        for row in r:\n",
    "            w.writerow(row)\n",
    "\n",
    "print()\n",
    "print(\"Save name is : \\'\"+savename+\"\\'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a309c42f-47a3-4d38-a2a7-fb09f997f2ab",
   "metadata": {},
   "source": [
    "## Saving sigma bands for brazil plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedea031-79e6-4656-840d-a6f6a4a42c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = \"limit_files/Brazil_plot/\"\n",
    "to_save_names = [\"exp_1sig_up\",\"exp_1sig_down\",\"exp_2sig_up\",\"exp_2sig_down\",\"exp_limit\",\"obs_limit\"]\n",
    "to_save_lists = [exp_1sig_up,exp_1sig_down,exp_2sig_up,exp_2sig_down,exp_limit,obs_limit]\n",
    "\n",
    "if Params_pyhf[\"Load_lepton_hists\"] == True:decay_type = \"ee\"\n",
    "elif Params_pyhf[\"Load_pi0_hists\"]==True: decay_type = \"pi0\"\n",
    "else: decay_type = \"single_file\"\n",
    "\n",
    "filename=f\"_{decay_type}_2_May.csv\"\n",
    "for i, lim in enumerate(to_save_lists):\n",
    "\n",
    "    r = zip(masses, lim)\n",
    "    savestr=to_save_names[i]\n",
    "    savename = save_loc+savestr+filename\n",
    "    with open(savename, \"w\") as s:\n",
    "        w = csv.writer(s)\n",
    "        for row in r:\n",
    "            w.writerow(row)\n",
    "\n",
    "print(\"Last saved is \" + savename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeac33b-58bc-4054-b1f4-20fe1f20cf3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Testing adjacent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e7cd9-cf28-49f0-a3a0-bef0f63c1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacent_hist_dict_run1, adjacent_hist_dict_run3 = {}, {}\n",
    "R1_BKG_ERR_dict_ADJ, R3_BKG_ERR_dict_ADJ, R1_SIGNAL_ERR_dict_ADJ, R3_SIGNAL_ERR_dict_ADJ = {}, {}, {}, {}\n",
    "mass_point = 50\n",
    "test_models = [20, 100]\n",
    "for mass_model in test_models:\n",
    "    adjacent_hist_dict_run1[mass_model] = uproot.open(f'bdt_output/adjacent_models/run1_{mass_point}MeV_{mass_model}MeV_model.root')\n",
    "    adjacent_hist_dict_run3[mass_model] = uproot.open(f'bdt_output/adjacent_models/run3_{mass_point}MeV_{mass_model}MeV_model.root')\n",
    "    R1_BKG_ERR_dict_ADJ, R1_SIGNAL_ERR_dict_ADJ = Calculate_total_uncertainty(Params_pyhf, adjacent_hist_dict_run1)\n",
    "    R3_BKG_ERR_dict_ADJ, R3_SIGNAL_ERR_dict_ADJ = Calculate_total_uncertainty(Params_pyhf, adjacent_hist_dict_run3)\n",
    "    # R1_BKG_ERR_dict_ADJ[mass_model]=R1_BKG_ERR_dict[mass_point]\n",
    "    # R3_BKG_ERR_dict_ADJ[mass_model]=R3_BKG_ERR_dict[mass_point]\n",
    "    # R1_SIGNAL_ERR_dict_ADJ[mass_model]=R1_SIGNAL_ERR_dict[mass_point]\n",
    "    # R3_SIGNAL_ERR_dict_ADJ[mass_model]=R3_SIGNAL_ERR_dict[mass_point]\n",
    "adjacent_hist_dict_run1[mass_point] = hist_dict_run1[mass_point]\n",
    "adjacent_hist_dict_run3[mass_point] = hist_dict_run3[mass_point]\n",
    "R1_BKG_ERR_dict_ADJ[mass_point]=R1_BKG_ERR_dict[mass_point]\n",
    "R3_BKG_ERR_dict_ADJ[mass_point]=R3_BKG_ERR_dict[mass_point]\n",
    "R1_SIGNAL_ERR_dict_ADJ[mass_point]=R1_SIGNAL_ERR_dict[mass_point]\n",
    "R3_SIGNAL_ERR_dict_ADJ[mass_point]=R3_SIGNAL_ERR_dict[mass_point]\n",
    "\n",
    "R1_BKG_ADJ, R1_SIGNAL_ADJ = Add_bkg_hists_make_signal(adjacent_hist_dict_run1)\n",
    "R3_BKG_ADJ, R3_SIGNAL_ADJ = Add_bkg_hists_make_signal(adjacent_hist_dict_run3)\n",
    "\n",
    "R1_output_ADJ = Functions.Make_into_lists(Params_pyhf, R1_BKG_ADJ, R1_SIGNAL_ADJ, R1_BKG_ERR_dict_ADJ, R1_SIGNAL_ERR_dict_ADJ)\n",
    "R3_output_ADJ = Functions.Make_into_lists(Params_pyhf, R3_BKG_ADJ, R3_SIGNAL_ADJ, R3_BKG_ERR_dict_ADJ, R3_SIGNAL_ERR_dict_ADJ)\n",
    "\n",
    "list_input_dicts_ADJ = [R1_output_ADJ, R3_output_ADJ]\n",
    "# list_input_dicts = [R1_output, R1_output] #Used when I didn't have Run3\n",
    "\n",
    "Total_dict_ADJ = Functions.Create_final_appended_runs_dict(list_input_dicts_ADJ)\n",
    "print(Total_dict_ADJ[100].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af73b4a8-0dad-4d60-a962-c7155024db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Total_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7688a367-1e06-44a8-95df-464d27a37a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_ADJ = create_model_dict(Total_dict_ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eeb77d-75dc-4454-9988-2c852640c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test = adjacent_hist_dict_run1.keys()\n",
    "DATA_OBS_dict = {}\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples: #removing the 240MeV point\n",
    "for HNL_mass in list_test:\n",
    "    init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "    model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "    bkg_pars = init_pars.copy()\n",
    "    bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "    model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "    DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass][\"BKG_dict\"]+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "    model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639d271-7e7c-4cff-9ed6-d20f1f7095cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for HNL_mass in list_test:\n",
    "\n",
    "    if Params_pyhf[\"Use_toys\"] == False:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"asymptotics\",\n",
    "            )\n",
    "    if Params_pyhf[\"Use_toys\"] == True:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"toybased\",\n",
    "            ntoys=Params_pyhf[\"Num_toys\"],\n",
    "            track_progress=True,\n",
    "            )\n",
    "    \n",
    "    print(f\"{HNL_mass}MeV\")\n",
    "    for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "        print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ab734-f7dd-40bb-8b3a-312eda608e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "print(\"If the output of the following is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "poi_values = np.linspace(0.001, 2, 100) #I could make a dict of this and have different pois for different mass points\n",
    "print(\"Max value is \" + str(max(poi_values)))\n",
    "print(\"Min value is \" + str(min(poi_values)))\n",
    "print(\"Next value is \" + str(poi_values[1]))\n",
    "print(\"Next value is \" + str(poi_values[2]) + \"\\n\")\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "# list_test = [180, 200, 220, 240, 245]\n",
    "for HNL_mass in list_test:\n",
    "\n",
    "    # poi_values = np.linspace(0.001, 4, 100)\n",
    "    obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "        DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_values, level=0.1, return_results=True\n",
    "    )\n",
    "\n",
    "    print(f\"Upper limit {HNL_mass}MeV (obs): μ = {obs_limit_dict[HNL_mass]:.6f}\")\n",
    "    print(f\"Upper limit {HNL_mass}MeV (exp): μ = {exp_limits_dict[HNL_mass][2]:.6f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb324293-40db-4be3-94b0-4498860ed94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = [] #entry 2\n",
    "exp_1sig_up = [] #entry 3\n",
    "exp_1sig_down = [] #entry 1\n",
    "exp_2sig_up = [] #entry 4\n",
    "exp_2sig_down = [] #entry 0\n",
    "obs_limit = []\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in exp_limits_dict:\n",
    "    theta_squared = (theta_dict_scaled[mass_point])**2\n",
    "    print(theta_squared)\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_dict[HNL_mass][2])*theta_squared\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][0])*theta_squared)\n",
    "    \n",
    "    LIMIT = np.sqrt(obs_limit_dict[HNL_mass])*theta_squared\n",
    "    # if HNL_mass in Constants.Old_generator_mass_points:\n",
    "    #     EXP_LIMIT = EXP_LIMIT*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "    #     LIMIT = LIMIT*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a5ecc-57ca-4f56-bc74-cc00bdadfdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [6.912810521978472e-06, 7.3827958770328454e-06, 1.9840221581842897e-05]\n",
    "x_points = [50, 50, 50]\n",
    "labels = [\"correct\", \"20MeV\", \"50MeV\"]\n",
    "for i, point in enumerate(points):\n",
    "    plt.plot(x_points[i],point,label=labels[i],marker=\"o\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c38a19-4c02-4f26-8aef-5947b26cf4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_20 = 6.912810521978472e-06/7.3827958770328454e-06\n",
    "rat_100 = 6.912810521978472e-06/1.9840221581842897e-05\n",
    "print(\"20MeV model is \" + str(rat_20))\n",
    "print(\"100MeV model is \" + str(rat_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c1c24-58ae-4433-8b1c-30f24a80e5b1",
   "metadata": {},
   "source": [
    "## Brazil plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2d76f-c16f-47a0-8c41-3889c5844e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting ee limit\n",
    "if Params_pyhf[\"Load_lepton_hists\"]==True:\n",
    "    plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "    savefig = False\n",
    "\n",
    "    plt.plot(HNL_masses,np.array(obs_limit),lw=4,ls='-',color='black',label='Observed')\n",
    "    plt.plot(HNL_masses,np.array(exp_limit),lw=2,ls='--',color='red',label='Expected')\n",
    "    plt.fill_between(HNL_masses,np.array(exp_2sig_down),np.array(exp_2sig_up),color='yellow',label=r'Exp. 2$\\sigma$')\n",
    "    plt.fill_between(HNL_masses,np.array(exp_1sig_down),np.array(exp_1sig_up),color='lightgreen',label=r'Exp. 1$\\sigma$')\n",
    "\n",
    "    plt.ylabel(r'$|U_{\\mu 4}|^2$ Limit at 90% CL',fontsize=22)\n",
    "    plt.xlabel('HNL Mass [MeV]',fontsize=22)\n",
    "\n",
    "    plt.ylim(5e-7,6e-3)\n",
    "    plt.legend(loc=\"lower left\",ncol=2,frameon=False,fontsize=22)\n",
    "    # plt.text(0.99,0.95,r'MicroBooNE NuMI POT:$7.01\\times 10^{20}$',fontsize=20,color='black',alpha=1,verticalalignment='top',horizontalalignment='right',transform=plt.gca().transAxes)\n",
    "    plt.tick_params(axis='x', labelsize=20)\n",
    "    plt.tick_params(axis='y', labelsize=20)\n",
    "    plt.xlim(0,155)\n",
    "    # plt.xlim(0,250)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    savename = \"ee_observed\"\n",
    "\n",
    "    if savefig == True:\n",
    "        plt.savefig('plots/Limits/'+savename+'.pdf',bbox_inches='tight', pad_inches=0.3)\n",
    "        plt.savefig('plots/Limits/'+savename+'.png',bbox_inches='tight', pad_inches=0.3)\n",
    "\n",
    "    plt.grid(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ce420-c92b-488f-9d87-1b2bac39dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting pi0 limit\n",
    "if Params_pyhf[\"Load_pi0_hists\"]==True:\n",
    "    plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "    savefig = True\n",
    "\n",
    "    plt.plot(HNL_masses,np.array(obs_limit),lw=5,ls='-',color='black',label='Observed')\n",
    "    plt.plot(HNL_masses,np.array(exp_limit),lw=2,ls='--',color='red',label='Expected')\n",
    "    plt.fill_between(HNL_masses,np.array(exp_2sig_down),np.array(exp_2sig_up),color='yellow',label=r'Exp. 2$\\sigma$')\n",
    "    plt.fill_between(HNL_masses,np.array(exp_1sig_down),np.array(exp_1sig_up),color='lightgreen',label=r'Exp. 1$\\sigma$')\n",
    "\n",
    "    plt.ylabel(r'$|U_{\\mu 4}|^2$ Limit at 90% CL',fontsize=22)\n",
    "    plt.xlabel('HNL Mass [MeV]',fontsize=22)\n",
    "\n",
    "    plt.ylim(1e-8,3e-7)\n",
    "    plt.legend(loc=\"lower left\",ncol=2,frameon=False,fontsize=22)\n",
    "    # plt.text(0.99,0.95,r'MicroBooNE NuMI POT:$7.01\\times 10^{20}$',fontsize=20,color='black',alpha=1,verticalalignment='top',horizontalalignment='right',transform=plt.gca().transAxes)\n",
    "    plt.tick_params(axis='x', labelsize=20)\n",
    "    plt.tick_params(axis='y', labelsize=20)\n",
    "    plt.xlim(140,255)\n",
    "    # plt.xlim(0,250)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    savename = \"pi0_observed\"\n",
    "\n",
    "    if savefig == True:\n",
    "        plt.savefig('plots/Limits/'+savename+'.pdf',bbox_inches='tight', pad_inches=0.3)\n",
    "        plt.savefig('plots/Limits/'+savename+'.png',bbox_inches='tight', pad_inches=0.3)\n",
    "\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aabef30-4da9-4e20-94ae-f99218373d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting BOTH on one plot\n",
    "plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "savefig = False\n",
    "\n",
    "save_loc = \"limit_files/Brazil_plot/\"\n",
    "to_save_names = [\"exp_1sig_up\",\"exp_1sig_down\",\"exp_2sig_up\",\"exp_2sig_down\",\"exp_limit\",\"obs_limit\"]\n",
    "to_save_lists = [exp_1sig_up,exp_1sig_down,exp_2sig_up,exp_2sig_down,exp_limit,obs_limit]\n",
    "\n",
    "#ee curves\n",
    "decay_type=\"ee\"\n",
    "filename=f\"_{decay_type}_21_April.csv\"\n",
    "# for i, lim in enumerate(to_save_lists):\n",
    "\n",
    "ee_exp_1sig_up = Functions.Pandafy_new(save_loc+\"exp_1sig_up\"+filename)\n",
    "ee_exp_1sig_down = Functions.Pandafy_new(save_loc+\"exp_1sig_down\"+filename)\n",
    "ee_exp_2sig_up = Functions.Pandafy_new(save_loc+\"exp_2sig_up\"+filename)\n",
    "ee_exp_2sig_down = Functions.Pandafy_new(save_loc+\"exp_2sig_down\"+filename)\n",
    "ee_exp_limit = Functions.Pandafy_new(save_loc+\"exp_limit\"+filename)\n",
    "ee_obs_limit = Functions.Pandafy_new(save_loc+\"obs_limit\"+filename)\n",
    "HNL_masses_ee = [10, 20, 50, 100, 150]\n",
    "\n",
    "plt.plot(HNL_masses_ee,np.array(ee_obs_limit[\"Value\"]),lw=5,ls='-',color='black')\n",
    "plt.plot(HNL_masses_ee,np.array(ee_exp_limit[\"Value\"]),lw=2,ls='--',color='red')\n",
    "plt.fill_between(HNL_masses_ee,np.array(ee_exp_2sig_down[\"Value\"]),np.array(ee_exp_2sig_up[\"Value\"]),color='yellow')\n",
    "plt.fill_between(HNL_masses_ee,np.array(ee_exp_1sig_down[\"Value\"]),np.array(ee_exp_1sig_up[\"Value\"]),color='lightgreen')\n",
    "\n",
    "#pi0 curves\n",
    "decay_type=\"pi0\"\n",
    "filename=f\"_{decay_type}_21_April.csv\"\n",
    "HNL_masses_pi0 = [150, 180, 200, 220, 240, 245]\n",
    "\n",
    "pi0_exp_1sig_up = Functions.Pandafy_new(save_loc+\"exp_1sig_up\"+filename)\n",
    "pi0_exp_1sig_down = Functions.Pandafy_new(save_loc+\"exp_1sig_down\"+filename)\n",
    "pi0_exp_2sig_up = Functions.Pandafy_new(save_loc+\"exp_2sig_up\"+filename)\n",
    "pi0_exp_2sig_down = Functions.Pandafy_new(save_loc+\"exp_2sig_down\"+filename)\n",
    "pi0_exp_limit = Functions.Pandafy_new(save_loc+\"exp_limit\"+filename)\n",
    "pi0_obs_limit = Functions.Pandafy_new(save_loc+\"obs_limit\"+filename)\n",
    "\n",
    "\n",
    "plt.plot(HNL_masses_pi0,np.array(pi0_obs_limit[\"Value\"]),lw=5,ls='-',color='black',label='Observed')\n",
    "plt.plot(HNL_masses_pi0,np.array(pi0_exp_limit[\"Value\"]),lw=2,ls='--',color='red',label='Expected')\n",
    "plt.fill_between(HNL_masses_pi0,np.array(pi0_exp_2sig_down[\"Value\"]),np.array(pi0_exp_2sig_up[\"Value\"]),color='yellow',label=r'Exp. 2$\\sigma$')\n",
    "plt.fill_between(HNL_masses_pi0,np.array(pi0_exp_1sig_down[\"Value\"]),np.array(pi0_exp_1sig_up[\"Value\"]),color='lightgreen',label=r'Exp. 1$\\sigma$')\n",
    "\n",
    "\n",
    "plt.ylabel(r'$|U_{\\mu 4}|^2$ Limit at 90% CL',fontsize=24)\n",
    "plt.xlabel('HNL Mass [MeV]',fontsize=22)\n",
    "\n",
    "plt.ylim(1e-8,6e-3)\n",
    "plt.legend(loc=\"lower left\",ncol=2,frameon=False,fontsize=24)\n",
    "plt.text(0.99,0.95,r'MicroBooNE NuMI POT:$7.01\\times 10^{20}$',fontsize=24,color='black',alpha=1,\n",
    "         verticalalignment='top',horizontalalignment='right',transform=plt.gca().transAxes)\n",
    "plt.tick_params(axis='x', labelsize=20)\n",
    "plt.tick_params(axis='y', labelsize=20)\n",
    "plt.xlim(0,255)\n",
    "# plt.xlim(0,250)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "savename = \"Both_channels_observed\"\n",
    "\n",
    "# plt.grid(True,lw=1, alpha=0.3)\n",
    "\n",
    "if savefig == True:\n",
    "    plt.savefig('plots/Limits/'+savename+'.pdf',bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.savefig('plots/Limits/'+savename+'.png',bbox_inches='tight', pad_inches=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830b945-1556-4c50-8ce1-86681c781f8b",
   "metadata": {},
   "source": [
    "## Dumping model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc7172-6236-477f-9d5c-1a50e81519ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 100\n",
    "print(json.dumps(model_dict[HNL_mass].spec, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416cdf2-0f11-4ae6-ac1b-9260325f2396",
   "metadata": {},
   "source": [
    "# Comparing to Collie output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608e383-7ca2-43da-99f1-4b5660318ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing to Collie output:\n",
    "\n",
    "# collie_exp = 17010\n",
    "# collie_obs = 20522\n",
    "\n",
    "collie_exp = 16956.64\n",
    "collie_obs = 20029.17\n",
    "\n",
    "SF = scaling_dict[150]\n",
    "\n",
    "pyhf_obs = 0.5027*SF\n",
    "pyhf_exp = 0.4109*SF\n",
    "\n",
    "print(\"collie obs mu is \" + str(collie_obs))\n",
    "print(\"collie exp mu is \" + str(collie_exp))\n",
    "print()\n",
    "\n",
    "print(\"pyhf obs mu is \" + str(pyhf_obs))\n",
    "print(\"pyhf exp mu is \" + str(pyhf_exp))\n",
    "print()\n",
    "\n",
    "print(\"collie divided by pyhf obs is \" + str(collie_obs/pyhf_obs))\n",
    "print(\"collie divided by pyhf exp is \" + str(collie_exp/pyhf_exp))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc1f65-4dfd-45cc-9351-ecdc937f7fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for HNL_mass in [HNL_mass]:\n",
    "    theta_squared = (theta_dict_scaled[HNL_mass])**2\n",
    "    \n",
    "    EXP_LIMIT = np.sqrt(exp_limits_single[2])*theta_squared\n",
    "    LIMIT = np.sqrt(obs_limit_single)*theta_squared\n",
    "    print(\"-----pyhf-----\")\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "print(\"-----Collie-----\")\n",
    "collie_exp_limit = np.sqrt(collie_exp)*theta_dict[HNL_mass]**2 #Collie input is NOT scaled\n",
    "collie_obs_limit = np.sqrt(collie_obs)*theta_dict[HNL_mass]**2 #Collie input is NOT scaled\n",
    "print(f\"Expected {HNL_mass}MeV limit is \" + str(collie_exp_limit))\n",
    "print(f\"Observed {HNL_mass}MeV limit is \" + str(collie_obs_limit)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80bf9e-cc1a-4d6a-807e-ec8653445b9d",
   "metadata": {},
   "source": [
    "# End of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaefdcf-f13a-4a02-b9cd-0155fe6e007f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
