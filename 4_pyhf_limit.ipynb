{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe0cab0-578b-42ea-86af-07342852e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<pyhf.tensor.numpy_backend.numpy_backend object at 0x7f4395e53bc0>, <pyhf.optimize.scipy_optimizer object at 0x7f4395e3f880>)\n",
      "pyhf version: 0.7.1\n",
      "Successful!\n"
     ]
    }
   ],
   "source": [
    "#Loading libraries\n",
    "import os, sys, string, time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import uproot\n",
    "import math\n",
    "import pickle\n",
    "import csv\n",
    "from importlib import reload\n",
    "import copy\n",
    "import pyhf\n",
    "import csv\n",
    "import matplotlib.ticker as mticker\n",
    "# from pyhf.contrib.viz import brazil\n",
    "\n",
    "import Utilities.Plotter as PT\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print(pyhf.get_backend())\n",
    "print(\"pyhf version:\",pyhf.__version__)\n",
    "\n",
    "print(\"Successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6939111e-d766-4112-ab60-1fd7c65239ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FLAT systematic uncertainty on signal and background\n",
      "With 30.0% on overlay, and 75.0% on dirt.\n",
      "With 36.05551275463989% on all signal\n",
      "[10, 100, 150]\n"
     ]
    }
   ],
   "source": [
    "Params_pyhf = {\"Stats_only\":False,\n",
    "               \"Use_flat_sys\":True,\n",
    "               \"Num_bins_for_calc\":5,\n",
    "               \"Use_part_only\":False,\n",
    "               \"Use_toys\":True,\n",
    "               \"Num_toys\":1000,\n",
    "               \"Load_lepton_hists\":False,\n",
    "               \"Load_pi0_hists\":False,\n",
    "               \"Flat_bkg_overlay_frac\":0.3,\n",
    "               \"Flat_bkg_dirt_frac\":0.75,\n",
    "               \"Flat_bkg_EXT_frac\":0.0,\n",
    "               \"Flat_sig_detvar\":0.2, #This is very conservative, could be fed in per mass point from signal detvar script\n",
    "               \"Signal_flux_error\":0.3, #This comes from the KDAR flux uncertainty.\n",
    "               \"Overlay_detvar_frac\":0.3,\n",
    "               \"Load_lepton_dirac\":True,\n",
    "               \"Load_pi0_dirac\":False,\n",
    "               \"Load_single_r1_file\":False}\n",
    "\n",
    "scaled = False #Just to keep track of if the histograms have been scaled\n",
    "\n",
    "if Params_pyhf[\"Load_lepton_hists\"] ==True: \n",
    "    name_type=\"ee\"\n",
    "    HNL_masses_list = Constants.HNL_mass_samples\n",
    "if Params_pyhf[\"Load_pi0_hists\"] ==True: \n",
    "    name_type=\"pi0\"\n",
    "    HNL_masses_list = Constants.HNL_mass_pi0_samples\n",
    "if Params_pyhf[\"Load_lepton_dirac\"] ==True: \n",
    "    name_type=\"ee_dirac\"\n",
    "    HNL_masses_list = Constants.HNL_ee_dirac_mass_samples\n",
    "if Params_pyhf[\"Load_pi0_dirac\"] ==True: \n",
    "    name_type=\"pi0_dirac\"\n",
    "    HNL_masses_list = Constants.HNL_pi0_dirac_mass_samples\n",
    "BDT_name = \"_full_Finished_10\"\n",
    "# filename = name_type+'_EXT_full_Finished'\n",
    "filename = name_type+BDT_name\n",
    "\n",
    "Functions.pyhf_params(Params_pyhf)\n",
    "\n",
    "print(HNL_masses_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431c728d-46f7-48d0-8cf8-e76c5fa5738f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dirac samples\n",
      "Missing hists for Run1 are: \n",
      "['ppfx_uncertainty;1', 'Genie_uncertainty;1', 'Reinteraction_uncertainty;1', 'ppfx_uncertainty_frac;1', 'Genie_uncertainty_frac;1', 'Reinteraction_uncertainty_frac;1', 'overlay_DetVar_uncertainty;1', 'overlay_DetVar_uncertainty_frac;1', 'signal_DetVar_uncertainty;1', 'signal_DetVar_uncertainty_frac;1']\n",
      "thetas are:\n",
      "{10: 0.01, 100: 1e-04, 150: 1e-04}\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# HNL_masses_list = [150,200,245]\n",
    "#USE THIS ONE FOR PREVIOUS HISTS\n",
    "# hist_dict_run1, hist_dict_run3, theta_dict = Functions.Load_pyhf_files(\"FINAL_3.root\", \n",
    "#                                                                        Params_pyhf)#, HNL_masses = HNL_masses_list)\n",
    "\n",
    "hist_dict_run1, hist_dict_run3, theta_dict = Functions.New_Load_pyhf_files(f\"{filename}.root\",\n",
    "                                                                           Params_pyhf, HNL_masses = HNL_masses_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2e45e-7b0c-45cc-a21a-65bfd2fafdc3",
   "metadata": {},
   "source": [
    "## Creating dictionaries containing all necessary information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c966e86-eb23-4594-a4fc-a03c6fe96440",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Functions)\n",
    "zero_bins_errors_run1 = Functions.make_zero_bin_unc(hist_dict_run1, Constants.run1_POT_scaling_dict, Params_pyhf)\n",
    "zero_bins_errors_run3 = Functions.make_zero_bin_unc(hist_dict_run3, Constants.run3_POT_scaling_dict, Params_pyhf)\n",
    "\n",
    "TOT_R1_ERR = Functions.Full_calculate_total_uncertainty(Params_pyhf, hist_dict_run1, zero_bins_errors_run1)\n",
    "TOT_R3_ERR = Functions.Full_calculate_total_uncertainty(Params_pyhf, hist_dict_run3, zero_bins_errors_run3)\n",
    "\n",
    "R1_BKG, R1_SIGNAL = Functions.Add_bkg_hists_make_signal(hist_dict_run1)\n",
    "R3_BKG, R3_SIGNAL = Functions.Add_bkg_hists_make_signal(hist_dict_run3)\n",
    "\n",
    "R1_output = Functions.Make_into_lists(Params_pyhf, R1_BKG, R1_SIGNAL, TOT_R1_ERR)\n",
    "R3_output = Functions.Make_into_lists(Params_pyhf, R3_BKG, R3_SIGNAL, TOT_R3_ERR)\n",
    "\n",
    "list_input_dicts = [R1_output, R3_output]\n",
    "\n",
    "Total_dict_both = Functions.Create_final_appended_runs_dict(list_input_dicts)\n",
    "Total_dict_run1 = Functions.Create_final_appended_runs_dict([R1_output])\n",
    "Total_dict_run3 = Functions.Create_final_appended_runs_dict([R3_output])\n",
    "\n",
    "if Params_pyhf[\"Use_part_only\"]==True:\n",
    "    NUMBINS = Params_pyhf[\"Num_bins_for_calc\"]\n",
    "else: NUMBINS=30 #This will just give the full hist\n",
    "    \n",
    "if 'data;1' in hist_dict_run1[150]:\n",
    "    Total_dict_run1=Functions.add_data(Total_dict_run1, hist_dict_run1, NUMBINS)\n",
    "    Total_dict_run3=Functions.add_data(Total_dict_run3, hist_dict_run3, NUMBINS)\n",
    "    Total_dict_both=Functions.add_data_appended(Total_dict_both, hist_dict_run1, hist_dict_run3, NUMBINS)\n",
    "    \n",
    "#Create separate dirt normalisation uncertainties for Run1 and Run3 \n",
    "for HNL_mass in Total_dict_both:\n",
    "\n",
    "    dirt_vals = Total_dict_both[HNL_mass]['BKG_DIRT']\n",
    "    numbins = int(len(dirt_vals)/2)\n",
    "    r1_dirt = dirt_vals[:numbins] + list(np.zeros(numbins))\n",
    "    r3_dirt = list(np.zeros(numbins)) + dirt_vals[numbins:] \n",
    "    Total_dict_both[HNL_mass]['BKG_DIRT_R1'] = r1_dirt\n",
    "    Total_dict_both[HNL_mass]['BKG_DIRT_R3'] = r3_dirt\n",
    "    \n",
    "print(Total_dict_both[150].keys())\n",
    "\n",
    "sig_stat, bkg_stat = Functions.create_stat_unc_safe(Total_dict_both)\n",
    "\n",
    "sig_stat_r1, bkg_stat_r1 = Functions.create_stat_unc_safe(Total_dict_run1)\n",
    "sig_stat_r3, bkg_stat_r3 = Functions.create_stat_unc_safe(Total_dict_run3)\n",
    "\n",
    "sig_stat, overlay_stat, dirt_stat, beamoff_stat = Functions.create_individual_stat_unc_safe(Total_dict_both)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b092e9e5-a61e-448a-97f5-2a5bee784aeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Making uncertainty breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c7c8d-4915-4c9c-abf9-bcdc206025cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 150\n",
    "hist_dict_run3[HNL_mass].keys()\n",
    "Uncertainties_list = [\"Statistics\", \"Detector\", r\"$\\nu$ Flux\", r\"$\\nu$ Cross-section\", \"Reinteractions\", \"Dirt normalization\", \"Flux rate\", \"Total\"]\n",
    "Unc_colors = {\"Statistics\":\"black\", \"Detector\":\"C1\", r\"$\\nu$ Flux\":\"C2\", r\"$\\nu$ Cross-section\":\"C3\",\n",
    "              \"Reinteractions\":\"C4\", \"Dirt normalization\":\"C5\", \"Flux rate\":\"C2\", \"Total\":\"gray\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17113cd8-a250-49c8-bd6b-2e2f5ddf978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bins_dicts(hist_dict, Params):\n",
    "    \"\"\"\n",
    "    Given a histogram dict returns the bins and bin centres dictionaries.\n",
    "    \"\"\"\n",
    "    bins_dict, bins_cents_dict = {}, {}\n",
    "    if Params[\"Use_part_only\"] == False:\n",
    "        for HNL_mass in hist_dict:\n",
    "            bins = hist_dict[HNL_mass]['bkg_overlay'].to_numpy()[1]\n",
    "            bin_cents = (bins[:-1]+bins[1:])/2\n",
    "            bins_dict[HNL_mass] = bins\n",
    "            bins_cents_dict[HNL_mass] = bin_cents\n",
    "    if Params[\"Use_part_only\"] == True:\n",
    "        for HNL_mass in hist_dict:\n",
    "            Num_bins=Params[\"Num_bins_for_calc\"]\n",
    "            bins = hist_dict[HNL_mass]['bkg_overlay'].to_numpy()[1]\n",
    "            bins = bins[-1*(Num_bins+1):]\n",
    "            bin_cents = (bins[:-1]+bins[1:])/2\n",
    "            bins_dict[HNL_mass] = bins\n",
    "            bins_cents_dict[HNL_mass] = bin_cents\n",
    "            \n",
    "    return bins_dict, bins_cents_dict\n",
    "    \n",
    "bins_dict_r1, bins_cent_dict_r1 = make_bins_dicts(hist_dict_run1, Params_pyhf)\n",
    "bins_dict_r3, bins_cent_dict_r3 = make_bins_dicts(hist_dict_run3, Params_pyhf)\n",
    "\n",
    "bins_overflow_r1, bins_cents_overflow_r1 = Functions.make_overflow_bin(bins_dict_r1, bins_cent_dict_r1)\n",
    "bins_overflow_r3, bins_cents_overflow_r3 = Functions.make_overflow_bin(bins_dict_r3, bins_cent_dict_r3)\n",
    "\n",
    "def make_xlims_dict(bins_dict, spacing, lower = None):\n",
    "    \"\"\"\n",
    "    Making a dict of xlims for plotting several mass points at once.\n",
    "    Also returns a dict of xticks for the purpose of indicating the overflow.\n",
    "    \"\"\"\n",
    "    xlims_adjusted, xticks_adjusted = {}, {}\n",
    "    vals_dict={}\n",
    "    for HNL_mass in bins_dict:\n",
    "        if isinstance(lower,(int, float)): lower_val = lower\n",
    "        else: lower_val = bins_dict[HNL_mass][0]\n",
    "        xlims_adjusted[HNL_mass] = [lower_val,bins_dict[HNL_mass][-1]]\n",
    "        ticks = np.arange(bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1], spacing)\n",
    "        if ticks[-1] != bins_dict[HNL_mass][-2]: ticks = np.append(ticks, bins_dict[HNL_mass][-1]-1)\n",
    "        ticks_strings = []\n",
    "        vals = []\n",
    "        for val in ticks:\n",
    "            ticks_strings.append(str(int(val)))\n",
    "            vals.append(val)\n",
    "        ticks_strings[-1] = str(ticks_strings[-1])+\"+\"\n",
    "        xticks_adjusted[HNL_mass] = ticks_strings\n",
    "        vals_dict[HNL_mass] = vals\n",
    "        \n",
    "    return xlims_adjusted, xticks_adjusted, vals_dict\n",
    "\n",
    "xlims_dict_r1, xticks_dict_r1, vals_dict_r1 = make_xlims_dict(bins_overflow_r1, 1)\n",
    "xlims_dict_r3, xticks_dict_r3, vals_dict_r3 = make_xlims_dict(bins_overflow_r3, 1)\n",
    "\n",
    "def Make_hist_dict_from_Total_dict(Total_dict):\n",
    "    \"\"\"\n",
    "    Creating a dict of just hists from Total dict.\n",
    "    For the purposes of creating bins.\n",
    "    \"\"\"\n",
    "    bins_dict, bins_cents_dict = {}, {}\n",
    "    for HNL_mass in Total_dict:\n",
    "        bins = np.arange(len(Total_dict[HNL_mass]['TOT_BKG_VALS'])+1)\n",
    "        bin_cents = (bins[:-1]+bins[1:])/2\n",
    "        bins_dict[HNL_mass] = bins\n",
    "        bins_cents_dict[HNL_mass] = bin_cents\n",
    "        \n",
    "    return bins_dict, bins_cents_dict\n",
    "\n",
    "Total_bins_dict, Total_bins_cent_dict = Make_hist_dict_from_Total_dict(Total_dict_both)\n",
    "Total_bins_overflow, Total_bins_cents_overflow = Functions.make_overflow_bin(Total_bins_dict, Total_bins_cent_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bdbe24-bc7c-40be-8db8-1c4e6656429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Functions)\n",
    "\n",
    "sig_stat, bkg_stat = Functions.create_stat_unc_safe(Total_dict_both)\n",
    "\n",
    "sig_stat_r1, bkg_stat_r1 = Functions.create_stat_unc_safe(Total_dict_run1)\n",
    "sig_stat_r3, bkg_stat_r3 = Functions.create_stat_unc_safe(Total_dict_run3)\n",
    "\n",
    "sig_stat, overlay_stat, dirt_stat, beamoff_stat = Functions.create_individual_stat_unc_safe(Total_dict_both)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198406b-55f8-42ad-b0dd-8eab81934653",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(PT)\n",
    "\n",
    "Run = \"run3\"\n",
    "\n",
    "if Run == \"run1\": \n",
    "    hist_dict, bins_overflow, bins_cents = hist_dict_run1, bins_overflow_r1, bins_cents_overflow_r1\n",
    "    bkg_stat_to_use, sig_stat_to_use = bkg_stat_r1, sig_stat_r1\n",
    "    xticks_dict, vals_dict = xticks_dict_r1, vals_dict_r1\n",
    "if Run == \"run3\": \n",
    "    hist_dict, bins_overflow, bins_cents = hist_dict_run3, bins_overflow_r3, bins_cents_overflow_r3\n",
    "    bkg_stat_to_use, sig_stat_to_use = bkg_stat_r3, sig_stat_r3\n",
    "    xticks_dict, vals_dict = xticks_dict_r3, vals_dict_r3\n",
    "\n",
    "PT.plot_bkg_total_unc_contributions(hist_dict, bkg_stat_to_use, bins_overflow, bins_cents, xticks_dict, vals_dict,\n",
    "                                    Params_pyhf, Unc_colors, Run=Run, name_type=name_type, plot_total=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8809f7b6-c9f8-4111-9f91-a82f29596882",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT.plot_signal_total_unc_contributions(hist_dict, sig_stat_to_use, bins_overflow, bins_cents, xticks_dict, vals_dict, \n",
    "                                       Params_pyhf,Unc_colors, Run=Run, name_type=name_type, \n",
    "                                       KDAR_unc=Params_pyhf[\"Signal_flux_error\"], plot_total=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd6b13-1435-4cdb-a37a-6ccddd5722be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scaling signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c1437b-4f00-499a-afb3-c0e2cc5bd296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_signal(Total_dict, theta_dict, scaling_dict={}):\n",
    "    \"\"\"\n",
    "    Scales the number of events by the number in the scaling dict.\n",
    "    Returns the new dict of histograms and the new thetas.\n",
    "    \"\"\"\n",
    "    if(scaling_dict=={}): raise Exception(\"Specify scalings\")\n",
    "    Total_dict_scaled, new_theta_dict = copy.deepcopy(Total_dict), {}\n",
    "    for HNL_mass in Total_dict.keys():\n",
    "        new_signal_hist = np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])*scaling_dict[HNL_mass]\n",
    "        new_signal_err_hist = np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR'])*scaling_dict[HNL_mass]\n",
    "        new_signal_stat_err = np.array(Total_dict[HNL_mass]['SIGNAL_STAT'])*scaling_dict[HNL_mass]\n",
    "        new_signal_shapesys = np.array(Total_dict[HNL_mass]['SIGNAL_SHAPESYS'])*scaling_dict[HNL_mass]\n",
    "        new_signal_detvar = np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR'])*scaling_dict[HNL_mass]\n",
    "        new_theta = theta_dict[HNL_mass]*scaling_dict[HNL_mass]**(1/4) # Number of events is proportional to theta**4\n",
    "        \n",
    "        Total_dict_scaled[HNL_mass]['TOT_SIGNAL_VALS'] = list(new_signal_hist)\n",
    "        Total_dict_scaled[HNL_mass]['TOT_SIGNAL_ERR'] = list(new_signal_err_hist)\n",
    "        Total_dict_scaled[HNL_mass]['SIGNAL_STAT'] = list(new_signal_stat_err)\n",
    "        Total_dict_scaled[HNL_mass]['SIGNAL_SHAPESYS'] = list(new_signal_shapesys)\n",
    "        Total_dict_scaled[HNL_mass]['SIGNAL_DETVAR'] = list(new_signal_detvar)\n",
    "        \n",
    "        new_theta_dict[HNL_mass] = new_theta\n",
    "        \n",
    "    return Total_dict_scaled, new_theta_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad4601-b177-49a6-93d0-a7e4387d6d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_SF = 2000\n",
    "if Params_pyhf[\"Load_lepton_hists\"]==True:\n",
    "    scaling_dict = {2:5000,10:2000,20:5e9,50:2e7,100:2e5,150:2e4} #Scaling for both r1 and r3 combined\n",
    "    scaling_dict_r1 = {2:5000,10:10000,20:2e9,50:5e6,100:1e6,150:5e4}\n",
    "elif Params_pyhf[\"Load_pi0_hists\"]==True:\n",
    "    # scaling_dict = {150:100,180:10,200:5,220:2,240:2,245:2}\n",
    "    scaling_dict = {150:500,180:50,200:25,220:10,240:10,245:10}\n",
    "elif Params_pyhf[\"Load_single_r1_file\"]==True: #Currently using this for pi0 Dirac samples\n",
    "    scaling_dict = {150:1000,180:50,200:50,220:102,240:20,245:20}\n",
    "scaled=True\n",
    "\n",
    "Total_dict, theta_dict_scaled  = scale_signal(Total_dict_both, theta_dict, scaling_dict)\n",
    "# Total_dict_run1_scaled, theta_dict_scaled_r1  = scale_signal(Total_dict_run1, theta_dict, scaling_dict_r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e1e7a-7f27-4d2a-b1e1-efde4c7f5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Total_dict[150].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae12345-a8e7-471b-926a-bea2bc7a2c21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plotting example Total hist after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9e985-2c90-457e-bde4-7b60889d0a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 150\n",
    "theta = theta_dict_scaled[HNL_mass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d6513-ed99-432d-90e7-79f05c62fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16\n",
    "plt.figure(figsize=(7,4),facecolor='white',dpi=100)\n",
    "\n",
    "num_bins = len(Total_dict[HNL_mass]['TOT_BKG_VALS'])\n",
    "\n",
    "bins = np.arange(0,num_bins+1,1)\n",
    "bins_cents=(bins[:-1]+bins[1:])/2\n",
    "plt.hist(bins_cents, weights=Total_dict[HNL_mass]['TOT_SIGNAL_VALS'], bins=bins, histtype=\"step\", lw=2, label=f\"Signal \\n\" + fr\"{HNL_mass}MeV $\\theta$={theta:.5f}\")\n",
    "plt.hist(bins_cents, weights=Total_dict[HNL_mass]['TOT_BKG_VALS'], bins=bins, histtype=\"step\",lw=2, label=\"Background\")\n",
    "\n",
    "bkg_up=np.append((Total_dict[HNL_mass]['TOT_BKG_VALS']+np.array(Total_dict[HNL_mass]['TOT_BKG_ERR'])),(Total_dict[HNL_mass]['TOT_BKG_VALS']+np.array(Total_dict[HNL_mass]['TOT_BKG_ERR']))[-1])\n",
    "bkg_down=np.append((Total_dict[HNL_mass]['TOT_BKG_VALS']-np.array(Total_dict[HNL_mass]['TOT_BKG_ERR'])),(Total_dict[HNL_mass]['TOT_BKG_VALS']-np.array(Total_dict[HNL_mass]['TOT_BKG_ERR']))[-1])\n",
    "sig_up=np.append((Total_dict[HNL_mass]['TOT_SIGNAL_VALS']+np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR'])),(Total_dict[HNL_mass]['TOT_SIGNAL_VALS']+np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR']))[-1])\n",
    "sig_down=np.append((Total_dict[HNL_mass]['TOT_SIGNAL_VALS']-np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR'])),(Total_dict[HNL_mass]['TOT_SIGNAL_VALS']-np.array(Total_dict[HNL_mass]['TOT_SIGNAL_ERR']))[-1])\n",
    "\n",
    "plt.fill_between(bins, bkg_down, bkg_up,step=\"post\",hatch='///',alpha=0,zorder=2)\n",
    "plt.fill_between(bins, sig_down, sig_up,step=\"post\",hatch='///',alpha=0,zorder=2)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"BDT score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ed0542-37d5-4eaf-a140-f97f5caa4fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['font.size'] = 16\n",
    "# plt.figure(figsize=(7,4),facecolor='white',dpi=100)\n",
    "# theta=theta_dict_scaled_r1[HNL_mass]\n",
    "\n",
    "# Run = \"run1\"\n",
    "\n",
    "# num_bins = len(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS'])\n",
    "\n",
    "# bins = np.arange(0,num_bins+1,1)\n",
    "# bins_cents=(bins[:-1]+bins[1:])/2\n",
    "# plt.hist(bins_cents, weights=Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_VALS'], bins=bins, histtype=\"step\", lw=2, label=f\"Signal \\n\" + fr\"{HNL_mass}MeV $\\theta$={theta:.5f}\")\n",
    "# plt.hist(bins_cents, weights=Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS'],bins=bins, histtype=\"step\",lw=2, label=\"Background\")\n",
    "\n",
    "# bkg_up=np.append((Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS']+np.array(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_ERR'])),(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS']+np.array(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_ERR']))[-1])\n",
    "# bkg_down=np.append((Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS']-np.array(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_ERR'])),(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS']-np.array(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_ERR']))[-1])\n",
    "# sig_up=np.append((Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_VALS']+np.array(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_ERR'])),(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_VALS']+np.array(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_ERR']))[-1])\n",
    "# sig_down=np.append((Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_VALS']-np.array(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_ERR'])),(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_VALS']-np.array(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_ERR']))[-1])\n",
    "\n",
    "# plt.fill_between(bins, bkg_down, bkg_up,step=\"post\",hatch='///',alpha=0,zorder=2)\n",
    "# plt.fill_between(bins, sig_down, sig_up,step=\"post\",hatch='///',alpha=0,zorder=2)\n",
    "\n",
    "# y_max = max(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS'])\n",
    "# y_min = min(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_VALS'])\n",
    "# # print(y_max)\n",
    "# # print(y_min)\n",
    "\n",
    "# plt.yscale(\"log\")\n",
    "# plt.ylim(y_min*0.5,y_max*3.0)\n",
    "# plt.legend(fontsize=13, loc=\"upper right\")\n",
    "# plt.xlabel(\"BDT score\")\n",
    "\n",
    "# savefig=False\n",
    "\n",
    "# if savefig == True:\n",
    "#     plt.savefig(f\"plots/CLs_plots/Example_signal_bkg_{Run}_{HNL_mass}MeV.pdf\")\n",
    "#     plt.savefig(f\"plots/CLs_plots/Example_signal_bkg_{Run}_{HNL_mass}MeV.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d44019-e751-4595-b5c9-6166719fa92f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Creating model (only do once happy with scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3737933-44c0-49a8-8753-76418efa4030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dict_separated(Total_dict, debug=False):\n",
    "    \"\"\"\n",
    "    Creating a model where the background samples are fed in individually (takes longer than summing into one total bkg hist).\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    sig_norm = {\"hi\": 1.0+Params_pyhf[\"Signal_flux_error\"], \"lo\": 1.0-Params_pyhf[\"Signal_flux_error\"]}\n",
    "    dirt_norm = {\"hi\": 1.0+Params_pyhf[\"Flat_bkg_dirt_frac\"], \"lo\": 1.0-Params_pyhf[\"Flat_bkg_dirt_frac\"]}\n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be scanned over in the hypo tests\n",
    "                {\"name\": \"stat_siguncrt\", \"type\": \"staterror\", \"data\": sig_stat[HNL_mass]},\n",
    "                {\"name\": \"norm_siguncrt\", \"type\": \"normsys\", \"data\": sig_norm}, #NuMI absorber KDAR rate\n",
    "                {\"name\": \"Detvar_sig\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['SIGNAL_DETVAR']} #shapesys assumes uncorrelated\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"overlay\",\n",
    "              \"data\": Total_dict[HNL_mass]['OVERLAY_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_overlay\", \"type\": \"staterror\", \"data\": overlay_stat[HNL_mass]},\n",
    "                {\"name\": \"Multisim_overlay\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_MULTISIM']},\n",
    "                {\"name\": \"Detvar_overlay\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_DETVAR']} #shapesys assumes uncorrelated\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"dirt\",\n",
    "              \"data\": Total_dict[HNL_mass]['DIRT_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_dirt\", \"type\": \"staterror\", \"data\": dirt_stat[HNL_mass]},\n",
    "                {\"name\": \"dirt_norm\", \"type\": \"normsys\", \"data\": dirt_norm}\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"beamoff\",\n",
    "              \"data\": Total_dict[HNL_mass]['BEAMOFF_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_beamoff\", \"type\": \"staterror\", \"data\": beamoff_stat[HNL_mass]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d595061-c00a-43c5-845f-16e9d176918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dict_same(Total_dict, debug=False):\n",
    "    \"\"\"\n",
    "    Creating a model where the uncertainties are all enveloped in one shapesys modifier for signal and bkg.\n",
    "    The total errors are taken from \\\"TOT_SIGNAL_ERR\\\" and \\\"TOT_BKG_ERR\\\" respectively.\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    \n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                {\"name\": \"uncorr_siguncrt\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass][\"TOT_SIGNAL_ERR\"]}\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"background\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_BKG_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass][\"TOT_BKG_ERR\"]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict\n",
    "\n",
    "def create_model_dict_split(Total_dict, debug=False): \n",
    "    \"\"\"\n",
    "    Creating a model where the uncertainties are split but have no correlations between sig and bkg.\n",
    "    Signal errors are shapesys taken from \\\"SIGNAL_SHAPESYS\\\", staterror taken from sig_stat dict,\n",
    "    and normsys taken from Params_pyhf[\\\"Signal_flux_error\\\"].\n",
    "    Bkg errors are shapesys taken from \\\"BKG_SHAPESYS\\\" and staterror taken from bkg_stat dict.\n",
    "    All bkg sys are enveloped in \\\"BKG_SHAPESYS\\\" here (dirt, detector and multisim).\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    sig_norm = {\"hi\": 1.0+Params_pyhf[\"Signal_flux_error\"], \"lo\": 1.0-Params_pyhf[\"Signal_flux_error\"]}\n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        \n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                {\"name\": \"uncorr_siguncrt\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['SIGNAL_SHAPESYS']},\n",
    "                {\"name\": \"stat_siguncrt\", \"type\": \"staterror\", \"data\": sig_stat[HNL_mass]},  \n",
    "                {\"name\": \"norm_siguncrt\", \"type\": \"normsys\", \"data\": sig_norm}  \n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"background\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_BKG_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_SHAPESYS']},\n",
    "                {\"name\": \"stat_bkguncrt\", \"type\": \"staterror\", \"data\": bkg_stat[HNL_mass]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict\n",
    "\n",
    "def create_model_correlated(Total_dict, debug=False):\n",
    "    \"\"\"\n",
    "    Creating a model where the uncertainties are split and detector variations are taken as fully correlated.\n",
    "    Signal errors are histosys taken from \\\"SIGNAL_DETVAR\\\", staterror taken from sig_stat dict,\n",
    "    and normsys taken from Params_pyhf[\\\"Signal_flux_error\\\"].\n",
    "    Bkg errors are histosys taken from \\\"BKG_DETVAR\\\", staterror taken from bkg_stat dict,\n",
    "    shapesys taken from \\'BKG_MULTISIM'\\, and histosys taken from \\'BKG_DIRT\\' .\n",
    "    Return pyhf model dictionary.\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    sig_norm = {\"hi\": 1.0+Params_pyhf[\"Signal_flux_error\"], \"lo\": 1.0-Params_pyhf[\"Signal_flux_error\"]}\n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        \n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          # \"name\": \"signal\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                {\"name\": \"Detvar\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])-np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR']), \n",
    "                 \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])+np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR'])}},\n",
    "                {\"name\": \"stat_siguncrt\", \"type\": \"staterror\", \"data\": sig_stat[HNL_mass]},  \n",
    "                {\"name\": \"norm_siguncrt\", \"type\": \"normsys\", \"data\": sig_norm}  \n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"background\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_BKG_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"Detvar\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])-np.array(Total_dict[HNL_mass]['BKG_DETVAR']), \n",
    "                 \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])+np.array(Total_dict[HNL_mass]['BKG_DETVAR'])}},\n",
    "                {\"name\": \"Multisim\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_MULTISIM']},\n",
    "                {\"name\": \"Dirt_err\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])-np.array(Total_dict[HNL_mass]['BKG_DIRT']),\n",
    "                  \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])+np.array(Total_dict[HNL_mass]['BKG_DIRT'])}},\n",
    "                {\"name\": \"stat_bkguncrt\", \"type\": \"staterror\", \"data\": bkg_stat[HNL_mass]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b621e-564e-4d5a-bd6d-2a0862a3433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_uncorrelated(Total_dict, debug=False):\n",
    "    \"\"\"\n",
    "    Creating a model where the uncertainties are split and detector variations are taken as fully UNcorrelated.\n",
    "    Signal errors are histosys taken from \\\"SIGNAL_DETVAR\\\", staterror taken from sig_stat dict,\n",
    "    and normsys taken from Params_pyhf[\\\"Signal_flux_error\\\"].\n",
    "    Bkg errors are shapesys taken from \\\"BKG_DETVAR_MULTISIM\\\", staterror taken from bkg_stat dict,\n",
    "    and histosys taken from \\'BKG_DIRT\\' .\n",
    "    Return pyhf model dictionary.\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    sig_norm = {\"hi\": 1.0+Params_pyhf[\"Signal_flux_error\"], \"lo\": 1.0-Params_pyhf[\"Signal_flux_error\"]}\n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        \n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          # \"name\": \"signal\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                # {\"name\": \"Detvar_sig\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])-np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR']), \n",
    "                #  \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])+np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR'])}},\n",
    "                {\"name\": \"Detvar_sig\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['SIGNAL_DETVAR']},\n",
    "                {\"name\": \"stat_siguncrt\", \"type\": \"staterror\", \"data\": sig_stat[HNL_mass]},  \n",
    "                {\"name\": \"norm_siguncrt\", \"type\": \"normsys\", \"data\": sig_norm}  \n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"background\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_BKG_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"Detvar_Multisim\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_DETVAR_MULTISIM']},\n",
    "                {\"name\": \"Dirt_err\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])-np.array(Total_dict[HNL_mass]['BKG_DIRT']),\n",
    "                  \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])+np.array(Total_dict[HNL_mass]['BKG_DIRT'])}},\n",
    "                {\"name\": \"stat_bkguncrt\", \"type\": \"staterror\", \"data\": bkg_stat[HNL_mass]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict\n",
    "\n",
    "def create_model_uncorrelated_dirt_runs_separated(Total_dict, debug=False):\n",
    "    \"\"\"\n",
    "    Creating a model where the uncertainties are split and detector variations are taken as fully UNcorrelated.\n",
    "    Signal errors are histosys taken from \\\"SIGNAL_DETVAR\\\", staterror taken from sig_stat dict,\n",
    "    and normsys taken from Params_pyhf[\\\"Signal_flux_error\\\"].\n",
    "    Bkg errors are shapesys taken from \\\"BKG_DETVAR_MULTISIM\\\", staterror taken from bkg_stat dict,\n",
    "    and histosys taken from \\'BKG_DIRT\\' .\n",
    "    Return pyhf model dictionary.\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    sig_norm = {\"hi\": 1.0+Params_pyhf[\"Signal_flux_error\"], \"lo\": 1.0-Params_pyhf[\"Signal_flux_error\"]}\n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        \n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          # \"name\": \"signal\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'], \n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                # {\"name\": \"Detvar_sig\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])-np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR']), \n",
    "                #  \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_SIGNAL_VALS'])+np.array(Total_dict[HNL_mass]['SIGNAL_DETVAR'])}},\n",
    "                {\"name\": \"Detvar_sig\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['SIGNAL_DETVAR']},\n",
    "                {\"name\": \"stat_siguncrt\", \"type\": \"staterror\", \"data\": sig_stat[HNL_mass]},  \n",
    "                {\"name\": \"norm_siguncrt\", \"type\": \"normsys\", \"data\": sig_norm}  \n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"background\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_BKG_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"Detvar_Multisim\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_DETVAR_MULTISIM']},\n",
    "                {\"name\": \"Dirt_err_r1\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])-np.array(Total_dict[HNL_mass]['BKG_DIRT_R1']),\n",
    "                  \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])+np.array(Total_dict[HNL_mass]['BKG_DIRT_R1'])}},\n",
    "                {\"name\": \"Dirt_err_r3\", \"type\": \"histosys\", \"data\": {\"lo_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])-np.array(Total_dict[HNL_mass]['BKG_DIRT_R3']),\n",
    "                  \"hi_data\": np.array(Total_dict[HNL_mass]['TOT_BKG_VALS'])+np.array(Total_dict[HNL_mass]['BKG_DIRT_R3'])}},\n",
    "                {\"name\": \"stat_bkguncrt\", \"type\": \"staterror\", \"data\": bkg_stat[HNL_mass]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447c26da-ff4f-430b-9b9f-cec8641caabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scaled==False:\n",
    "    print(\"Using unscaled hists\")\n",
    "    Total_dict, theta_dict_scaled = Total_dict_both, theta_dict\n",
    "\n",
    "# model_dict_both = create_model_dict_same(Total_dict)\n",
    "# model_dict_split = create_model_dict_split(Total_dict_both)\n",
    "\n",
    "# model_dict_both = create_model_correlated(Total_dict)\n",
    "model_dict_both = create_model_dict_separated(Total_dict) #All individual samples, all individual errors\n",
    "# model_dict_corr = create_model_correlated(Total_dict) \n",
    "# model_dict_uncorr = create_model_uncorrelated(Total_dict) \n",
    "\n",
    "# model_dict_run1 = create_model_dict_same(Total_dict_run1_scaled)\n",
    "# model_dict_run3 = create_model_dict(Total_dict_run3)\n",
    "print(\"Created models \\n\")\n",
    "\n",
    "print(f'Samples:\\n {model_dict_both[150].config.samples}')\n",
    "print(f'Modifiers are:\\n {model_dict_both[150].config.modifiers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de95dd-88b6-484c-988f-014528bb70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(theta_dict_scaled_r1)\n",
    "print(theta_dict_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fbf6f5-63dc-41f9-9d59-f66c8d067793",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Testing MLE etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8586e91-fcc2-4e89-bcf5-689f64d5bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 100\n",
    "\n",
    "model_test = model_dict_corr[HNL_mass]\n",
    "\n",
    "parameters = model_test.config.suggested_init()\n",
    "\n",
    "print(f\"aux data: {model_test.config.auxdata}\")\n",
    "print(f\"nominal: {model_test.expected_data(parameters)}\")\n",
    "print(f'Parameters:\\n {model_test.config.parameters}')\n",
    "print(f'Modifiers:\\n {model_test.config.modifiers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5402aba-8097-4dd1-a338-28585ebfa686",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = model_dict_uncorr[HNL_mass]\n",
    "\n",
    "parameters = model_test.config.suggested_init()\n",
    "\n",
    "print(f\"aux data: {model_test.config.auxdata}\")\n",
    "print(f\"nominal: {model_test.expected_data(parameters)}\")\n",
    "print(f'Parameters:\\n {model_test.config.parameters}')\n",
    "print(f'Modifiers:\\n {model_test.config.modifiers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1347b305-d044-4cba-a0e2-c128f0d735ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 10\n",
    "\n",
    "# hist_dict_test = Total_dict_run1_scaled\n",
    "hist_dict_test = Total_dict_both\n",
    "\n",
    "model_bkg = pyhf.Model(\n",
    "    {\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"name\": \"singlechannel\",\n",
    "                \"samples\": [\n",
    "                    {\n",
    "                        \"name\": \"background\",\n",
    "                        # \"name\": \"signal\",\n",
    "                        \"data\": hist_dict_test[HNL_mass]['TOT_BKG_VALS'],\n",
    "                        \"modifiers\": [\n",
    "                            {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": hist_dict_test[HNL_mass][\"TOT_BKG_ERR\"]}\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    poi_name=None,\n",
    ")\n",
    "\n",
    "up_error = np.array(hist_dict_test[HNL_mass]['TOT_BKG_VALS'])+np.array(hist_dict_test[HNL_mass][\"TOT_BKG_ERR\"]) \n",
    "down_error = np.array(hist_dict_test[HNL_mass]['TOT_BKG_VALS'])-np.array(hist_dict_test[HNL_mass][\"TOT_BKG_ERR\"])\n",
    "\n",
    "model_bkg_corr = pyhf.Model(\n",
    "    {\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"name\": \"singlechannel\",\n",
    "                \"samples\": [\n",
    "                    {\n",
    "                        \"name\": \"background\",\n",
    "                        # \"name\": \"signal\",\n",
    "                        \"data\": hist_dict_test[HNL_mass]['TOT_BKG_VALS'],\n",
    "                        \"modifiers\": [\n",
    "                            {\"name\": \"corr_bkguncrt\", \"type\": \"histosys\", \"data\": {\"hi_data\": list(up_error), \"lo_data\": list(down_error)}}\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    poi_name=None,\n",
    ")\n",
    "\n",
    "model_bkg_norm = pyhf.Model(\n",
    "    {\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"name\": \"singlechannel\",\n",
    "                \"samples\": [\n",
    "                    {\n",
    "                        \"name\": \"background\",\n",
    "                        \"data\": hist_dict_test[HNL_mass]['TOT_BKG_VALS'],\n",
    "                        \"modifiers\": [\n",
    "                            {\"name\": \"norm_bkguncrt\", \"type\": \"normsys\", \"data\": {\"hi\": 1.25, \"lo\": 0.75}}\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    poi_name=None,\n",
    ")\n",
    "\n",
    "#----\"shapesys\"----#\n",
    "data = hist_dict_test[HNL_mass][\"data\"]+model_bkg.config.auxdata\n",
    "parameters = model_bkg.config.suggested_init()\n",
    "#----\"histosys\"----#\n",
    "corr_data = hist_dict_test[HNL_mass][\"data\"]+model_bkg_corr.config.auxdata\n",
    "corr_data_edited = list(np.array(hist_dict_test[HNL_mass][\"data\"])*0.7)+model_bkg_corr.config.auxdata\n",
    "corr_parameters = model_bkg_corr.config.suggested_init()\n",
    "#----\"normsys\"----#\n",
    "norm_data = hist_dict_test[HNL_mass][\"data\"]+model_bkg_norm.config.auxdata\n",
    "norm_parameters = model_bkg_norm.config.suggested_init()\n",
    "\n",
    "# print(json.dumps(model_bkg.spec, indent=2))\n",
    "print(f\"aux data: {model_bkg.config.auxdata}\")\n",
    "print(f\"nominal: {model_bkg.expected_data(parameters)}\")\n",
    "print(f'Parameters:\\n {model_bkg.config.parameters}')\n",
    "print(f'Modifiers:\\n {model_bkg.config.modifiers}')\n",
    "\n",
    "best_fit, twice_nll = pyhf.infer.mle.fit(data, model_bkg,return_fitted_val=True)\n",
    "best_fit_corr, twice_nll_corr = pyhf.infer.mle.fit(corr_data, model_bkg_corr,return_fitted_val=True) \n",
    "# best_fit_corr_edited, twice_nll_corr_edited = pyhf.infer.mle.fit(corr_data_edited, model_bkg_corr,return_fitted_val=True)\n",
    "best_fit_norm, twice_nll_norm = pyhf.infer.mle.fit(norm_data, model_bkg_norm,return_fitted_val=True) \n",
    "\n",
    "print(\"best_fit:\")\n",
    "print(best_fit)\n",
    "print(\"Twice NLL:\")\n",
    "print(twice_nll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f869d2e-3671-41e6-8b5f-7ef24f3820ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----\"histosys\"----#\n",
    "print(\"----histosys----\")\n",
    "print(f\"aux data: {model_bkg_corr.config.auxdata}\")\n",
    "print(f\"nominal: {model_bkg_corr.expected_data(corr_parameters)}\")\n",
    "print(f'Parameters:\\n {model_bkg_corr.config.parameters}')\n",
    "print(f'Modifiers:\\n {model_bkg_corr.config.modifiers}')\n",
    "\n",
    "print(best_fit_corr)\n",
    "print(\"Twice NLL:\")\n",
    "print(twice_nll_corr)\n",
    "\n",
    "#----\"normsys\"----#\n",
    "print()\n",
    "print(\"----normsys----\")\n",
    "print(f\"aux data: {model_bkg_norm.config.auxdata}\")\n",
    "print(f\"nominal: {model_bkg_norm.expected_data(corr_parameters)}\")\n",
    "print(f'Parameters:\\n {model_bkg_norm.config.parameters}')\n",
    "print(f'Modifiers:\\n {model_bkg_norm.config.modifiers}')\n",
    "\n",
    "print(best_fit_norm)\n",
    "print(\"Twice NLL:\")\n",
    "print(twice_nll_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369fba94-78a0-4eee-8825-5706ce218198",
   "metadata": {},
   "outputs": [],
   "source": [
    "upvals = np.append(up_error, [0])\n",
    "downvals = np.append(down_error, [0])\n",
    "\n",
    "#----\"shapesys\"----#\n",
    "bkg_times_mle = np.multiply(hist_dict_test[HNL_mass]['TOT_BKG_VALS'],best_fit)\n",
    "\n",
    "plt.hist(Total_bins_cent_dict[HNL_mass], weights=hist_dict_test[HNL_mass]['TOT_BKG_VALS'], bins=Total_bins_overflow[HNL_mass], histtype=\"step\", lw=2, label=\"Bkg prediction\")\n",
    "plt.errorbar(Total_bins_cent_dict[HNL_mass], hist_dict_test[HNL_mass][\"data\"],fmt='.',color='black',lw=5,capsize=5,elinewidth=3,label=\"Data\")\n",
    "plt.hist(Total_bins_cent_dict[HNL_mass], weights=bkg_times_mle, bins=Total_bins_overflow[HNL_mass], histtype=\"step\", lw=2, label=\"Uncorrelated best fit\")\n",
    "#add errors\n",
    "plt.fill_between(Total_bins_overflow[HNL_mass], downvals, upvals, step=\"post\",color=\"grey\",alpha=0.2,zorder=2)\n",
    "\n",
    "#add NLLR\n",
    "# plt.text(0, 800, r\"$-2\\ln (\\lambda) = $\"+str(np.round(twice_nll,1)), fontsize=18)\n",
    "\n",
    "plt.xlabel('BDT score', fontsize=24)\n",
    "plt.ylabel('Events', fontsize=24)\n",
    "plt.legend(fontsize=16, loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "savefig = False\n",
    "\n",
    "if savefig == True:\n",
    "    plt.savefig(f\"plots/CLs_plots/MLE_best_fit_bkg_uncorrelated.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/MLE_best_fit_bkg_uncorrelated.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b1727b-620e-4d22-9d05-5ac30614ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----\"histosys\"----#\n",
    "corr_bkg_times_mle = np.multiply(hist_dict_test[HNL_mass][\"TOT_BKG_ERR\"],best_fit_corr)+hist_dict_test[HNL_mass]['TOT_BKG_VALS']\n",
    "\n",
    "plt.hist(Total_bins_cent_dict[HNL_mass], weights=hist_dict_test[HNL_mass]['TOT_BKG_VALS'], bins=Total_bins_overflow[HNL_mass], histtype=\"step\", lw=2, label=\"Bkg prediction\")\n",
    "plt.errorbar(Total_bins_cent_dict[HNL_mass],hist_dict_test[HNL_mass][\"data\"],fmt='.',color='black',lw=5,capsize=5,elinewidth=3,label=\"Data\")\n",
    "plt.hist(Total_bins_cent_dict[HNL_mass], weights=corr_bkg_times_mle, bins=Total_bins_overflow[HNL_mass], histtype=\"step\", lw=2, label=\"Correlated best fit\")\n",
    "#add errors\n",
    "plt.fill_between(Total_bins_overflow[HNL_mass], downvals, upvals, step=\"post\",color=\"grey\",alpha=0.2,zorder=2)\n",
    "\n",
    "# plt.text(0, 800, r\"$-2\\ln (\\lambda) = $\"+str(np.round(twice_nll_corr,1)), fontsize=18)\n",
    "\n",
    "plt.xlabel('BDT score', fontsize=24)\n",
    "plt.ylabel('Events', fontsize=24)\n",
    "plt.legend(fontsize=16, loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "savefig = False\n",
    "\n",
    "if savefig == True:\n",
    "    plt.savefig(f\"plots/CLs_plots/MLE_best_fit_bkg_correlated.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/MLE_best_fit_bkg_correlated.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cecc57-f9e6-4695-bd3e-9b04d1675ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bkg_times_mle)\n",
    "print(np.array(Total_dict_run1_scaled[HNL_mass][\"data\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146707b-b256-4d7c-9a50-8be73facc7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(model_dict_run1[HNL_mass].logpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4526ae65-1984-4fa5-82c3-513bde4038f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 10\n",
    "\n",
    "data = Total_dict_run1_scaled[HNL_mass][\"data\"]+model_dict_run1[HNL_mass].config.auxdata\n",
    "parameters = model_dict_run1[HNL_mass].config.suggested_init()\n",
    "\n",
    "best_fit, twice_nll = pyhf.infer.mle.fit(data, model_dict_run1[HNL_mass],return_fitted_val=True)\n",
    "twice_nll_fit = pyhf.infer.mle.twice_nll(parameters, data, model_dict_run1[HNL_mass])\n",
    "\n",
    "logpdf = model_dict_run1[HNL_mass].logpdf(best_fit, data)\n",
    "\n",
    "print(\"best_fit:\")\n",
    "print(best_fit)\n",
    "print(\"Twice NLL fit:\")\n",
    "print(twice_nll)\n",
    "print(twice_nll_fit)\n",
    "print(\"logpdf:\")\n",
    "print(logpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d4058-72cf-461d-8852-3a76db29dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_explore = model_dict_split[HNL_mass]\n",
    "\n",
    "print(f'Samples:\\n {model_to_explore.config.samples}')\n",
    "print(f'Parameters:\\n {model_to_explore.config.parameters}')\n",
    "print(f'Modifiers:\\n {model_to_explore.config.modifiers}')\n",
    "print()\n",
    "\n",
    "print(len(model_to_explore.config.auxdata))\n",
    "print(model_to_explore.config.auxdata)\n",
    "print(model_to_explore.config.poi_index)  \n",
    "print(model_to_explore.config.suggested_bounds()) \n",
    "print(model_to_explore.config.channel_nbins)\n",
    "\n",
    "# print(json.dumps(model_to_explore.spec, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e944bdc-62ce-42bf-930a-5184e6c6c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLs_obs, p_values = pyhf.infer.hypotest(\n",
    "    test_mu, data, model, test_stat=\"qtilde\", return_tail_probs=True\n",
    ")\n",
    "print(f\"Observed CL_s: {CLs_obs}, CL_sb: {p_values[0]}, CL_b: {p_values[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68fc82f-8497-4465-88d7-ea486bdb2f3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Making test statistic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf8e66d-3772-4e69-9b66-6b645656e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the model with signal and bkg\n",
    "HNL_mass = 10\n",
    "\n",
    "model_full_uncorr = pyhf.Model(\n",
    "    {\n",
    "  \"channels\": [\n",
    "    {\n",
    "      \"name\": \"singlechannel\",\n",
    "      \"samples\": [\n",
    "        {\n",
    "          \"name\": \"signal\",\n",
    "          \"data\": Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "          \"modifiers\": [\n",
    "            {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "            {\"name\": \"uncorr_siguncrt\", \"type\": \"shapesys\", \"data\": Total_dict_run1_scaled[HNL_mass][\"TOT_SIGNAL_ERR\"]} #Quadsum of ALL uncertainties\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"background\",\n",
    "          \"data\": Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS'],\n",
    "          \"modifiers\": [\n",
    "            {\"name\": \"uncorr_bkguncrt\", \"type\": \"shapesys\", \"data\": Total_dict_run1_scaled[HNL_mass][\"TOT_BKG_ERR\"]} #Quadsum of ALL uncertainties\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    ")\n",
    "\n",
    "sig_up = np.array(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_VALS'])+np.array(Total_dict_run1_scaled[HNL_mass][\"TOT_SIGNAL_ERR\"])\n",
    "sig_down = np.array(Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_VALS'])-np.array(Total_dict_run1_scaled[HNL_mass][\"TOT_SIGNAL_ERR\"])\n",
    "bkg_up = np.array(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS'])+np.array(Total_dict_run1_scaled[HNL_mass][\"TOT_BKG_ERR\"]) \n",
    "bkg_down = np.array(Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS'])-np.array(Total_dict_run1_scaled[HNL_mass][\"TOT_BKG_ERR\"])\n",
    "\n",
    "model_full_corr = pyhf.Model(\n",
    "    {\n",
    "        \"channels\": [\n",
    "            {\n",
    "                \"name\": \"singlechannel\",\n",
    "                \"samples\": [\n",
    "                {\n",
    "                  \"name\": \"signal\",\n",
    "                  \"data\": Total_dict_run1_scaled[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "                  \"modifiers\": [\n",
    "                    {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None}, #This is the scaling which is to be calculated\n",
    "                    {\"name\": \"corr_siguncrt\", \"type\": \"histosys\", \"data\": {\"hi_data\": list(sig_up), \"lo_data\": list(sig_down)}} \n",
    "                  ]\n",
    "                    },\n",
    "                    {\n",
    "                    \"name\": \"background\",\n",
    "                    \"data\": Total_dict_run1_scaled[HNL_mass]['TOT_BKG_VALS'],\n",
    "                    \"modifiers\": [\n",
    "                        {\"name\": \"corr_bkguncrt\", \"type\": \"histosys\", \"data\": {\"hi_data\": list(bkg_up), \"lo_data\": list(bkg_down)}} #This is dodgy as includes stat.\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9ff10-074e-47fa-b9b8-e1b706a8c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 10 #Mass point to test\n",
    "\n",
    "obs_scaling = 0.2913938580237639\n",
    "exp_scaling = 0.3709444625002695\n",
    "\n",
    "Use_fully_uncorrelated = True\n",
    "\n",
    "if Use_fully_uncorrelated == True: model_test = model_full_uncorr\n",
    "if Use_fully_uncorrelated == False: model_test = model_full_corr\n",
    "\n",
    "DATA_OBS_dict = {}\n",
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "init_pars = model_test.config.suggested_init()\n",
    "# model_test.expected_actualdata(init_pars) #signal plus bkg expected data\n",
    "\n",
    "bkg_pars = init_pars.copy()\n",
    "# bkg_pars = model_full_uncorr_no_signal.config.suggested_init()\n",
    "bkg_pars[model_test.config.poi_index] = 0 #mu must be zero for the bkg-only model\n",
    "\n",
    "# for i in range(11,21): #Setting siguncrt to 0\n",
    "#     bkg_pars[i] = 0.0\n",
    "    \n",
    "# model_test.expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "print(list(zip(model_test.config.parameters, init_pars)))\n",
    "print(list(zip(model_test.config.parameters, bkg_pars)))\n",
    "\n",
    "pdf_sig = model_test.make_pdf(pyhf.tensorlib.astensor(init_pars)) #Making the pdfs\n",
    "pdf_bkg = model_test.make_pdf(pyhf.tensorlib.astensor(bkg_pars)) #Making the pdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56def80-4891-466e-b064-9a4814d51050",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bkg_pars)\n",
    "# print(model_test.config.par_order)\n",
    "\n",
    "# print(model_test.config.param_set('uncorr_siguncrt').n_parameters)\n",
    "\n",
    "# print(model_test.config.suggested_init())\n",
    "# print(model_test.config.suggested_fixed())\n",
    "# bkg_pars[model_test.config.poi_index] = 0\n",
    "# for i in range(11,21):\n",
    "#     bkg_pars[i] = 0.0\n",
    "print(\"Expected data given init pars\")\n",
    "print(model_test.expected_data(init_pars)) #should this be bkg_pars?\n",
    "print(\"Expected data given bkg pars\")\n",
    "print(model_test.expected_data(bkg_pars)) #should this be bkg_pars?\n",
    "\n",
    "print(Total_dict_run1_scaled[HNL_mass]['data']+model_test.config.auxdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a12a35-c8b3-422d-b8a6-efd8ce89fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "\n",
    "# mu' = 0\n",
    "mc_bkg = pdf_bkg.sample((n_samples,))\n",
    "# mu' = 1\n",
    "mc_sig = pdf_sig.sample((n_samples,))\n",
    "\n",
    "print(mc_bkg.shape)\n",
    "print(mc_sig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e5770-2881-4cda-856d-0159004cf7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_calculator_qtilde_data = pyhf.infer.utils.create_calculator( #only seems to support q-like test statistics\n",
    "    \"toybased\",\n",
    "    # model_test.expected_data(init_pars),\n",
    "    Total_dict_run1_scaled[HNL_mass]['data']+model_test.config.auxdata,\n",
    "    model_test,\n",
    "    ntoys=n_samples,\n",
    "    test_stat=\"qtilde\",\n",
    ")\n",
    "toy_calculator_qtilde_exp = pyhf.infer.utils.create_calculator( #only seems to support q-like test statistics\n",
    "    \"toybased\",\n",
    "    model_test.expected_data(bkg_pars),\n",
    "    # Total_dict_run1_scaled[HNL_mass]['data']+model_test.config.auxdata,\n",
    "    model_test,\n",
    "    ntoys=n_samples,\n",
    "    test_stat=\"qtilde\",\n",
    ")\n",
    "\n",
    "print(\"mu = 1 distributions\")\n",
    "# qtilde_sig, qtilde_bkg = toy_calculator_qtilde.distributions(1.0) #1.0 for full signal contribution, \"null\" defaults to mu=1 in pyhf\n",
    "qtilde_sig_obs, qtilde_bkg_obs = toy_calculator_qtilde_data.distributions(obs_scaling) \n",
    "qtilde_sig_exp, qtilde_bkg_exp = toy_calculator_qtilde_exp.distributions(exp_scaling) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e6560-dc8b-49ab-be8e-43db22fe4ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#small p-value means outcome v unlikely under null hypothesis\n",
    "\n",
    "CLsb, CLb, CLs = toy_calculator_qtilde_exp.expected_pvalues(qtilde_sig_exp, qtilde_bkg_exp)\n",
    "print(\"Expected\")\n",
    "print(\"CLsb \" + str(CLsb[2]))\n",
    "print(\"CLb \" + str(CLb[2]))\n",
    "print(\"CLs \" + str(CLs[2]))\n",
    "print(CLsb[2]/CLb[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bde7b2-0630-4645-9920-bcb691d6ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tilde_obs = toy_calculator_qtilde_data.teststatistic(obs_scaling)\n",
    "\n",
    "CLsb_obs, CLb_obs, CLs_obs = toy_calculator_qtilde_data.pvalues(q_tilde_obs, qtilde_sig_obs, qtilde_bkg_obs)\n",
    "print(\"Observed\")\n",
    "print(\"qtilde sig \" + str(q_tilde_obs))\n",
    "print(\"CLsb \" + str(CLsb_obs))\n",
    "print(\"CLb \" + str(CLb_obs))\n",
    "print(\"CLs \" + str(CLs_obs))\n",
    "\n",
    "print(CLsb_obs/CLb_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f44891d-bcc3-4bed-a23e-8ae07cafe51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Total_dict_run1_scaled[HNL_mass][\"data\"]+model_test.config.auxdata\n",
    "data_exp = model_test.expected_data(bkg_pars)\n",
    "par_bounds = model_test.config.suggested_bounds()\n",
    "fixed_params = model_test.config.suggested_fixed()\n",
    "\n",
    "qmu_tilde_obs = pyhf.infer.test_statistics.qmu_tilde(obs_scaling, data, model_test, init_pars, par_bounds, fixed_params, return_fitted_pars=False)\n",
    "qmu_tilde_exp = pyhf.infer.test_statistics.qmu_tilde(exp_scaling, data_exp, model_test, init_pars, par_bounds, fixed_params, return_fitted_pars=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b565ae07-5324-41e2-a60e-4b27f342e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(qmu_tilde_obs)\n",
    "print(q_tilde_obs)\n",
    "print(qmu_tilde_exp)\n",
    "\n",
    "\n",
    "def Get_proportion(dist, test_stat_val):\n",
    "    \"\"\"\n",
    "    Given a pyhf sample \"dist\" and a given value, \n",
    "    returns the proportion of the total dist up to that point.\n",
    "    \"\"\"\n",
    "    if max(dist) < test_stat_val:\n",
    "        print(\"Value is above max in dist\")\n",
    "        return 0\n",
    "    ordered_test_stat = np.sort(dist) #Test stat vals going from lowest to highest\n",
    "    itemindex = np.where(ordered_test_stat >= test_stat_val)\n",
    "    prop = itemindex[0][0]/len(dist)\n",
    "    \n",
    "    return prop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca77ec-ced8-4126-96d8-ed97303d2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val_b = Get_proportion(qtilde_bkg_obs.samples, qmu_tilde_obs) #Integral of b-only dist where test stat < observed val\n",
    "p_val_sb = 1-Get_proportion(qtilde_sig_obs.samples, qmu_tilde_obs) #Integral of s+b dist where test stat > observed val\n",
    "print(\"Observed\")\n",
    "print(\"p_b: \" + str(p_val_b))\n",
    "print(\"p_sb: \" + str(p_val_sb))\n",
    "\n",
    "print(p_val_sb/(1-p_val_b))\n",
    "\n",
    "p_val_b_exp = Get_proportion(qtilde_bkg_exp.samples, qmu_tilde_exp) \n",
    "p_val_sb_exp = 1-Get_proportion(qtilde_sig_exp.samples, qmu_tilde_exp)\n",
    "\n",
    "print()\n",
    "print(\"Expected\")\n",
    "print(\"p_b: \" + str(p_val_b_exp))\n",
    "print(\"p_sb: \" + str(p_val_sb_exp))\n",
    "\n",
    "print(p_val_sb_exp/(1-p_val_b_exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad355c4-c5e1-4840-8f8c-61ca0b68ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max qtilde signal value: \" + str(max(qtilde_sig_obs.samples)))\n",
    "print(\"Max qtilde background value: \" + str(max(qtilde_bkg_obs.samples)))\n",
    "\n",
    "#Finding probability for one value of test statistic\n",
    "ordered_test_stat = np.sort(qtilde_sig_obs.samples)\n",
    "ordered_bkg = np.sort(qtilde_bkg_obs.samples)\n",
    "length =len(ordered_test_stat)\n",
    "\n",
    "prob = 0.9\n",
    "slice_at = 0.9*length\n",
    "print(\"-----signal-----\")\n",
    "print(np.floor(slice_at))\n",
    "print(ordered_test_stat[int(slice_at)])\n",
    "value_for_prob = ordered_test_stat[int(slice_at)]\n",
    "\n",
    "print(\"-----bkg-----\")\n",
    "print(np.floor(slice_at))\n",
    "print(ordered_bkg[int(slice_at)])\n",
    "bkg_expected_qtilde = ordered_bkg[int(0.5*length)]\n",
    "print(bkg_expected_qtilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc300d6a-fdab-47dd-baee-dcd9f0b19349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unedited one cell below\n",
    "savefig=False\n",
    "\n",
    "# x_max = max(qtilde_sig.samples)\n",
    "x_max = max(qtilde_bkg_obs.samples)\n",
    "# x_max=2e-8\n",
    "print(x_max)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "nbins = 30 #Should make an array for the bin edges\n",
    "bins = np.linspace(0,x_max*1.2,nbins) #was 0, 45 before\n",
    "\n",
    "hist, edges = np.histogram(qtilde_sig_obs.samples, bins=bins, density=True)\n",
    "edges = np.repeat(edges, 2)\n",
    "hist = np.hstack((0, np.repeat(hist, 2), 0))\n",
    "\n",
    "hist_bkg, edges_bkg = np.histogram(qtilde_bkg_obs.samples, bins=bins, density=True)\n",
    "edges_bkg = np.repeat(edges_bkg, 2)\n",
    "hist_bkg = np.hstack((0, np.repeat(hist_bkg, 2), 0))\n",
    "\n",
    "hatch_from= qmu_tilde_obs\n",
    "hatch_from_bkg= 0.0\n",
    "print(qmu_tilde_obs)\n",
    "hatch_till= 15\n",
    "hatch_till_bkg= qmu_tilde_obs\n",
    "\n",
    "fill_region = (hatch_from<edges)&(edges<hatch_till)\n",
    "\n",
    "fill_region_bkg = (hatch_from_bkg<edges_bkg)&(edges_bkg<hatch_till_bkg)\n",
    "test_fill_bkg = hist_bkg[fill_region_bkg].copy()\n",
    "# test_fill_bkg[-1] = 0.0\n",
    "new_hist=np.append(test_fill_bkg, test_fill_bkg[-1])\n",
    "new_hist=np.append(new_hist, 0.0)\n",
    "\n",
    "test_fill_edges = edges_bkg[fill_region_bkg].copy()\n",
    "new_edges=np.append(test_fill_edges, qmu_tilde_obs)\n",
    "new_edges=np.append(new_edges, qmu_tilde_obs)\n",
    "\n",
    "\n",
    "plt.hist(\n",
    "    qtilde_sig_obs.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_1|1)$ signal-like\",\n",
    "    color=\"#D62728\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.hist(\n",
    "    qtilde_bkg_obs.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_1|0)$ background-like\",\n",
    "    color=\"#1F77B4\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "plt.fill_betweenx(hist[fill_region], \n",
    "                  edges[fill_region], qmu_tilde_obs,\n",
    "                  color='none', edgecolor='#D62728',\n",
    "                  hatch='xxx', label=r\"$p_{s+b}$\")\n",
    "\n",
    "plt.fill_betweenx(new_hist, \n",
    "                  new_edges, 0,\n",
    "                  color='none', edgecolor='#1F77B4',\n",
    "                  hatch='xxx', label=r\"$p_{b}$\")\n",
    "\n",
    "if Use_fully_uncorrelated == True: model_name = \"fully_uncorrelated\"\n",
    "if Use_fully_uncorrelated == False: model_name = \"correlated\"\n",
    "\n",
    "plt.axvline(x = qmu_tilde_obs, color = 'black', label = r'Data',linestyle=\"dashed\", lw=3)\n",
    "# plt.axvline(x = bkg_expected_qtilde, color = 'gray', label = 'Background expectation',linestyle=\"dashed\", lw=3)\n",
    "\n",
    "# plt.plot(x_plot, test_f_q, color=\"black\")\n",
    "\n",
    "plt.xlabel(r\"$\\tilde{q}_1$\", fontsize=22)\n",
    "plt.ylabel(r\"$f\\,(\\tilde{q}_1|\\mu')$\", fontsize=22)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(3e-3, 2.0)\n",
    "plt.xlim(0,15)\n",
    "plt.legend(fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "if savefig==True:\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_qtilde_obs_asymp_{model_name}_{HNL_mass}MeV.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_qtilde_obs_asymp_{model_name}_{HNL_mass}MeV.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195623a-0f61-454a-a7ef-88bd9e3a0574",
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig=False\n",
    "\n",
    "# x_max = max(qtilde_sig.samples)\n",
    "x_max = max(qtilde_bkg_obs.samples)\n",
    "# x_max=2e-8\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "nbins = 30 #Should make an array for the bin edges\n",
    "bins = np.linspace(0,x_max*1.2,nbins) #was 0, 45 before\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(\n",
    "    qtilde_sig_obs.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_1|1)$ signal-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.hist(\n",
    "    qtilde_bkg_obs.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_1|0)$ background-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "# value_for_prob = 2.16\n",
    "if Use_fully_uncorrelated == True: model_name = \"fully_uncorrelated\"\n",
    "if Use_fully_uncorrelated == False: model_name = \"correlated\"\n",
    "\n",
    "# plt.axvline(x = value_for_prob, color = 'red', label = '90% signal-like',linestyle=\"dashed\", lw=3)\n",
    "plt.axvline(x = qmu_tilde_obs, color = 'black', label = 'Data',linestyle=\"dashed\", lw=3)\n",
    "plt.axvline(x = bkg_expected_qtilde, color = 'gray', label = 'Background expectation',linestyle=\"dashed\", lw=3)\n",
    "\n",
    "plt.xlabel(r\"$\\tilde{q}_1$\", fontsize=22)\n",
    "plt.ylabel(r\"$f\\,(\\tilde{q}_1|\\mu')$\", fontsize=22)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(3e-3, 2.0)\n",
    "plt.legend(fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "if savefig==True:\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_qtilde_obs_{model_name}_{HNL_mass}MeV.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_qtilde_obs_{model_name}_{HNL_mass}MeV.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e9cec-4452-4fc7-84ec-e288239a39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All just zeros, I don't fully understand this way. \n",
    "savefig=False\n",
    "\n",
    "# x_max = max(qtilde_sig.samples)\n",
    "x_max = max(qtilde_sig_q0.samples)\n",
    "# x_max=2e-8\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "nbins = 30 #Should make an array for the bin edges\n",
    "bins = np.linspace(0,x_max*1.2,nbins) #was 0, 45 before\n",
    "plt.hist(\n",
    "    qtilde_sig_q0.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_0|1)$ signal-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.hist(\n",
    "    qtilde_bkg_q0.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=r\"$f(\\tilde{q}_0|0)$ background-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "# value_for_prob = 2.16\n",
    "if Use_fully_uncorrelated == True: model_name = \"fully_uncorrelated\"\n",
    "if Use_fully_uncorrelated == False: model_name = \"correlated\"\n",
    "\n",
    "# plt.axvline(x = value_for_prob, color = 'red', label = '90% signal-like',linestyle=\"dashed\", lw=3)\n",
    "\n",
    "plt.xlabel(r\"$\\tilde{q}_0$\", fontsize=22)\n",
    "plt.ylabel(r\"$f\\,(\\tilde{q}_0|\\mu')$\", fontsize=22)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=22)\n",
    "plt.tight_layout()\n",
    "\n",
    "if savefig==True:\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_qtilde_0_{model_name}_{HNL_mass}MeV.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_qtilde_0_{model_name}_{HNL_mass}MeV.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97268c1e-74e7-4c6d-a8c5-ffa08eb621c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmu_bounds = model_test.config.suggested_bounds()\n",
    "print(f\"Old bounds: {qmu_bounds}\")\n",
    "# qmu_bounds[model_dict[HNL_mass].config.poi_index] = (-10, 10) #Made these larger and didn't get minimization error in followin cell\n",
    "qmu_bounds[model_test.config.poi_index] = (-50, 50)\n",
    "print(f\"New bounds: {qmu_bounds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1247f-80ae-490f-9799-98bf2f5fda18",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_calculator_qmu = pyhf.infer.utils.create_calculator(\n",
    "    \"toybased\",\n",
    "    model_test.expected_data(model_test.config.suggested_init()),\n",
    "    model_test,\n",
    "    par_bounds=qmu_bounds,\n",
    "    ntoys=n_samples,\n",
    "    test_stat=\"q\",\n",
    ")\n",
    "qmu_sig, qmu_bkg = toy_calculator_qmu.distributions(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cd24f-3977-4835-8406-7b20bad19c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(qmu_sig.samples))\n",
    "print(max(qmu_bkg.samples))\n",
    "print(min(qmu_sig.samples))\n",
    "print(min(qmu_bkg.samples))\n",
    "\n",
    "ordered_test_stat = np.sort(qmu_sig.samples)\n",
    "length =len(ordered_test_stat)\n",
    "\n",
    "prob = 0.9\n",
    "slice_at = 0.9*length\n",
    "print(np.floor(slice_at))\n",
    "print(ordered_test_stat[int(slice_at)])\n",
    "value_for_prob = ordered_test_stat[int(slice_at)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bd896-544c-42f1-9d23-80c783ef6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig=True\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "nbins = 30 #Should make an array for the bin edges\n",
    "x_max = max(qmu_bkg.samples)\n",
    "x_min = min(qmu_sig.samples)\n",
    "bins = np.linspace(0,x_max*1.2,nbins)\n",
    "plt.hist(\n",
    "    qmu_sig.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=\"$f(q_1|1)$ signal-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.hist(\n",
    "    qmu_bkg.samples,\n",
    "    bins=bins,\n",
    "    histtype=\"step\",\n",
    "    density=True,\n",
    "    label=\"$f(q_1|0)$ background-like\",\n",
    "    linewidth=2,\n",
    ")\n",
    "\n",
    "plt.axvline(x = value_for_prob, color = 'red', label = '90% signal-like',linestyle=\"dashed\", lw=3)\n",
    "\n",
    "plt.xlabel(r\"$q_1$\", fontsize=18)\n",
    "plt.ylabel(r\"$f\\,(q_1|\\mu')$\", fontsize=18)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=22)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "if savefig==True:\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_q_{model_name}_{HNL_mass}MeV.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/Test_stat_q_{model_name}_{HNL_mass}MeV.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1ef3c-b55b-4830-8f63-436da533686c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Testing single point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90186d-96f5-48bb-88a5-456477ad403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_uncorr = {}\n",
    "model_dict_uncorr[10] = model_full_uncorr\n",
    "# model_dict_uncorr[10] = model_dict_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c533c-cc65-4f3c-8be0-dee7b236807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 150 #The mass point to test\n",
    "n_sigmas = 2\n",
    "if n_sigmas==1: sigma_list = np.arange(-1, 2)\n",
    "if n_sigmas==2: sigma_list = np.arange(-2, 3)\n",
    "\n",
    "# model_dict_to_use = model_dict_both\n",
    "# model_dict_to_use = model_dict_run1 #model_full_uncorr\n",
    "\n",
    "# model_dict_to_use = model_dict_uncorr\n",
    "model_dict_to_use = model_dict_both\n",
    "# Total_dict_to_use = Total_dict_run1_scaled #Total_dict_both, Total_dict_run1, Total_dict_run3\n",
    "Total_dict_to_use = Total_dict_both\n",
    "\n",
    "DATA_OBS_dict = {}\n",
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "init_pars = model_dict_to_use[HNL_mass].config.suggested_init()\n",
    "model_dict_to_use[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "bkg_pars = init_pars.copy()\n",
    "bkg_pars[model_dict_to_use[HNL_mass].config.poi_index] = 0\n",
    "model_dict_to_use[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "if 'data' not in Total_dict_to_use[HNL_mass].keys(): \n",
    "    print(\"No data in this sample, using bkg as data\")\n",
    "    DATA_OBS_dict[HNL_mass] = Total_dict_to_use[HNL_mass]['TOT_BKG_VALS']+model_dict_to_use[HNL_mass].config.auxdata\n",
    "else: \n",
    "    print(\"Using real data for observed limit\")\n",
    "    DATA_OBS_dict[HNL_mass] = Total_dict_to_use[HNL_mass][\"data\"]+model_dict_to_use[HNL_mass].config.auxdata    \n",
    "\n",
    "model_dict_to_use[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass]) #Is this only for the test stat distribution stuff?\n",
    "    \n",
    "poi_values = np.linspace(0.001, 10, 500) #I could make a dict of this and have different pois for different mass points\n",
    "print(\"Max value is \" + str(max(poi_values)))\n",
    "print(\"Min value is \" + str(min(poi_values)))\n",
    "print(\"Next value is \" + str(poi_values[1]))\n",
    "print(\"Next value is \" + str(poi_values[2]) + \"\\n\")\n",
    "\n",
    "obs_limit_single, exp_limits_single, (scan, results) = pyhf.infer.intervals.upper_limits.upper_limit(DATA_OBS_dict[HNL_mass], \n",
    "                                                                                       model_dict_to_use[HNL_mass], poi_values, \n",
    "                                                                                       level=0.1, return_results=True)\n",
    "print(f\"Upper limit {HNL_mass}MeV (obs):  = {obs_limit_single:.4f}\")\n",
    "print(f\"Upper limit {HNL_mass}MeV (exp):  = {exp_limits_single[2]:.4f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a89e3b-4db5-4353-aa14-3d9316a52c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Params_pyhf[\"Use_toys\"] == True:\n",
    "    CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            # 1.0,  # null hypothesis mu\n",
    "            obs_limit_single, #mu\n",
    "            # exp_limits_single[2], #mu\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict_to_use[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"toybased\",\n",
    "            # ntoys=Params_pyhf[\"Num_toys\"],\n",
    "            ntoys=2000,\n",
    "            # track_progress=True, \n",
    "            )\n",
    "for expected_value, n_sigma in zip(CLs_exp, sigma_list): #-2, 3\n",
    "    print(f\"Expected CLs({n_sigma:2d} ): {expected_value:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57adbf-3935-4075-bfb7-079fbafda89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CLs_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30a6bc-7a6c-47b4-89df-7688667c8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = []\n",
    "obs_limit = []\n",
    "exp_1sig_up = [] #entry 3\n",
    "exp_1sig_down = [] #entry 1\n",
    "exp_2sig_up = [] #entry 4\n",
    "exp_2sig_down = [] #entry 0\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in [HNL_mass]:\n",
    "    theta_squared = (theta_dict_scaled[HNL_mass])**2\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_single[2])*theta_squared\n",
    "    LIMIT = np.sqrt(obs_limit_single)*theta_squared\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_single[3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_single[4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_single[1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_single[0])*theta_squared)\n",
    "    obs_limit.append(LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39599ac7-a757-4b36-a2e6-ca778c1e377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for HNL_mass in [HNL_mass]:\n",
    "    theta_squared = (theta_dict_scaled[HNL_mass])**2\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_single[2])*theta_squared\n",
    "    LIMIT = np.sqrt(obs_limit_single)*theta_squared\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_single[3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_single[4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_single[1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_single[0])*theta_squared)\n",
    "    obs_limit.append(LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910cc934-a481-4719-a82d-087aa712e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs_limit)\n",
    "print(\"Expected limit\")\n",
    "print(exp_limit)\n",
    "print(\"1 sigma up\")\n",
    "print(exp_1sig_up)\n",
    "print(\"2 sigma up\")\n",
    "print(exp_2sig_up)\n",
    "print(\"1 sigma down\")\n",
    "print(exp_1sig_down)\n",
    "print(\"2 sigma down\")\n",
    "print(exp_2sig_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a459eea-84c1-4045-90aa-e1af4d11c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_limits_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d29e6-f7f4-4088-b9c5-0ccc2fd1f419",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Running through all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ace42d-de08-48dd-8c1a-ed51a6f79b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dict = model_dict_split #Uncertainties are split into different modifiers, takes several times longer to run.\n",
    "# model_dict = model_dict_both #Quicker, uncertainties are entered as one uncertainty which is the quadsum of components.\n",
    "# Total_dict = Total_dict_both\n",
    "\n",
    "# model_dict = model_dict_run1\n",
    "# Total_dict = Total_dict_run1_scaled\n",
    "\n",
    "model_dict = model_dict_both\n",
    "Total_dict = Total_dict_both\n",
    "\n",
    "if Params_pyhf[\"Load_pi0_hists\"]==True:HNL_masses=Constants.HNL_mass_pi0_samples\n",
    "elif Params_pyhf[\"Load_lepton_hists\"]==True:HNL_masses=Constants.HNL_mass_samples\n",
    "elif Params_pyhf[\"Load_single_r1_file\"]==True: HNL_masses = HNL_masses_list\n",
    "    \n",
    "# HNL_masses = HNL_masses_list\n",
    "\n",
    "# list_test = Constants.HNL_mass_samples\n",
    "print(\"Running hypothesis tests for these masses: \" + str(HNL_masses))\n",
    "\n",
    "DATA_OBS_dict = {}\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "    model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "    bkg_pars = init_pars.copy()\n",
    "    bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "    model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "    list_keys = list(Total_dict[HNL_mass].keys())\n",
    "    if \"data\" in list_keys: #haven't made this yet, need to test\n",
    "        DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass][\"data\"]+model_dict[HNL_mass].config.auxdata\n",
    "    else:\n",
    "        DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass]['TOT_BKG_VALS']+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "    model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727ad3d-56ab-42bf-ae92-e24e5b50f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Should take ~10 mins for each with 5 bins, 100 mu values\n",
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "print(\"If the output of an upper limit calculation is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "poi_values = np.linspace(0.001, 5, 100) #Values of mu which are scanned over in hypo tests\n",
    "print(\"Max value is \" + str(max(poi_values)))\n",
    "print(\"Min value is \" + str(min(poi_values)))\n",
    "print(\"Next value is \" + str(poi_values[1]))\n",
    "print(\"Next value is \" + str(poi_values[2]) + \"\\n\")\n",
    "\n",
    "print(\"-----Starting Hypothesis tests-----\" + \"\\n\")\n",
    "for HNL_mass in HNL_masses:\n",
    "\n",
    "    # poi_values = np.linspace(0.001, 4, 100) pyhf.infer.intervals.upper_limits.upper_limit\n",
    "    obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upper_limits.upper_limit(\n",
    "        DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_values, level=0.1, return_results=True\n",
    "    )\n",
    "\n",
    "    print(f\"Upper limit {HNL_mass}MeV (obs):  = {obs_limit_dict[HNL_mass]:.6f}\")\n",
    "    print(f\"Upper limit {HNL_mass}MeV (exp):  = {exp_limits_dict[HNL_mass][2]:.6f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0dcf7f-0665-430c-8239-66452b8bf24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = [] #entry 2\n",
    "exp_1sig_up = [] #entry 3\n",
    "exp_1sig_down = [] #entry 1\n",
    "exp_2sig_up = [] #entry 4\n",
    "exp_2sig_down = [] #entry 0\n",
    "obs_limit = []\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in exp_limits_dict:\n",
    "    theta_squared = (theta_dict_scaled[HNL_mass])**2\n",
    "    print(theta_squared)\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_dict[HNL_mass][2])*theta_squared\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][0])*theta_squared)\n",
    "    \n",
    "    LIMIT = np.sqrt(obs_limit_dict[HNL_mass])*theta_squared\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)\n",
    "    obs_limit.append(LIMIT)\n",
    "    \n",
    "print(exp_limit)\n",
    "print(obs_limit)\n",
    "removed_2MeV = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ae591-6907-40f0-88fe-08ad10831664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masses = HNL_masses\n",
    "# # limts_Maj = [0.005413019157172526,5.447736619270424e-06,1.585234277739009e-06] #ee\n",
    "# limts_Maj = [1.8571539668620982e-07,4.214921434135996e-08,2.217237268632437e-08] #pi0\n",
    "# limits_Dir = [5.378917430264621e-07, 1.2004619124764445e-07, 6.640407306572216e-08]\n",
    "# plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "# plt.plot(masses, limts_Maj, label=\"Maj.\")\n",
    "# plt.plot(masses, limits_Dir, label=\"Dir.\")\n",
    "# plt.yscale('log')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167745e-cd79-4559-b68f-b93151b57ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio_Dirac = {}\n",
    "# ratio_sum = 0\n",
    "# for i, HNL_mass in enumerate(HNL_masses):\n",
    "#     ratio_Dirac[HNL_mass] = limts_Maj[i]/limits_Dir[i]\n",
    "#     ratio_sum += ratio_Dirac[HNL_mass]\n",
    "    \n",
    "# print(ratio_Dirac)\n",
    "# print(ratio_sum/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa025c-f087-4303-882d-3b7093ed712a",
   "metadata": {},
   "source": [
    "## Cross-checking limit from asymptotic test-stat approximations by manually plotting test-stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4234fa8-33c9-48dc-9eaa-674262afcba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Use \"Making test statistic plots\" code for this now\n",
    "# #Takes ~10 mins with 100 toys and 4 bins, for all errors in one modifier\n",
    "# for HNL_mass in HNL_masses:\n",
    "\n",
    "#     if Params_pyhf[\"Use_toys\"] == False:\n",
    "#         CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "#             1.0,  # null hypothesis\n",
    "#             DATA_OBS_dict[HNL_mass],\n",
    "#             model_dict[HNL_mass],\n",
    "#             test_stat=\"qtilde\",\n",
    "#             return_expected_set=True,\n",
    "#             calctype=\"asymptotics\",\n",
    "#             )\n",
    "#     if Params_pyhf[\"Use_toys\"] == True:\n",
    "#         CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "#             1.0,  # null hypothesis\n",
    "#             DATA_OBS_dict[HNL_mass],\n",
    "#             model_dict[HNL_mass],\n",
    "#             test_stat=\"qtilde\",\n",
    "#             return_expected_set=True,\n",
    "#             calctype=\"toybased\",\n",
    "#             ntoys=Params_pyhf[\"Num_toys\"],\n",
    "#             # track_progress=True, \n",
    "#             # track_progress=False, #Used to have as true, but this gave an error when the signal was scaled up\n",
    "#             )\n",
    "    \n",
    "#     print(f\"{HNL_mass}MeV\")\n",
    "#     for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "#         print(f\"Expected CLs({n_sigma:2d} ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22113a-be3b-404d-95b5-9c4154641beb",
   "metadata": {},
   "source": [
    "## Removing 2MeV point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d61f76-b44a-42a5-ba62-7e5340758ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is crap, could just change what is loaded in, but leaving like this for compatibility\n",
    "reload(Constants)\n",
    "if Params_pyhf[\"Load_lepton_hists\"]==True:\n",
    "    HNL_masses=Constants.HNL_mass_samples.copy()\n",
    "    if removed_2MeV == False:\n",
    "        print(\"Removing the first element in the limit and mass lists\")\n",
    "        exp_limit.pop(0)\n",
    "        obs_limit.pop(0)\n",
    "        HNL_masses.pop(0)\n",
    "        exp_1sig_up.pop(0)\n",
    "        exp_2sig_up.pop(0)\n",
    "        exp_1sig_down.pop(0)\n",
    "        exp_2sig_down.pop(0)\n",
    "        print(HNL_masses)\n",
    "        print(exp_limit)\n",
    "        print(obs_limit)\n",
    "    else: print(\"First element has already been removed\")\n",
    "    removed_2MeV = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8437737-0007-42bb-9cfa-bd8e2a45194e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving limit as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefcef15-f96e-4934-bd33-5fc7ff782499",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_limit)\n",
    "print(obs_limit)\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e73a7-2fee-4aad-9067-e3dd5d7d9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = HNL_masses\n",
    "\n",
    "if Params_pyhf[\"Stats_only\"] == True: stats =  \"Stats_only\"\n",
    "elif Params_pyhf[\"Use_flat_sys\"] == True: stats = \"Flat_sys\"\n",
    "else: stats = \"Full_sys\"\n",
    "    \n",
    "if Params_pyhf[\"Use_part_only\"] == True: part_hist = str(Params_pyhf[\"Num_bins_for_calc\"])+\"_bins\"\n",
    "else: part_hist = \"full_hist\"\n",
    "\n",
    "print(masses)\n",
    "print(exp_limit)\n",
    "\n",
    "# filename = \"EXT_full_Finished.csv\"\n",
    "filename += \"full_separated.csv\"\n",
    "\n",
    "save_limits = input(f\"Do you want to save the limits in new {filename} files? y/n \")\n",
    "if save_limits == \"y\":\n",
    "# r = zip(masses, exp_limit)\n",
    "    r = zip(masses, obs_limit)\n",
    "    q = zip(masses, exp_limit)\n",
    "    if Params_pyhf[\"Load_lepton_hists\"] == True:\n",
    "        savename = f'limit_files/My_limits/observed_{stats}_{part_hist}_{filename}'\n",
    "        with open(savename, \"w\") as s:\n",
    "            w = csv.writer(s)\n",
    "            for row in r:\n",
    "                w.writerow(row)\n",
    "\n",
    "        savename_exp = f'limit_files/My_limits/expected_{stats}_{part_hist}_{filename}'\n",
    "        with open(savename_exp, \"w\") as s:\n",
    "            w = csv.writer(s)\n",
    "            for row in q:\n",
    "                w.writerow(row)\n",
    "\n",
    "    if Params_pyhf[\"Load_pi0_hists\"] == True:\n",
    "        savename = f'limit_files/My_limits/{stats}_{part_hist}_pi0_{filename}'\n",
    "        with open(f'limit_files/My_limits/{stats}_{part_hist}_pi0_{filename}', \"w\") as s:\n",
    "            w = csv.writer(s)\n",
    "            for row in r:\n",
    "                w.writerow(row)\n",
    "\n",
    "    if Params_pyhf[\"Load_single_r1_file\"] == True:\n",
    "        savename = f'limit_files/My_limits/{stats}_{part_hist}_single_file_{filename}'\n",
    "        with open(f'limit_files/My_limits/{stats}_{part_hist}_single_file_{filename}', \"w\") as s:\n",
    "            w = csv.writer(s)\n",
    "            for row in r:\n",
    "                w.writerow(row)\n",
    "\n",
    "    print()\n",
    "    print(\"Save name is : \\'\"+savename+\"\\'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a309c42f-47a3-4d38-a2a7-fb09f997f2ab",
   "metadata": {},
   "source": [
    "## Saving sigma bands for brazil plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedea031-79e6-4656-840d-a6f6a4a42c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = \"limit_files/Brazil_plot/\"\n",
    "to_save_names = [\"exp_1sig_up\",\"exp_1sig_down\",\"exp_2sig_up\",\"exp_2sig_down\",\"exp_limit\",\"obs_limit\"]\n",
    "to_save_lists = [exp_1sig_up,exp_1sig_down,exp_2sig_up,exp_2sig_down,exp_limit,obs_limit]\n",
    "\n",
    "if Params_pyhf[\"Load_lepton_hists\"] == True:decay_type = \"ee\"\n",
    "elif Params_pyhf[\"Load_pi0_hists\"]==True: decay_type = \"pi0\"\n",
    "else: decay_type = \"single_file\"\n",
    "\n",
    "filename=f\"_{decay_type}_19_June.csv\"\n",
    "for i, lim in enumerate(to_save_lists):\n",
    "\n",
    "    r = zip(masses, lim)\n",
    "    savestr=to_save_names[i]\n",
    "    savename = save_loc+savestr+filename\n",
    "    with open(savename, \"w\") as s:\n",
    "        w = csv.writer(s)\n",
    "        for row in r:\n",
    "            w.writerow(row)\n",
    "\n",
    "print(\"Last saved is \" + savename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b13b856-e47a-4b8e-b67c-740dcd02d13b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Making pull plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0a637-ac65-42fb-886e-8c66bc3ed15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_bkg_only(Total_dict, debug=False):\n",
    "    \"\"\"\n",
    "    Creating a model where the background samples are fed in individually (takes longer than summing into one total bkg hist).\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    sig_norm = {\"hi\": 1.0+Params_pyhf[\"Signal_flux_error\"], \"lo\": 1.0-Params_pyhf[\"Signal_flux_error\"]}\n",
    "    dirt_norm = {\"hi\": 1.0+Params_pyhf[\"Flat_bkg_dirt_frac\"], \"lo\": 1.0-Params_pyhf[\"Flat_bkg_dirt_frac\"]}\n",
    "    for HNL_mass in Total_dict:\n",
    "        if(debug):print(HNL_mass)\n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass]['TOT_SIGNAL_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"mu\", \"type\": \"normfactor\", \"data\": None} #This is the scaling which is to be scanned over in the hypo tests\n",
    "                # {\"name\": \"stat_siguncrt\", \"type\": \"staterror\", \"data\": sig_stat[HNL_mass]},\n",
    "                # {\"name\": \"norm_siguncrt\", \"type\": \"normsys\", \"data\": sig_norm}, #NuMI absorber KDAR rate\n",
    "                # {\"name\": \"Detvar_sig\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['SIGNAL_DETVAR']} #shapesys assumes uncorrelated\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"overlay\",\n",
    "              \"data\": Total_dict[HNL_mass]['OVERLAY_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_overlay\", \"type\": \"staterror\", \"data\": overlay_stat[HNL_mass]},\n",
    "                {\"name\": \"Multisim_overlay\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_MULTISIM']},\n",
    "                {\"name\": \"Detvar_overlay\", \"type\": \"shapesys\", \"data\": Total_dict[HNL_mass]['BKG_DETVAR']} #shapesys assumes uncorrelated\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"dirt\",\n",
    "              \"data\": Total_dict[HNL_mass]['DIRT_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_dirt\", \"type\": \"staterror\", \"data\": dirt_stat[HNL_mass]},\n",
    "                {\"name\": \"dirt_norm\", \"type\": \"normsys\", \"data\": dirt_norm}\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"beamoff\",\n",
    "              \"data\": Total_dict[HNL_mass]['BEAMOFF_VALS'],\n",
    "              \"modifiers\": [\n",
    "                {\"name\": \"stat_beamoff\", \"type\": \"staterror\", \"data\": beamoff_stat[HNL_mass]}\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict\n",
    "\n",
    "model_bkg_only = create_model_bkg_only(Total_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374769d-2898-4c75-957c-f321fc9fc214",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bkg_only = create_model_correlated(Total_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d1bd07-e77d-4089-a97b-65f5e521b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhf.set_backend(\"numpy\", \"minuit\")\n",
    "pyhf.set_backend(pyhf.tensorlib, pyhf.optimize.minuit_optimizer(tolerance=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88355dc9-71d0-42a3-be71-8e0f41d8e85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tensor Lib: {pyhf.tensorlib}\")\n",
    "print(f\"Optimizer:  {pyhf.optimizer}\")\n",
    "print(f\"Tolerance:  {pyhf.optimizer.tolerance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f7bb1-b354-481c-8fb6-c9f4a71bc681",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 150\n",
    "print(HNL_mass)\n",
    "model = model_bkg_only[HNL_mass]\n",
    "data = Total_dict[HNL_mass][\"data\"]+model.config.auxdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b68ca-16e8-4817-b9da-45a33de1d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pyhf.infer.mle.fit(data, model, return_uncertainties=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa81a2d-2af9-496f-a2bd-b7202bbb0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestfit, errors = result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395e0b8-3868-4f4a-b884-0901a3c85a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing first value! hacky, should load bkg-only somehow\n",
    "new_bestfit = np.delete(bestfit, model.config.poi_index)\n",
    "new_errors = np.delete(errors, model.config.poi_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594b8b0-7612-4b24-aa20-882ed65d450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.par_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9997e067-02c4-4fd9-bc29-1f382534cf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulls = pyhf.tensorlib.concatenate(\n",
    "    [\n",
    "        (bestfit[model.config.par_slice(k)] - model.config.param_set(k).suggested_init)\n",
    "        / model.config.param_set(k).width()\n",
    "        for k in model.config.par_order\n",
    "        if model.config.param_set(k).constrained\n",
    "    ]\n",
    ")\n",
    "\n",
    "pullerr = pyhf.tensorlib.concatenate(\n",
    "    [\n",
    "        errors[model.config.par_slice(k)] / model.config.param_set(k).width()\n",
    "        for k in model.config.par_order\n",
    "        if model.config.param_set(k).constrained\n",
    "    ]\n",
    ")\n",
    "\n",
    "labels = np.asarray(\n",
    "    [\n",
    "        f\"{k}[{i}]\" if model.config.param_set(k).n_parameters > 1 else k\n",
    "        for k in model.config.par_order\n",
    "        if model.config.param_set(k).constrained\n",
    "        for i in range(model.config.param_set(k).n_parameters)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20daf0e-3942-40f8-a724-2dffaeada86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_order = np.argsort(new_errors)\n",
    "bestfit = bestfit[_order]\n",
    "errors = errors[_order]\n",
    "labels = labels[_order]\n",
    "pulls = pulls[_order]\n",
    "pullerr = pullerr[_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a1f89-ac2a-4a49-998a-3ad1d5dd0e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(20, 8)\n",
    "\n",
    "# set up axes labeling, ranges, etc...\n",
    "ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(labels.size).tolist()))\n",
    "ax.set_xticklabels(labels, rotation=80, ha=\"right\")\n",
    "ax.set_xlim(-0.5, len(pulls) - 0.5)\n",
    "ax.set_title(\"Pull Plot\", fontsize=18)\n",
    "ax.set_ylabel(r\"$(\\theta - \\hat{\\theta})\\,/ \\Delta \\theta$\", fontsize=18)\n",
    "\n",
    "# draw the +/- 2.0 horizontal lines\n",
    "ax.hlines([-2, 2], -0.5, len(pulls) - 0.5, colors=\"black\", linestyles=\"dotted\")\n",
    "# draw the +/- 1.0 horizontal lines\n",
    "ax.hlines([-1, 1], -0.5, len(pulls) - 0.5, colors=\"black\", linestyles=\"dashdot\")\n",
    "# draw the +/- 2.0 sigma band\n",
    "ax.fill_between([-0.5, len(pulls) - 0.5], [-2, -2], [2, 2], facecolor=\"yellow\")\n",
    "# drawe the +/- 1.0 sigma band\n",
    "ax.fill_between([-0.5, len(pulls) - 0.5], [-1, -1], [1, 1], facecolor=\"green\")\n",
    "# draw a horizontal line at pull=0.0\n",
    "ax.hlines([0], -0.5, len(pulls) - 0.5, colors=\"black\", linestyles=\"dashed\")\n",
    "# finally draw the pulls\n",
    "ax.scatter(range(len(pulls)), pulls, color=\"black\")\n",
    "# and their uncertainties\n",
    "ax.errorbar(\n",
    "    range(len(pulls)),\n",
    "    pulls,\n",
    "    color=\"black\",\n",
    "    xerr=0,\n",
    "    yerr=pullerr,\n",
    "    marker=\".\",\n",
    "    fmt=\"none\",\n",
    ")\n",
    "\n",
    "# error > 1\n",
    "error_gt1 = np.argmax(errors > 1) - 0.5\n",
    "ax.axvline(x=error_gt1, color=\"red\", linestyle=\"--\")\n",
    "ax.text(\n",
    "    error_gt1 + 0.1, 1.5, r\"$\\sigma \\geq 1 \\longrightarrow$\", color=\"red\", fontsize=18\n",
    ");\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig = input(\"Do you want to save the pull plot? y/n \")\n",
    "if save_fig == 'y':\n",
    "    plt.savefig(f\"plots/CLs_plots/Fitting_tests/Pull_plot_{HNL_mass}_{name_type}.pdf\")\n",
    "    plt.savefig(f\"plots/CLs_plots/Fitting_tests/Pull_plot_{HNL_mass}_{name_type}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeac33b-58bc-4054-b1f4-20fe1f20cf3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing adjacent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da1e60-6a48-4060-9907-67ff7de1e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in sigma bands for Brazil plot\n",
    "#Not finished yet, will take a few hours to get this working\n",
    "\n",
    "def Get_sigma_bands(decay_type, filename):\n",
    "    \"\"\"\n",
    "    Load sigma bands for each mass point from .csv files saved above.\n",
    "    Filename is without the decay type or .csv\n",
    "    \"\"\"\n",
    "    if decay_type not in [\"ee\", \"pi0\"]:\n",
    "        print(\"Need to choose decay type \\\"ee\\\" or \\\"pi0\\\". Exiting.\")\n",
    "        return 1\n",
    "    \n",
    "    to_load_names = [\"exp_1sig_up\",\"exp_1sig_down\",\"exp_2sig_up\",\"exp_2sig_down\",\"exp_limit\",\"obs_limit\"]\n",
    "    loaded_lists = []\n",
    "    for i, name in enumerate(to_load_names):\n",
    "        full_path = f\"limit_files/Brazil_plot/{name}_{decay_type}_{filename}.csv\"\n",
    "        if(os.path.exists(full_path)):\n",
    "            with open(full_path, \"r\") as fp:   # Unpickling\n",
    "                reader = csv.reader(fp)\n",
    "                loaded_list = list(reader)\n",
    "                loaded_lists.append(loaded_list)\n",
    "                \n",
    "    return loaded_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e774def-be03-441b-8a3f-c8280c126e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_test = Get_sigma_bands(\"pi0\", \"19_June\")\n",
    "print(loaded_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da226c-15c4-4050-aed5-372579a8931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_list = list(hist_dict_run1.keys())\n",
    "\n",
    "all_files_dict = {}\n",
    "Run = \"run1\"\n",
    "end_string = BDT_name\n",
    "\n",
    "for i, HNL_mass in enumerate(mass_list):\n",
    "    adj_models_dict = {}\n",
    "    # original_theta = Constants.theta_mu_4_dict[HNL_mass]\n",
    "    \n",
    "    # bins = merged_bins_dict[HNL_mass]\n",
    "    # sample_list = bkg_samples + [sig_name]\n",
    "\n",
    "    model_masses = [] #for masses either side of test point (if available)\n",
    "    \n",
    "    if i > 0: model_masses.append(mass_list[i-1])\n",
    "    if i < len(mass_list)-1: model_masses.append(mass_list[i+1])\n",
    "    \n",
    "    for model in model_masses: #Looping over adjacent models\n",
    "        \n",
    "        save_name = Run+f\"_Test_{HNL_mass}_{name_type}_model_{model}{end_string}.root\"\n",
    "        \n",
    "        file = uproot.open(\"bdt_output/adjacent_models/\"+save_name)\n",
    "    sample_str = f\"{HNL_mass}_{model}\"\n",
    "    # all_files_dict[HNL_mass] = adj_models_dict\n",
    "    all_files_dict[sample_str] = adj_models_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ec2712-ea2f-4589-ba97-a197cb4a9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Functions)\n",
    "zero_bins_errors_run1 = Functions.make_zero_bin_unc(hist_dict_run1, Constants.run1_POT_scaling_dict, Params_pyhf)\n",
    "zero_bins_errors_run3 = Functions.make_zero_bin_unc(hist_dict_run3, Constants.run3_POT_scaling_dict, Params_pyhf)\n",
    "\n",
    "TOT_R1_ERR = Calculate_total_uncertainty(Params_pyhf, hist_dict_run1, zero_bins_errors_run1)\n",
    "TOT_R3_ERR = Calculate_total_uncertainty(Params_pyhf, hist_dict_run3, zero_bins_errors_run3)\n",
    "\n",
    "R1_BKG, R1_SIGNAL = Add_bkg_hists_make_signal(hist_dict_run1)\n",
    "R3_BKG, R3_SIGNAL = Add_bkg_hists_make_signal(hist_dict_run3)\n",
    "\n",
    "R1_output = Functions.Make_into_lists(Params_pyhf, R1_BKG, R1_SIGNAL, TOT_R1_ERR)\n",
    "R3_output = Functions.Make_into_lists(Params_pyhf, R3_BKG, R3_SIGNAL, TOT_R3_ERR)\n",
    "\n",
    "list_input_dicts = [R1_output, R3_output]\n",
    "\n",
    "Total_dict_both = Functions.Create_final_appended_runs_dict(list_input_dicts)\n",
    "Total_dict_run1 = Functions.Create_final_appended_runs_dict([R1_output])\n",
    "Total_dict_run3 = Functions.Create_final_appended_runs_dict([R3_output])\n",
    "\n",
    "if Params_pyhf[\"Use_part_only\"]==True:\n",
    "    NUMBINS = Params_pyhf[\"Num_bins_for_calc\"]\n",
    "else: NUMBINS=30 #This will just give the full hist\n",
    "    \n",
    "if 'data;1' in hist_dict_run1[150]:\n",
    "    Total_dict_run1=add_data(Total_dict_run1, hist_dict_run1, NUMBINS)\n",
    "    Total_dict_run3=add_data(Total_dict_run3, hist_dict_run3, NUMBINS)\n",
    "    Total_dict_both=add_data_appended(Total_dict_both, hist_dict_run1, hist_dict_run3, NUMBINS)\n",
    "    \n",
    "print(Total_dict_both[150].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79e7cd9-cf28-49f0-a3a0-bef0f63c1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacent_hist_dict_run1, adjacent_hist_dict_run3 = {}, {}\n",
    "R1_BKG_ERR_dict_ADJ, R3_BKG_ERR_dict_ADJ, R1_SIGNAL_ERR_dict_ADJ, R3_SIGNAL_ERR_dict_ADJ = {}, {}, {}, {}\n",
    "mass_point = 50\n",
    "test_models = [20, 100]\n",
    "for mass_model in test_models:\n",
    "    \n",
    "    adjacent_hist_dict_run1[mass_model] = uproot.open(f'bdt_output/adjacent_models/run1_{mass_point}MeV_{mass_model}MeV_model.root')\n",
    "    adjacent_hist_dict_run3[mass_model] = uproot.open(f'bdt_output/adjacent_models/run3_{mass_point}MeV_{mass_model}MeV_model.root')\n",
    "    R1_BKG_ERR_dict_ADJ, R1_SIGNAL_ERR_dict_ADJ = Calculate_total_uncertainty(Params_pyhf, adjacent_hist_dict_run1)\n",
    "    R3_BKG_ERR_dict_ADJ, R3_SIGNAL_ERR_dict_ADJ = Calculate_total_uncertainty(Params_pyhf, adjacent_hist_dict_run3)\n",
    "    \n",
    "adjacent_hist_dict_run1[mass_point] = hist_dict_run1[mass_point]\n",
    "adjacent_hist_dict_run3[mass_point] = hist_dict_run3[mass_point]\n",
    "R1_BKG_ERR_dict_ADJ[mass_point]=R1_BKG_ERR_dict[mass_point]\n",
    "R3_BKG_ERR_dict_ADJ[mass_point]=R3_BKG_ERR_dict[mass_point]\n",
    "R1_SIGNAL_ERR_dict_ADJ[mass_point]=R1_SIGNAL_ERR_dict[mass_point]\n",
    "R3_SIGNAL_ERR_dict_ADJ[mass_point]=R3_SIGNAL_ERR_dict[mass_point]\n",
    "\n",
    "R1_BKG_ADJ, R1_SIGNAL_ADJ = Add_bkg_hists_make_signal(adjacent_hist_dict_run1)\n",
    "R3_BKG_ADJ, R3_SIGNAL_ADJ = Add_bkg_hists_make_signal(adjacent_hist_dict_run3)\n",
    "\n",
    "R1_output_ADJ = Functions.Make_into_lists(Params_pyhf, R1_BKG_ADJ, R1_SIGNAL_ADJ, R1_BKG_ERR_dict_ADJ, R1_SIGNAL_ERR_dict_ADJ)\n",
    "R3_output_ADJ = Functions.Make_into_lists(Params_pyhf, R3_BKG_ADJ, R3_SIGNAL_ADJ, R3_BKG_ERR_dict_ADJ, R3_SIGNAL_ERR_dict_ADJ)\n",
    "\n",
    "list_input_dicts_ADJ = [R1_output_ADJ, R3_output_ADJ]\n",
    "# list_input_dicts = [R1_output, R1_output] #Used when I didn't have Run3\n",
    "\n",
    "Total_dict_ADJ = Functions.Create_final_appended_runs_dict(list_input_dicts_ADJ)\n",
    "print(Total_dict_ADJ[100].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af73b4a8-0dad-4d60-a962-c7155024db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Total_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7688a367-1e06-44a8-95df-464d27a37a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_ADJ = create_model_dict(Total_dict_ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eeb77d-75dc-4454-9988-2c852640c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test = adjacent_hist_dict_run1.keys()\n",
    "DATA_OBS_dict = {}\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples: #removing the 240MeV point\n",
    "for HNL_mass in list_test:\n",
    "    init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "    model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "    bkg_pars = init_pars.copy()\n",
    "    bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "    model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "    DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass]['TOT_BKG_VALS']+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "    model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639d271-7e7c-4cff-9ed6-d20f1f7095cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for HNL_mass in list_test:\n",
    "\n",
    "    if Params_pyhf[\"Use_toys\"] == False:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"asymptotics\",\n",
    "            )\n",
    "    if Params_pyhf[\"Use_toys\"] == True:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"toybased\",\n",
    "            ntoys=Params_pyhf[\"Num_toys\"],\n",
    "            track_progress=True,\n",
    "            )\n",
    "    \n",
    "    print(f\"{HNL_mass}MeV\")\n",
    "    for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "        print(f\"Expected CLs({n_sigma:2d} ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ab734-f7dd-40bb-8b3a-312eda608e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "print(\"If the output of the following is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "poi_values = np.linspace(0.001, 2, 100) #I could make a dict of this and have different pois for different mass points\n",
    "print(\"Max value is \" + str(max(poi_values)))\n",
    "print(\"Min value is \" + str(min(poi_values)))\n",
    "print(\"Next value is \" + str(poi_values[1]))\n",
    "print(\"Next value is \" + str(poi_values[2]) + \"\\n\")\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "# list_test = [180, 200, 220, 240, 245]\n",
    "for HNL_mass in list_test:\n",
    "\n",
    "    # poi_values = np.linspace(0.001, 4, 100)\n",
    "    obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "        DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_values, level=0.1, return_results=True\n",
    "    )\n",
    "\n",
    "    print(f\"Upper limit {HNL_mass}MeV (obs):  = {obs_limit_dict[HNL_mass]:.6f}\")\n",
    "    print(f\"Upper limit {HNL_mass}MeV (exp):  = {exp_limits_dict[HNL_mass][2]:.6f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb324293-40db-4be3-94b0-4498860ed94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = [] #entry 2\n",
    "exp_1sig_up = [] #entry 3\n",
    "exp_1sig_down = [] #entry 1\n",
    "exp_2sig_up = [] #entry 4\n",
    "exp_2sig_down = [] #entry 0\n",
    "obs_limit = []\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in exp_limits_dict:\n",
    "    theta_squared = (theta_dict_scaled[mass_point])**2\n",
    "    print(theta_squared)\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_dict[HNL_mass][2])*theta_squared\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][0])*theta_squared)\n",
    "    \n",
    "    LIMIT = np.sqrt(obs_limit_dict[HNL_mass])*theta_squared\n",
    "    # if HNL_mass in Constants.Old_generator_mass_points:\n",
    "    #     EXP_LIMIT = EXP_LIMIT*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "    #     LIMIT = LIMIT*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a5ecc-57ca-4f56-bc74-cc00bdadfdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [6.912810521978472e-06, 7.3827958770328454e-06, 1.9840221581842897e-05]\n",
    "x_points = [50, 50, 50]\n",
    "labels = [\"correct\", \"20MeV\", \"50MeV\"]\n",
    "for i, point in enumerate(points):\n",
    "    plt.plot(x_points[i],point,label=labels[i],marker=\"o\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c38a19-4c02-4f26-8aef-5947b26cf4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_20 = 6.912810521978472e-06/7.3827958770328454e-06\n",
    "rat_100 = 6.912810521978472e-06/1.9840221581842897e-05\n",
    "print(\"20MeV model is \" + str(rat_20))\n",
    "print(\"100MeV model is \" + str(rat_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f000d9-ff89-4ade-b17f-c45a59bedb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs_limit[-1])\n",
    "print(exp_1sig_down[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c1c24-58ae-4433-8b1c-30f24a80e5b1",
   "metadata": {},
   "source": [
    "## Brazil plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2d76f-c16f-47a0-8c41-3889c5844e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting ee limit\n",
    "if Params_pyhf[\"Load_lepton_hists\"]==True:\n",
    "    plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "    savefig = True\n",
    "\n",
    "    plt.plot(HNL_masses,np.array(obs_limit),lw=4,ls='-',color='black',label='Observed')\n",
    "    plt.plot(HNL_masses,np.array(exp_limit),lw=2,ls='--',color='red',label='Expected')\n",
    "    plt.fill_between(HNL_masses,np.array(exp_2sig_down),np.array(exp_2sig_up),color='yellow',label=r'Exp. 2$\\sigma$')\n",
    "    plt.fill_between(HNL_masses,np.array(exp_1sig_down),np.array(exp_1sig_up),color='lightgreen',label=r'Exp. 1$\\sigma$')\n",
    "\n",
    "    plt.ylabel(r'$|U_{\\mu 4}|^2$ Limit at 90% CL',fontsize=22)\n",
    "    plt.xlabel('HNL Mass [MeV]',fontsize=22)\n",
    "\n",
    "    plt.ylim(5e-7,6e-3)\n",
    "    plt.legend(loc=\"lower left\",ncol=2,frameon=False,fontsize=22)\n",
    "    # plt.text(0.99,0.95,r'MicroBooNE NuMI POT:$7.01\\times 10^{20}$',fontsize=20,color='black',alpha=1,verticalalignment='top',horizontalalignment='right',transform=plt.gca().transAxes)\n",
    "    plt.tick_params(axis='x', labelsize=20)\n",
    "    plt.tick_params(axis='y', labelsize=20)\n",
    "    plt.xlim(0,155)\n",
    "    # plt.xlim(0,250)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    savename = f\"{decay_type}_observed\"\n",
    "\n",
    "    if savefig == True:\n",
    "        plt.savefig('plots/Limits/'+savename+'.pdf',bbox_inches='tight', pad_inches=0.3)\n",
    "        plt.savefig('plots/Limits/'+savename+'.png',bbox_inches='tight', pad_inches=0.3)\n",
    "\n",
    "    plt.grid(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ce420-c92b-488f-9d87-1b2bac39dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting pi0 limit\n",
    "if Params_pyhf[\"Load_pi0_hists\"]==True:\n",
    "    plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "    savefig = True\n",
    "\n",
    "    plt.plot(HNL_masses,np.array(obs_limit),lw=5,ls='-',color='black',label='Observed')\n",
    "    plt.plot(HNL_masses,np.array(exp_limit),lw=2,ls='--',color='red',label='Expected')\n",
    "    plt.fill_between(HNL_masses,np.array(exp_2sig_down),np.array(exp_2sig_up),color='yellow',label=r'Exp. 2$\\sigma$')\n",
    "    plt.fill_between(HNL_masses,np.array(exp_1sig_down),np.array(exp_1sig_up),color='lightgreen',label=r'Exp. 1$\\sigma$')\n",
    "\n",
    "    plt.ylabel(r'$|U_{\\mu 4}|^2$ Limit at 90% CL',fontsize=22)\n",
    "    plt.xlabel('HNL Mass [MeV]',fontsize=22)\n",
    "\n",
    "    plt.ylim(1e-8,4e-7)\n",
    "    plt.legend(loc=\"lower left\",ncol=2,frameon=False,fontsize=22)\n",
    "    # plt.text(0.99,0.95,r'MicroBooNE NuMI POT:$7.01\\times 10^{20}$',fontsize=20,color='black',alpha=1,verticalalignment='top',horizontalalignment='right',transform=plt.gca().transAxes)\n",
    "    plt.tick_params(axis='x', labelsize=20)\n",
    "    plt.tick_params(axis='y', labelsize=20)\n",
    "    plt.xlim(140,255)\n",
    "    # plt.xlim(0,250)\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    savename = f\"{decay_type}_observed\"\n",
    "\n",
    "    if savefig == True:\n",
    "        plt.savefig('plots/Limits/'+savename+'.pdf',bbox_inches='tight', pad_inches=0.3)\n",
    "        plt.savefig('plots/Limits/'+savename+'.png',bbox_inches='tight', pad_inches=0.3)\n",
    "\n",
    "    plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aabef30-4da9-4e20-94ae-f99218373d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting BOTH on one plot\n",
    "plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "savefig = False\n",
    "\n",
    "save_loc = \"limit_files/Brazil_plot/\"\n",
    "to_save_names = [\"exp_1sig_up\",\"exp_1sig_down\",\"exp_2sig_up\",\"exp_2sig_down\",\"exp_limit\",\"obs_limit\"]\n",
    "to_save_lists = [exp_1sig_up,exp_1sig_down,exp_2sig_up,exp_2sig_down,exp_limit,obs_limit]\n",
    "\n",
    "#ee curves\n",
    "decay_type=\"ee\"\n",
    "filename=f\"_{decay_type}_21_April.csv\"\n",
    "# for i, lim in enumerate(to_save_lists):\n",
    "\n",
    "ee_exp_1sig_up = Functions.Pandafy_new(save_loc+\"exp_1sig_up\"+filename)\n",
    "ee_exp_1sig_down = Functions.Pandafy_new(save_loc+\"exp_1sig_down\"+filename)\n",
    "ee_exp_2sig_up = Functions.Pandafy_new(save_loc+\"exp_2sig_up\"+filename)\n",
    "ee_exp_2sig_down = Functions.Pandafy_new(save_loc+\"exp_2sig_down\"+filename)\n",
    "ee_exp_limit = Functions.Pandafy_new(save_loc+\"exp_limit\"+filename)\n",
    "ee_obs_limit = Functions.Pandafy_new(save_loc+\"obs_limit\"+filename)\n",
    "HNL_masses_ee = [10, 20, 50, 100, 150]\n",
    "\n",
    "plt.plot(HNL_masses_ee,np.array(ee_obs_limit[\"Value\"]),lw=5,ls='-',color='black')\n",
    "plt.plot(HNL_masses_ee,np.array(ee_exp_limit[\"Value\"]),lw=2,ls='--',color='red')\n",
    "plt.fill_between(HNL_masses_ee,np.array(ee_exp_2sig_down[\"Value\"]),np.array(ee_exp_2sig_up[\"Value\"]),color='yellow')\n",
    "plt.fill_between(HNL_masses_ee,np.array(ee_exp_1sig_down[\"Value\"]),np.array(ee_exp_1sig_up[\"Value\"]),color='lightgreen')\n",
    "\n",
    "#pi0 curves\n",
    "decay_type=\"pi0\"\n",
    "filename=f\"_{decay_type}_21_April.csv\"\n",
    "HNL_masses_pi0 = [150, 180, 200, 220, 240, 245]\n",
    "\n",
    "pi0_exp_1sig_up = Functions.Pandafy_new(save_loc+\"exp_1sig_up\"+filename)\n",
    "pi0_exp_1sig_down = Functions.Pandafy_new(save_loc+\"exp_1sig_down\"+filename)\n",
    "pi0_exp_2sig_up = Functions.Pandafy_new(save_loc+\"exp_2sig_up\"+filename)\n",
    "pi0_exp_2sig_down = Functions.Pandafy_new(save_loc+\"exp_2sig_down\"+filename)\n",
    "pi0_exp_limit = Functions.Pandafy_new(save_loc+\"exp_limit\"+filename)\n",
    "pi0_obs_limit = Functions.Pandafy_new(save_loc+\"obs_limit\"+filename)\n",
    "\n",
    "\n",
    "plt.plot(HNL_masses_pi0,np.array(pi0_obs_limit[\"Value\"]),lw=5,ls='-',color='black',label='Observed')\n",
    "plt.plot(HNL_masses_pi0,np.array(pi0_exp_limit[\"Value\"]),lw=2,ls='--',color='red',label='Expected')\n",
    "plt.fill_between(HNL_masses_pi0,np.array(pi0_exp_2sig_down[\"Value\"]),np.array(pi0_exp_2sig_up[\"Value\"]),color='yellow',label=r'Exp. 2$\\sigma$')\n",
    "plt.fill_between(HNL_masses_pi0,np.array(pi0_exp_1sig_down[\"Value\"]),np.array(pi0_exp_1sig_up[\"Value\"]),color='lightgreen',label=r'Exp. 1$\\sigma$')\n",
    "\n",
    "\n",
    "plt.ylabel(r'$|U_{\\mu 4}|^2$ Limit at 90% CL',fontsize=24)\n",
    "plt.xlabel('HNL Mass [MeV]',fontsize=22)\n",
    "\n",
    "plt.ylim(1e-8,6e-3)\n",
    "plt.legend(loc=\"lower left\",ncol=2,frameon=False,fontsize=24)\n",
    "plt.text(0.99,0.95,r'MicroBooNE NuMI POT:$7.01\\times 10^{20}$',fontsize=24,color='black',alpha=1,\n",
    "         verticalalignment='top',horizontalalignment='right',transform=plt.gca().transAxes)\n",
    "plt.tick_params(axis='x', labelsize=20)\n",
    "plt.tick_params(axis='y', labelsize=20)\n",
    "plt.xlim(0,255)\n",
    "# plt.xlim(0,250)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "savename = \"Both_channels_observed\"\n",
    "\n",
    "# plt.grid(True,lw=1, alpha=0.3)\n",
    "\n",
    "if savefig == True:\n",
    "    plt.savefig('plots/Limits/'+savename+'.pdf',bbox_inches='tight', pad_inches=0.3)\n",
    "    plt.savefig('plots/Limits/'+savename+'.png',bbox_inches='tight', pad_inches=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917fdd5-6480-4802-87f6-35fb140421aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Changing poi values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cae526-6444-454f-9c77-08280027d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CL_range(observations, model,unbounded_bounds,cl):\n",
    "\n",
    "    alpha = round(1-0.01*cl,2)\n",
    "    low_end  = 1000\n",
    "    high_end = 2000\n",
    "    obs_limit = 2000\n",
    "    iterations = 0\n",
    "    delta_iter = 1000\n",
    "    while obs_limit >= high_end:\n",
    "        iterations += 1\n",
    "        if iterations > 1:\n",
    "            delta_iter = iterations*10000\n",
    "            high_end   += delta_iter\n",
    "        poi_values = np.linspace(low_end,high_end,3)\n",
    "        obs_limit, exp_limits = pyhf.infer.intervals.upper_limits.upper_limit(\n",
    "            observations,model,poi_values,level=alpha,par_bounds=unbounded_bounds\n",
    "        )\n",
    "        print(iterations,\"iterations\")\n",
    "    #return np.linspace(low_end,2*high_end,10)\n",
    "    #lo_range = max(1000,obs_limit-obs_limit/2.)\n",
    "    lo_range = 1000.\n",
    "    if obs_limit > 10000:\n",
    "        lo_range = obs_limit-obs_limit/2.\n",
    "    up_range = obs_limit+obs_limit/2.\n",
    "    return np.linspace(lo_range,up_range,50)\n",
    "\n",
    "\n",
    "nbins = model.config.channel_nbins['singlechannel']\n",
    "print(\"\\nNumber of bins:\",nbins)\n",
    "print(type(nbins))\n",
    "\n",
    "# suggested initial parameters\n",
    "init_pars = model.config.suggested_init()\n",
    "print(\"\\nInitial parameters:\",init_pars)\n",
    "print(\"\\nSuggested bounds:\",model.config.suggested_bounds())\n",
    "\n",
    "unbounded_bounds = model.config.suggested_bounds()\n",
    "unbounded_bounds[model.config.poi_index] = (0, 160000000) #Something very large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830b945-1556-4c50-8ce1-86681c781f8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dumping model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc7172-6236-477f-9d5c-1a50e81519ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 100\n",
    "print(json.dumps(model_dict[HNL_mass].spec, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416cdf2-0f11-4ae6-ac1b-9260325f2396",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Comparing to Collie output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608e383-7ca2-43da-99f1-4b5660318ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing to Collie output:\n",
    "\n",
    "# collie_exp = 17010\n",
    "# collie_obs = 20522\n",
    "\n",
    "collie_exp = 16956.64\n",
    "collie_obs = 20029.17\n",
    "\n",
    "SF = scaling_dict[150]\n",
    "\n",
    "pyhf_obs = 0.5027*SF\n",
    "pyhf_exp = 0.4109*SF\n",
    "\n",
    "print(\"collie obs mu is \" + str(collie_obs))\n",
    "print(\"collie exp mu is \" + str(collie_exp))\n",
    "print()\n",
    "\n",
    "print(\"pyhf obs mu is \" + str(pyhf_obs))\n",
    "print(\"pyhf exp mu is \" + str(pyhf_exp))\n",
    "print()\n",
    "\n",
    "print(\"collie divided by pyhf obs is \" + str(collie_obs/pyhf_obs))\n",
    "print(\"collie divided by pyhf exp is \" + str(collie_exp/pyhf_exp))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc1f65-4dfd-45cc-9351-ecdc937f7fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for HNL_mass in [HNL_mass]:\n",
    "    theta_squared = (theta_dict_scaled[HNL_mass])**2\n",
    "    \n",
    "    EXP_LIMIT = np.sqrt(exp_limits_single[2])*theta_squared\n",
    "    LIMIT = np.sqrt(obs_limit_single)*theta_squared\n",
    "    print(\"-----pyhf-----\")\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "print(\"-----Collie-----\")\n",
    "collie_exp_limit = np.sqrt(collie_exp)*theta_dict[HNL_mass]**2 #Collie input is NOT scaled\n",
    "collie_obs_limit = np.sqrt(collie_obs)*theta_dict[HNL_mass]**2 #Collie input is NOT scaled\n",
    "print(f\"Expected {HNL_mass}MeV limit is \" + str(collie_exp_limit))\n",
    "print(f\"Observed {HNL_mass}MeV limit is \" + str(collie_obs_limit)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80bf9e-cc1a-4d6a-807e-ec8653445b9d",
   "metadata": {},
   "source": [
    "# End of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaefdcf-f13a-4a02-b9cd-0155fe6e007f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
