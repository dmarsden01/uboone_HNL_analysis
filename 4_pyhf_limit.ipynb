{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe0cab0-578b-42ea-86af-07342852e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful!\n"
     ]
    }
   ],
   "source": [
    "#Loading libraries\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyhf\n",
    "from pyhf.contrib.viz import brazil\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import uproot\n",
    "import math\n",
    "import awkward as ak\n",
    "import pickle\n",
    "\n",
    "import csv\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print(\"Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6939111e-d766-4112-ab60-1fd7c65239ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating stats-only limit.\n"
     ]
    }
   ],
   "source": [
    "Params_pyhf = {\"Use_flat_sys_bkg\":False,\n",
    "               \"Use_flat_sys_signal\":True,\n",
    "               \"Stats_only\":True,\n",
    "               \"Use_second_half_only\":True,\n",
    "               \"Load_logit_hists\":True,\n",
    "               \"Use_toys\":False,\n",
    "               \"Num_toys\":100,\n",
    "               \"Load_lepton_hists\":True,\n",
    "               \"Load_pi0_hists\":False,\n",
    "               \"Flat_overlay_bkg_frac\":0.5,\n",
    "               \"Flat_dirt_bkg_frac\":1.0,\n",
    "               \"Flat_sig_KDAR\":0.5,\n",
    "               \"Flat_sig_detvar\":0.2, #This is very conservative, could be fed in per mass point from signal detvar script\n",
    "               \"Signal_flux_error\":0.3, #This comes from the KDAR flux uncertainty.\n",
    "               \"Overlay_detvar_frac\":0.5}\n",
    "\n",
    "Functions.pyhf_params(Params_pyhf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb78650-72a3-4fa1-ad92-465a31cba48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "#Loading My BDT histograms\n",
    "# loc_hists = 'bdt_output/'\n",
    "loc_hists = 'Uncertainties/'\n",
    "\n",
    "hist_dict_run1 = {}\n",
    "hist_dict_run3 = {}\n",
    "theta_dict = {}\n",
    "\n",
    "#Loading in the .root files\n",
    "if Params_pyhf[\"Load_lepton_hists\"] == True:\n",
    "    for HNL_mass in Constants.HNL_mass_samples:\n",
    "    # for HNL_mass in [10]:\n",
    "        if Params_pyhf[\"Load_logit_hists\"] == False:\n",
    "            hist_dict_run1[HNL_mass] = uproot.open(loc_hists+f'run1_{HNL_mass}MeV_test2.root')\n",
    "            hist_dict_run3[HNL_mass] = uproot.open(loc_hists+f'run3_{HNL_mass}MeV_test2.root')\n",
    "        if Params_pyhf[\"Load_logit_hists\"] == True:\n",
    "            hist_dict_run1[HNL_mass] = uproot.open(loc_hists+f'run1_{HNL_mass}MeV_logit_NEWEST.root')\n",
    "            hist_dict_run3[HNL_mass] = uproot.open(loc_hists+f'run3_{HNL_mass}MeV_logit_NEWEST.root')\n",
    "        theta_dict[HNL_mass] = hist_dict_run1[HNL_mass][\"theta_tree/theta\"].array()[0][0] #assuming scaled theta is the same for all runs\n",
    "    \n",
    "if Params_pyhf[\"Load_pi0_hists\"] == True:\n",
    "    pi0_dict_run1, pi0_dict_run3 = {}, {}\n",
    "    for HNL_mass in Constants.HNL_mass_pi0_samples:\n",
    "        if Params_pyhf[\"Load_logit_hists\"] == False:\n",
    "            hist_dict_run1[HNL_mass] = uproot.open(loc_hists+f'pi0/run1_{HNL_mass}MeV_test1.root') #run1_150MeV_logit_test1.root\n",
    "            hist_dict_run3[HNL_mass] = uproot.open(loc_hists+f'pi0/run3_{HNL_mass}MeV_test1.root')\n",
    "        if Params_pyhf[\"Load_logit_hists\"] == True:\n",
    "            hist_dict_run1[HNL_mass] = uproot.open(loc_hists+f'pi0/run1_{HNL_mass}MeV_logit_test1.root')\n",
    "            hist_dict_run3[HNL_mass] = uproot.open(loc_hists+f'pi0/run3_{HNL_mass}MeV_logit_test1.root')\n",
    "\n",
    "#list_of_dicts = [hist_dict_run1, hist_dict_run3] #Add run2 when available, not using yet\n",
    "\n",
    "#Constants\n",
    "theta_squared = Constants.theta_mu_4*Constants.theta_mu_4\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2e45e-7b0c-45cc-a21a-65bfd2fafdc3",
   "metadata": {},
   "source": [
    "## Loading in Uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466c18c5-63d4-4237-96ec-e8ac7a64789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_total_uncertainty(Params, hist_dict, bkg_reweight_err_dict=None, bkg_detvar_dict=None, sig_detvar_dict=None): #Takes the dictionary of all root files\n",
    "    BKG_ERR_dict, SIGNAL_ERR_dict = {}, {}\n",
    "    for HNL_mass in hist_dict:\n",
    "        bkg_stat_err_list = [hist_dict[HNL_mass]['bkg_overlay'].errors(), \n",
    "                             hist_dict[HNL_mass]['bkg_EXT'].errors(), \n",
    "                             hist_dict[HNL_mass]['bkg_dirt'].errors()]\n",
    "        sig_stat_err = hist_dict[HNL_mass]['signal'].errors()\n",
    "        if Params[\"Stats_only\"] == True:\n",
    "        #As default the errors saved in the files are stat errors, this will change once I properly calculate them\n",
    "            bkg_err_list = bkg_stat_err_list\n",
    "            sig_err = sig_stat_err\n",
    "        elif Params[\"Use_flat_sys_bkg\"] == True:\n",
    "            zero_bins = []\n",
    "            for i,val in enumerate(hist_dict[HNL_mass]['bkg_overlay'].values()):\n",
    "                if val == 0:\n",
    "                    zero_bins.append(i)\n",
    "                    print(f\"{HNL_mass} last bin 0, setting error to 2.0\")\n",
    "            if len(zero_bins) != 0:\n",
    "                bkg_sys_err_list = [hist_dict[HNL_mass]['bkg_overlay'].values()*Params[\"Flat_overlay_bkg_frac\"] + np.ones_like(hist_dict[HNL_mass]['bkg_overlay'].values())*2.0, #This is horrible need to rewrite \n",
    "                                    np.zeros_like(hist_dict[HNL_mass]['bkg_EXT'].errors()), #No systematic error on the EXT sample\n",
    "                                    hist_dict[HNL_mass]['bkg_dirt'].values()*Params[\"Flat_dirt_bkg_frac\"]]\n",
    "            else:    \n",
    "                bkg_sys_err_list = [hist_dict[HNL_mass]['bkg_overlay'].values()*Params[\"Flat_overlay_bkg_frac\"], \n",
    "                                    np.zeros_like(hist_dict[HNL_mass]['bkg_EXT'].errors()), #No systematic error on the EXT sample\n",
    "                                    hist_dict[HNL_mass]['bkg_dirt'].values()*Params[\"Flat_dirt_bkg_frac\"]]\n",
    "            bkg_err_list = [Functions.add_all_errors([bkg_stat_err_list[0],bkg_sys_err_list[0]]), #adding the sys and stat error in quadrature for each bkg type\n",
    "                            Functions.add_all_errors([bkg_stat_err_list[1],bkg_sys_err_list[1]]),\n",
    "                            Functions.add_all_errors([bkg_stat_err_list[2],bkg_sys_err_list[2]])]\n",
    "        elif Params[\"Use_flat_sys_bkg\"] == False:\n",
    "            ppfx_unc = hist_dict[HNL_mass][\"ppfx_uncertainty\"].values()\n",
    "            genie_unc = hist_dict[HNL_mass][\"Genie_uncertainty\"].values()\n",
    "            reint_unc = hist_dict[HNL_mass][\"Reinteraction_uncertainty\"].values()\n",
    "            # detvar_unc = bkg_detvar_dict[HNL_mass][\"Total_DetVar_uncertainty\"].values() #Don't know what this looks like yet, as I haven't made\n",
    "            detvar_unc = hist_dict[HNL_mass]['bkg_overlay'].values()*Params[\"Overlay_detvar_frac\"] #Just setting as flat. Too much variation in samples\n",
    "            tot_overlay_sys = Functions.add_all_errors([ppfx_unc, genie_unc, reint_unc, detvar_unc])\n",
    "            bkg_sys_err_list = [tot_overlay_sys, \n",
    "                                np.zeros_like(hist_dict[HNL_mass]['bkg_EXT'].errors()), #No systematic error on the EXT sample\n",
    "                                hist_dict[HNL_mass]['bkg_dirt'].values()*Params[\"Flat_dirt_bkg_frac\"]] #Don't have reweight or DetVar samples for dirt\n",
    "            bkg_err_list = [Functions.add_all_errors([bkg_stat_err_list[0],bkg_sys_err_list[0]]), #adding the sys and stat error in quadrature for each bkg type\n",
    "                            Functions.add_all_errors([bkg_stat_err_list[1],bkg_sys_err_list[1]]),\n",
    "                            Functions.add_all_errors([bkg_stat_err_list[2],bkg_sys_err_list[2]])]\n",
    "        if (Params[\"Stats_only\"] == False) and (Params[\"Use_flat_sys_signal\"] == True):\n",
    "            zero_bins = []\n",
    "            for i,val in enumerate(hist_dict[HNL_mass]['signal'].values()):\n",
    "                if val == 0:\n",
    "                    zero_bins.append(i)\n",
    "                    print(f\"{HNL_mass} signal last bin 0, setting error to 2.0\")\n",
    "            if len(zero_bins) != 0:\n",
    "                sig_sys_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Flat_sig_frac\"]+2.0\n",
    "            else:\n",
    "                sig_sys_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Flat_sig_frac\"]\n",
    "            sig_err = Functions.add_all_errors([sig_stat_err,sig_sys_err])\n",
    "        if (Params[\"Stats_only\"] == False) and (Params[\"Use_flat_sys_signal\"] == False):\n",
    "            sig_detvar_err = sig_detvar_dict[HNL_mass][\"Total_DetVar_uncertainty\"].values()\n",
    "            sig_flux_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Signal_flux_error\"]\n",
    "            sig_err = Functions.add_all_errors([sig_stat_err,sig_detvar_err,sig_flux_err]) #Adding stat, detvar and flux errors in quadrature\n",
    "        total_bkg_err = Functions.add_all_errors(bkg_err_list) #Now adding the errors of overlay, EXT and dirt in quadrature\n",
    "        BKG_ERR_dict[HNL_mass] = total_bkg_err\n",
    "        SIGNAL_ERR_dict[HNL_mass] = sig_err\n",
    "    return BKG_ERR_dict, SIGNAL_ERR_dict\n",
    "\n",
    "def Uncertainty_breakdown(Params, hist_dict, bkg_reweight_err_dict=None, bkg_detvar_dict=None, sig_detvar_dict=None): #Takes the dictionary of all root files\n",
    "    BKG_ERR_dict, SIGNAL_ERR_dict = {}, {}\n",
    "    for HNL_mass in hist_dict:\n",
    "        bkg_stat_err_list = [hist_dict[HNL_mass]['bkg_overlay'].errors(), \n",
    "                             hist_dict[HNL_mass]['bkg_EXT'].errors(), \n",
    "                             hist_dict[HNL_mass]['bkg_dirt'].errors()]\n",
    "        sig_stat_err = hist_dict[HNL_mass]['signal'].errors()\n",
    "        print(\"Signal stat error:\")\n",
    "        print(sig_stat_err)\n",
    "        if Params[\"Stats_only\"] == True:\n",
    "        #As default the errors saved in the files are stat errors, this will change once I properly calculate them\n",
    "            bkg_err_list = bkg_stat_err_list\n",
    "            sig_err = sig_stat_err\n",
    "        elif Params[\"Use_flat_sys_bkg\"] == True:\n",
    "            zero_bins = []\n",
    "            for i,val in enumerate(hist_dict[HNL_mass]['bkg_overlay'].values()):\n",
    "                if val == 0:\n",
    "                    zero_bins.append(i)\n",
    "                    print(f\"{HNL_mass} last bin 0, setting error to 2.0\")\n",
    "            if len(zero_bins) != 0:\n",
    "                bkg_sys_err_list = [hist_dict[HNL_mass]['bkg_overlay'].values()*Params[\"Flat_overlay_bkg_frac\"] + np.ones_like(hist_dict[HNL_mass]['bkg_overlay'].values())*2.0, #This is horrible need to rewrite \n",
    "                                    np.zeros_like(hist_dict[HNL_mass]['bkg_EXT'].errors()), #No systematic error on the EXT sample\n",
    "                                    hist_dict[HNL_mass]['bkg_dirt'].values()*Params[\"Flat_dirt_bkg_frac\"]]\n",
    "            else:    \n",
    "                bkg_sys_err_list = [hist_dict[HNL_mass]['bkg_overlay'].values()*Params[\"Flat_overlay_bkg_frac\"], \n",
    "                                    np.zeros_like(hist_dict[HNL_mass]['bkg_EXT'].errors()), #No systematic error on the EXT sample\n",
    "                                    hist_dict[HNL_mass]['bkg_dirt'].values()*Params[\"Flat_dirt_bkg_frac\"]]\n",
    "            bkg_err_list = [Functions.add_all_errors([bkg_stat_err_list[0],bkg_sys_err_list[0]]), #adding the sys and stat error in quadrature for each bkg type\n",
    "                            Functions.add_all_errors([bkg_stat_err_list[1],bkg_sys_err_list[1]]),\n",
    "                            Functions.add_all_errors([bkg_stat_err_list[2],bkg_sys_err_list[2]])]\n",
    "        elif Params[\"Use_flat_sys_bkg\"] == False:\n",
    "            ppfx_unc = hist_dict[HNL_mass][\"ppfx_uncertainty\"].values()\n",
    "            genie_unc = hist_dict[HNL_mass][\"Genie_uncertainty\"].values()\n",
    "            reint_unc = hist_dict[HNL_mass][\"Reinteraction_uncertainty\"].values()\n",
    "            # detvar_unc = bkg_detvar_dict[HNL_mass][\"Total_DetVar_uncertainty\"].values() #Don't know what this looks like yet, as I haven't made\n",
    "            detvar_unc = hist_dict[HNL_mass]['bkg_overlay'].values()*Params[\"Overlay_detvar_frac\"] #Just setting as flat. Too much variation in samples\n",
    "            tot_overlay_sys = Functions.add_all_errors([ppfx_unc, genie_unc, reint_unc, detvar_unc])\n",
    "            bkg_sys_err_list = [tot_overlay_sys, \n",
    "                                np.zeros_like(hist_dict[HNL_mass]['bkg_EXT'].errors()), #No systematic error on the EXT sample\n",
    "                                hist_dict[HNL_mass]['bkg_dirt'].values()*Params[\"Flat_dirt_bkg_frac\"]] #Don't have reweight or DetVar samples for dirt\n",
    "            bkg_err_list = [Functions.add_all_errors([bkg_stat_err_list[0],bkg_sys_err_list[0]]), #adding the sys and stat error in quadrature for each bkg type\n",
    "                            Functions.add_all_errors([bkg_stat_err_list[1],bkg_sys_err_list[1]]),\n",
    "                            Functions.add_all_errors([bkg_stat_err_list[2],bkg_sys_err_list[2]])]\n",
    "            bkg_stat_err_total = Functions.add_all_errors([bkg_stat_err_list[0],bkg_stat_err_list[1],bkg_stat_err_list[2]])\n",
    "            print(\"bkg stat error:\")\n",
    "            print(bkg_stat_err_total)\n",
    "            print(\"bkg flux error:\")\n",
    "            print(ppfx_unc)\n",
    "            print(\"bkg genie error:\")\n",
    "            print(genie_unc)\n",
    "            print(\"bkg reint error:\")\n",
    "            print(reint_unc)\n",
    "            \n",
    "        if (Params[\"Stats_only\"] == False) and (Params[\"Use_flat_sys_signal\"] == True):\n",
    "            zero_bins = []\n",
    "            for i,val in enumerate(hist_dict[HNL_mass]['signal'].values()):\n",
    "                if val == 0:\n",
    "                    zero_bins.append(i)\n",
    "                    print(f\"{HNL_mass} signal last bin 0, setting error to 2.0\")\n",
    "            if len(zero_bins) != 0:\n",
    "                sig_sys_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Flat_sig_frac\"]+2.0\n",
    "            else:\n",
    "                sig_sys_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Flat_sig_frac\"]\n",
    "            sig_err = Functions.add_all_errors([sig_stat_err,sig_sys_err])\n",
    "        if (Params[\"Stats_only\"] == False) and (Params[\"Use_flat_sys_signal\"] == False):\n",
    "            sig_detvar_err = sig_detvar_dict[HNL_mass][\"Total_DetVar_uncertainty\"].values()\n",
    "            sig_flux_err = hist_dict[HNL_mass]['signal'].values()*Params[\"Signal_flux_error\"]\n",
    "            sig_err = Functions.add_all_errors([sig_stat_err,sig_detvar_err,sig_flux_err]) #Adding stat, detvar and flux errors in quadrature\n",
    "        total_bkg_err = Functions.add_all_errors(bkg_err_list) #Now adding the errors of overlay, EXT and dirt in quadrature\n",
    "        BKG_ERR_dict[HNL_mass] = total_bkg_err\n",
    "        SIGNAL_ERR_dict[HNL_mass] = sig_err\n",
    "        print(\"Total bkg error:\")\n",
    "        print(total_bkg_err)\n",
    "        print(\"Total signal error:\")\n",
    "        print(sig_err)\n",
    "    return BKG_ERR_dict, SIGNAL_ERR_dict\n",
    "    \n",
    "def Add_bkg_hists_make_signal(hist_dict):\n",
    "    BKG_dict, SIGNAL_dict = {}, {}\n",
    "    for HNL_mass in hist_dict:\n",
    "        bkg_hists = [hist_dict[HNL_mass]['bkg_EXT'], hist_dict[HNL_mass]['bkg_overlay'], hist_dict[HNL_mass]['bkg_dirt']]\n",
    "        \n",
    "        total_bkg = Functions.add_hists_vals(bkg_hists)\n",
    "        BKG_dict[HNL_mass] = total_bkg\n",
    "        SIGNAL_dict[HNL_mass] = hist_dict[HNL_mass]['signal'].values()\n",
    " \n",
    "    return BKG_dict, SIGNAL_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d595061-c00a-43c5-845f-16e9d176918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_dict(Total_dict):\n",
    "    model_dict = {}\n",
    "    \n",
    "    for HNL_mass in Total_dict:\n",
    "        print(HNL_mass)\n",
    "        model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "      \"channels\": [\n",
    "        {\n",
    "          \"name\": \"singlechannel\",\n",
    "          \"samples\": [\n",
    "            {\n",
    "              \"name\": \"signal\",\n",
    "              \"data\": Total_dict[HNL_mass][\"SIGNAL_dict\"],\n",
    "              \"modifiers\": [\n",
    "                {\n",
    "                  \"name\": \"mu\",\n",
    "                  \"type\": \"normfactor\",\n",
    "                  \"data\": None\n",
    "                },\n",
    "                {\n",
    "                  \"name\": \"uncorr_siguncrt\",\n",
    "                  \"type\": \"shapesys\",\n",
    "                  \"data\": Total_dict[HNL_mass][\"SIGNAL_ERR_dict\"]  \n",
    "                }\n",
    "              ]\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"background\",\n",
    "              \"data\": Total_dict[HNL_mass][\"BKG_dict\"],\n",
    "              \"modifiers\": [\n",
    "                {\n",
    "                  \"name\": \"uncorr_bkguncrt\",\n",
    "                  \"type\": \"shapesys\",\n",
    "                  \"data\": Total_dict[HNL_mass][\"BKG_ERR_dict\"]\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "    )\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e370570b-20ae-4a1f-aeda-8a8b2e95ce0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([2, 10, 20, 50, 100, 150, 180, 200, 220, 240, 245])\n"
     ]
    }
   ],
   "source": [
    "R1_BKG_ERR_dict, R1_SIGNAL_ERR_dict = Calculate_total_uncertainty(Params_pyhf, hist_dict_run1)\n",
    "R3_BKG_ERR_dict, R3_SIGNAL_ERR_dict = Calculate_total_uncertainty(Params_pyhf, hist_dict_run3)\n",
    "\n",
    "R1_BKG, R1_SIGNAL = Add_bkg_hists_make_signal(hist_dict_run1)\n",
    "R3_BKG, R3_SIGNAL = Add_bkg_hists_make_signal(hist_dict_run3)\n",
    "\n",
    "R1_output = Functions.Make_into_lists(Params_pyhf, R1_BKG, R1_SIGNAL, R1_BKG_ERR_dict, R1_SIGNAL_ERR_dict)\n",
    "R3_output = Functions.Make_into_lists(Params_pyhf, R3_BKG, R3_SIGNAL, R3_BKG_ERR_dict, R3_SIGNAL_ERR_dict)\n",
    "\n",
    "list_input_dicts = [R1_output, R3_output]\n",
    "# list_input_dicts = [R1_output, R1_output] #Used when I didn't have Run3\n",
    "\n",
    "Total_dict = Functions.Create_final_appended_runs_dict(list_input_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba893d6-5821-489d-ba0a-30c6ab879cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Total_dict[2].keys())\n",
    "# print(Total_dict[2]['BKG_dict'])\n",
    "# print(Total_dict[2]['BKG_ERR_dict'])\n",
    "\n",
    "HNL_mass = 100\n",
    "test_dict = {}\n",
    "test_dict[HNL_mass] = hist_dict_run3[HNL_mass]\n",
    "\n",
    "print(\"Signal:\")\n",
    "print(test_dict[HNL_mass]['signal'].values())\n",
    "print(\"Background:\")\n",
    "print(test_dict[HNL_mass]['bkg_overlay'].values() + test_dict[HNL_mass]['bkg_EXT'].values() + test_dict[HNL_mass]['bkg_dirt'].values())\n",
    "print()\n",
    "print(\"Bkg overlay:\")\n",
    "print(test_dict[HNL_mass]['bkg_overlay'].values())\n",
    "print(\"Bkg EXT:\")\n",
    "print(test_dict[HNL_mass]['bkg_EXT'].values())\n",
    "print(\"Bkg dirt:\")\n",
    "print(test_dict[HNL_mass]['bkg_dirt'].values())\n",
    "print()\n",
    "\n",
    "TEST_BKG_ERR_dict, TEST_SIGNAL_ERR_dict = Uncertainty_breakdown(Params_pyhf, test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a0fd0bb-d3a4-4317-bf1c-66aa8b4191ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = create_model_dict(Total_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68fc82f-8497-4465-88d7-ea486bdb2f3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Testing single point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c533c-cc65-4f3c-8be0-dee7b236807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 10 #Mass point to test\n",
    "DATA_OBS_dict = {}\n",
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "bkg_pars = init_pars.copy()\n",
    "bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass][\"BKG_dict\"]+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])\n",
    "\n",
    "CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"asymptotics\",\n",
    "            )\n",
    "for expected_value, n_sigma in zip(CLs_exp, np.arange(-1, 2)):\n",
    "    print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")\n",
    "    \n",
    "poi_values = np.linspace(0.001, 10, 50)\n",
    "obs_limit_single, exp_limits_single, (scan, results) = pyhf.infer.intervals.upperlimit(DATA_OBS_dict[HNL_mass], \n",
    "                                                                                       model_dict[HNL_mass], poi_values, \n",
    "                                                                                       level=0.1, return_results=True)\n",
    "print(f\"Upper limit {HNL_mass}MeV (obs): μ = {obs_limit_single:.4f}\")\n",
    "print(f\"Upper limit {HNL_mass}MeV (exp): μ = {exp_limits_single[2]:.4f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30a6bc-7a6c-47b4-89df-7688667c8c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = []\n",
    "obs_limit = []\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in [10]:\n",
    "    theta_squared = (theta_dict[HNL_mass])**2\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_single[2])*theta_squared\n",
    "    LIMIT = np.sqrt(obs_limit_single)*theta_squared\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d29e6-f7f4-4088-b9c5-0ccc2fd1f419",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running through all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d15f43-88df-42fe-886b-d3b8ac4a02f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test = Constants.HNL_mass_samples\n",
    "print(list_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ace42d-de08-48dd-8c1a-ed51a6f79b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OBS_dict = {}\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples: #removing the 240MeV point\n",
    "for HNL_mass in list_test:\n",
    "    init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "    model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "    bkg_pars = init_pars.copy()\n",
    "    bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "    model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "    DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass][\"BKG_dict\"]+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "    model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4234fa8-33c9-48dc-9eaa-674262afcba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in list_test:\n",
    "\n",
    "    if Params_pyhf[\"Use_toys\"] == False:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"asymptotics\",\n",
    "            )\n",
    "    if Params_pyhf[\"Use_toys\"] == True:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"toybased\",\n",
    "            ntoys=Params_pyhf[\"Num_toys\"],\n",
    "            track_progress=True,\n",
    "            )\n",
    "    \n",
    "    print(f\"{HNL_mass}MeV\")\n",
    "    for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "        print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2727ad3d-56ab-42bf-ae92-e24e5b50f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "print(\"If the output of the following is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "poi_values = np.linspace(0.001, 2, 100) #I could make a dict of this and have different pois for different mass points\n",
    "print(\"Max value is \" + str(max(poi_values)))\n",
    "print(\"Min value is \" + str(min(poi_values)))\n",
    "print(\"Next value is \" + str(poi_values[1]))\n",
    "print(\"Next value is \" + str(poi_values[2]) + \"\\n\")\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "# list_test = [180, 200, 220, 240, 245]\n",
    "for HNL_mass in list_test:\n",
    "\n",
    "    # poi_values = np.linspace(0.001, 4, 100)\n",
    "    obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "        DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_values, level=0.1, return_results=True\n",
    "    )\n",
    "\n",
    "    print(f\"Upper limit {HNL_mass}MeV (obs): μ = {obs_limit_dict[HNL_mass]:.6f}\")\n",
    "    print(f\"Upper limit {HNL_mass}MeV (exp): μ = {exp_limits_dict[HNL_mass][2]:.6f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0dcf7f-0665-430c-8239-66452b8bf24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_limit = [] #entry 2\n",
    "exp_1sig_up = [] #entry 3\n",
    "exp_1sig_down = [] #entry 1\n",
    "exp_2sig_up = [] #entry 4\n",
    "exp_2sig_down = [] #entry 0\n",
    "obs_limit = []\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in exp_limits_dict:\n",
    "    theta_squared = (theta_dict[HNL_mass])**2\n",
    "    print(theta_squared)\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_dict[HNL_mass][2])*theta_squared\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][0])*theta_squared)\n",
    "    \n",
    "    LIMIT = np.sqrt(obs_limit_dict[HNL_mass])*theta_squared\n",
    "    # if HNL_mass in Constants.Old_generator_mass_points:\n",
    "    #     EXP_LIMIT = EXP_LIMIT*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "    #     LIMIT = LIMIT*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8437737-0007-42bb-9cfa-bd8e2a45194e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Saving limit as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20e73a7-2fee-4aad-9067-e3dd5d7d9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = Constants.HNL_mass_samples\n",
    "# masses = list_test\n",
    "\n",
    "if Params_pyhf[\"Stats_only\"] == True:\n",
    "    stats =  \"Stats_only\"\n",
    "else:\n",
    "    stats = \"Flat_sys\"\n",
    "    \n",
    "if Params_pyhf[\"Use_second_half_only\"] == True:\n",
    "    half_hist = \"havled\"\n",
    "else:\n",
    "    half_hist = \"full_hist\"\n",
    "\n",
    "print(masses)\n",
    "print(exp_limit)\n",
    "\n",
    "r = zip(masses, exp_limit)\n",
    "if Params_pyhf[\"Load_pi0_hists\"] == False:\n",
    "    with open(f'limit_files/My_limits/{stats}_{half_hist}_expected_mu_top_20_logit_FIXED.csv', \"w\") as s:\n",
    "        w = csv.writer(s)\n",
    "        for row in r:\n",
    "            w.writerow(row)\n",
    "            \n",
    "if Params_pyhf[\"Load_pi0_hists\"] == True:\n",
    "    with open(f'limit_files/My_limits/{stats}_{half_hist}_expected_pi0_mu_all_vars_logit_FIXED.csv', \"w\") as s:\n",
    "        w = csv.writer(s)\n",
    "        for row in r:\n",
    "            w.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeac33b-58bc-4054-b1f4-20fe1f20cf3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Testing adjacent models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f79e7cd9-cf28-49f0-a3a0-bef0f63c1cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([20, 100, 50])\n",
      "dict_keys(['BKG_dict', 'BKG_ERR_dict', 'SIGNAL_dict', 'SIGNAL_ERR_dict'])\n"
     ]
    }
   ],
   "source": [
    "adjacent_hist_dict_run1, adjacent_hist_dict_run3 = {}, {}\n",
    "R1_BKG_ERR_dict_ADJ, R3_BKG_ERR_dict_ADJ, R1_SIGNAL_ERR_dict_ADJ, R3_SIGNAL_ERR_dict_ADJ = {}, {}, {}, {}\n",
    "mass_point = 50\n",
    "test_models = [20, 100]\n",
    "for mass_model in test_models:\n",
    "    adjacent_hist_dict_run1[mass_model] = uproot.open(f'bdt_output/adjacent_models/run1_{mass_point}MeV_{mass_model}MeV_model.root')\n",
    "    adjacent_hist_dict_run3[mass_model] = uproot.open(f'bdt_output/adjacent_models/run3_{mass_point}MeV_{mass_model}MeV_model.root')\n",
    "    R1_BKG_ERR_dict_ADJ, R1_SIGNAL_ERR_dict_ADJ = Calculate_total_uncertainty(Params_pyhf, adjacent_hist_dict_run1)\n",
    "    R3_BKG_ERR_dict_ADJ, R3_SIGNAL_ERR_dict_ADJ = Calculate_total_uncertainty(Params_pyhf, adjacent_hist_dict_run3)\n",
    "    # R1_BKG_ERR_dict_ADJ[mass_model]=R1_BKG_ERR_dict[mass_point]\n",
    "    # R3_BKG_ERR_dict_ADJ[mass_model]=R3_BKG_ERR_dict[mass_point]\n",
    "    # R1_SIGNAL_ERR_dict_ADJ[mass_model]=R1_SIGNAL_ERR_dict[mass_point]\n",
    "    # R3_SIGNAL_ERR_dict_ADJ[mass_model]=R3_SIGNAL_ERR_dict[mass_point]\n",
    "adjacent_hist_dict_run1[mass_point] = hist_dict_run1[mass_point]\n",
    "adjacent_hist_dict_run3[mass_point] = hist_dict_run3[mass_point]\n",
    "R1_BKG_ERR_dict_ADJ[mass_point]=R1_BKG_ERR_dict[mass_point]\n",
    "R3_BKG_ERR_dict_ADJ[mass_point]=R3_BKG_ERR_dict[mass_point]\n",
    "R1_SIGNAL_ERR_dict_ADJ[mass_point]=R1_SIGNAL_ERR_dict[mass_point]\n",
    "R3_SIGNAL_ERR_dict_ADJ[mass_point]=R3_SIGNAL_ERR_dict[mass_point]\n",
    "\n",
    "R1_BKG_ADJ, R1_SIGNAL_ADJ = Add_bkg_hists_make_signal(adjacent_hist_dict_run1)\n",
    "R3_BKG_ADJ, R3_SIGNAL_ADJ = Add_bkg_hists_make_signal(adjacent_hist_dict_run3)\n",
    "\n",
    "R1_output_ADJ = Functions.Make_into_lists(Params_pyhf, R1_BKG_ADJ, R1_SIGNAL_ADJ, R1_BKG_ERR_dict_ADJ, R1_SIGNAL_ERR_dict_ADJ)\n",
    "R3_output_ADJ = Functions.Make_into_lists(Params_pyhf, R3_BKG_ADJ, R3_SIGNAL_ADJ, R3_BKG_ERR_dict_ADJ, R3_SIGNAL_ERR_dict_ADJ)\n",
    "\n",
    "list_input_dicts_ADJ = [R1_output_ADJ, R3_output_ADJ]\n",
    "# list_input_dicts = [R1_output, R1_output] #Used when I didn't have Run3\n",
    "\n",
    "Total_dict_ADJ = Functions.Create_final_appended_runs_dict(list_input_dicts_ADJ)\n",
    "print(Total_dict_ADJ[100].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af73b4a8-0dad-4d60-a962-c7155024db95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([2, 10, 20, 50, 100, 150, 180, 200, 220, 240, 245])\n"
     ]
    }
   ],
   "source": [
    "print(Total_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7688a367-1e06-44a8-95df-464d27a37a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "100\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "model_dict_ADJ = create_model_dict(Total_dict_ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95eeb77d-75dc-4454-9988-2c852640c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test = adjacent_hist_dict_run1.keys()\n",
    "DATA_OBS_dict = {}\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples: #removing the 240MeV point\n",
    "for HNL_mass in list_test:\n",
    "    init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "    model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "    bkg_pars = init_pars.copy()\n",
    "    bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "    model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "    DATA_OBS_dict[HNL_mass] = Total_dict[HNL_mass][\"BKG_dict\"]+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "    model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6639d271-7e7c-4cff-9ed6-d20f1f7095cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20MeV\n",
      "Expected CLs(-2 σ): 0.0000\n",
      "Expected CLs(-1 σ): 0.0000\n",
      "Expected CLs( 0 σ): 0.0000\n",
      "Expected CLs( 1 σ): 0.0000\n",
      "Expected CLs( 2 σ): 0.0000\n",
      "100MeV\n",
      "Expected CLs(-2 σ): 0.0001\n",
      "Expected CLs(-1 σ): 0.0014\n",
      "Expected CLs( 0 σ): 0.0121\n",
      "Expected CLs( 1 σ): 0.0780\n",
      "Expected CLs( 2 σ): 0.3124\n",
      "50MeV\n",
      "Expected CLs(-2 σ): 0.0000\n",
      "Expected CLs(-1 σ): 0.0000\n",
      "Expected CLs( 0 σ): 0.0000\n",
      "Expected CLs( 1 σ): 0.0000\n",
      "Expected CLs( 2 σ): 0.0000\n"
     ]
    }
   ],
   "source": [
    "for HNL_mass in list_test:\n",
    "\n",
    "    if Params_pyhf[\"Use_toys\"] == False:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"asymptotics\",\n",
    "            )\n",
    "    if Params_pyhf[\"Use_toys\"] == True:\n",
    "        CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "            1.0,  # null hypothesis\n",
    "            DATA_OBS_dict[HNL_mass],\n",
    "            model_dict[HNL_mass],\n",
    "            test_stat=\"qtilde\",\n",
    "            return_expected_set=True,\n",
    "            calctype=\"toybased\",\n",
    "            ntoys=Params_pyhf[\"Num_toys\"],\n",
    "            track_progress=True,\n",
    "            )\n",
    "    \n",
    "    print(f\"{HNL_mass}MeV\")\n",
    "    for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "        print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "269ab734-f7dd-40bb-8b3a-312eda608e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the output of the following is equal to the lowest or highest value of poi, the range needs to be extended\n",
      "Max value is 2.0\n",
      "Min value is 0.001\n",
      "Next value is 0.021191919191919192\n",
      "Next value is 0.041383838383838384\n",
      "\n",
      "Upper limit 20MeV (obs): μ = 0.087209\n",
      "Upper limit 20MeV (exp): μ = 0.087209\n",
      "\n",
      "Upper limit 100MeV (obs): μ = 0.629815\n",
      "Upper limit 100MeV (exp): μ = 0.629815\n",
      "\n",
      "Upper limit 50MeV (obs): μ = 0.076459\n",
      "Upper limit 50MeV (exp): μ = 0.076459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "print(\"If the output of the following is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "poi_values = np.linspace(0.001, 2, 100) #I could make a dict of this and have different pois for different mass points\n",
    "print(\"Max value is \" + str(max(poi_values)))\n",
    "print(\"Min value is \" + str(min(poi_values)))\n",
    "print(\"Next value is \" + str(poi_values[1]))\n",
    "print(\"Next value is \" + str(poi_values[2]) + \"\\n\")\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "# list_test = [180, 200, 220, 240, 245]\n",
    "for HNL_mass in list_test:\n",
    "\n",
    "    # poi_values = np.linspace(0.001, 4, 100)\n",
    "    obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "        DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_values, level=0.1, return_results=True\n",
    "    )\n",
    "\n",
    "    print(f\"Upper limit {HNL_mass}MeV (obs): μ = {obs_limit_dict[HNL_mass]:.6f}\")\n",
    "    print(f\"Upper limit {HNL_mass}MeV (exp): μ = {exp_limits_dict[HNL_mass][2]:.6f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb324293-40db-4be3-94b0-4498860ed94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5e-05\n",
      "Expected 20MeV limit is 7.3827958770328454e-06\n",
      "Observed 20MeV limit is 7.3827958770328454e-06\n",
      "\n",
      "2.5e-05\n",
      "Expected 100MeV limit is 1.9840221581842897e-05\n",
      "Observed 100MeV limit is 1.9840221581842897e-05\n",
      "\n",
      "2.5e-05\n",
      "Expected 50MeV limit is 6.912810521978472e-06\n",
      "Observed 50MeV limit is 6.912810521978472e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_limit = [] #entry 2\n",
    "exp_1sig_up = [] #entry 3\n",
    "exp_1sig_down = [] #entry 1\n",
    "exp_2sig_up = [] #entry 4\n",
    "exp_2sig_down = [] #entry 0\n",
    "obs_limit = []\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in exp_limits_dict:\n",
    "    theta_squared = (theta_dict[mass_point])**2\n",
    "    print(theta_squared)\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_dict[HNL_mass][2])*theta_squared\n",
    "    exp_1sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][3])*theta_squared)\n",
    "    exp_2sig_up.append(np.sqrt(exp_limits_dict[HNL_mass][4])*theta_squared)\n",
    "    exp_1sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][1])*theta_squared)\n",
    "    exp_2sig_down.append(np.sqrt(exp_limits_dict[HNL_mass][0])*theta_squared)\n",
    "    \n",
    "    LIMIT = np.sqrt(obs_limit_dict[HNL_mass])*theta_squared\n",
    "    # if HNL_mass in Constants.Old_generator_mass_points:\n",
    "    #     EXP_LIMIT = EXP_LIMIT*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "    #     LIMIT = LIMIT*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae2a5ecc-57ca-4f56-bc74-cc00bdadfdb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7284e3cd90>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "points = [6.912810521978472e-06, 7.3827958770328454e-06, 1.9840221581842897e-05]\n",
    "x_points = [50, 50, 50]\n",
    "labels = [\"correct\", \"20MeV\", \"50MeV\"]\n",
    "for i, point in enumerate(points):\n",
    "    plt.plot(x_points[i],point,label=labels[i],marker=\"o\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "73c38a19-4c02-4f26-8aef-5947b26cf4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20MeV model is 0.9363404646583211\n",
      "100MeV model is 0.3484240583434231\n"
     ]
    }
   ],
   "source": [
    "rat_20 = 6.912810521978472e-06/7.3827958770328454e-06\n",
    "rat_100 = 6.912810521978472e-06/1.9840221581842897e-05\n",
    "print(\"20MeV model is \" + str(rat_20))\n",
    "print(\"100MeV model is \" + str(rat_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32c1c24-58ae-4433-8b1c-30f24a80e5b1",
   "metadata": {},
   "source": [
    "## Brazil plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2d76f-c16f-47a0-8c41-3889c5844e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8),facecolor='white',dpi=100)\n",
    "\n",
    "savefig = True\n",
    "# exp_limit = [] #entry 2\n",
    "# exp_1sig_up = [] #entry 3\n",
    "# exp_1sig_down = [] #entry 1\n",
    "# exp_2sig_up = [] #entry 4\n",
    "# exp_2sig_down = [] #entry 0\n",
    "\n",
    "# plt.plot(masses_hnl_val,np.sqrt(np.array(obs_limit)),lw=5,ls='-',color='black',label='Observed')\n",
    "plt.plot(Constants.HNL_mass_samples,np.array(exp_limit),lw=2,ls='--',color='red',label='Expected')\n",
    "plt.fill_between(Constants.HNL_mass_samples,np.array(exp_2sig_down),np.array(exp_2sig_up),color='yellow',label=r'Exp. 2$\\sigma$')\n",
    "plt.fill_between(Constants.HNL_mass_samples,np.array(exp_1sig_down),np.array(exp_1sig_up),color='lightgreen',label=r'Exp. 1$\\sigma$')\n",
    "# plt.fill_between(masses,m2s,p2s,color='lightgreen',label=r'Exp. 2$\\sigma$')\n",
    "# plt.grid(ls='--',color='C7',alpha=0.1)\n",
    "\n",
    "plt.ylabel(r'$|U_{\\mu 4}|^2$ Limit at 90% CL',fontsize=20)\n",
    "plt.xlabel('HNL Mass [MeV]',fontsize=20)\n",
    "# plt.title(\"HNL Mixing Upper Limit\")\n",
    "plt.ylim(1e-8,1e-3)\n",
    "plt.legend(loc=\"lower left\",ncol=2,frameon=False,fontsize=20)\n",
    "# plt.text(0.99,0.95,r'MicroBooNE NuMI POT:$7.01\\times 10^{20}$',fontsize=20,color='black',alpha=1,verticalalignment='top',horizontalalignment='right',transform=plt.gca().transAxes)\n",
    "plt.tick_params(axis='x', labelsize=20)\n",
    "plt.tick_params(axis='y', labelsize=20)\n",
    "plt.xlim(0,250)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "\n",
    "if savefig == True:\n",
    "    plt.savefig('plots/Limits/Brazil_preliminary.pdf',bbox_inches='tight', pad_inches=0.3)\n",
    "\n",
    "plt.grid(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf87921-c9cd-4488-95f0-b359183879ca",
   "metadata": {},
   "source": [
    "## End of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814efb9e-a208-4b77-87ec-8e9b203f45b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_vals = input(\"Do you want to run with new pois? y/n \")\n",
    "if print_vals == \"y\":\n",
    "    poi_new_dict = {}\n",
    "    for HNL_mass in list_test:\n",
    "        best_fit_value = exp_limits_dict[HNL_mass][2]\n",
    "        new_poi_array = np.linspace(0.001, best_fit_value*2, 100)\n",
    "        poi_new_dict[HNL_mass] = new_poi_array\n",
    "\n",
    "    obs_limit_dict = {}\n",
    "    exp_limits_dict = {}\n",
    "    print(\"If the output of the following is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "    # for HNL_mass in Constants.HNL_mass_samples:\n",
    "    for HNL_mass in list_test:\n",
    "\n",
    "        obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "            DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_new_dict[HNL_mass], level=0.1, return_results=True\n",
    "        )\n",
    "\n",
    "        print(f\"Upper limit {HNL_mass}MeV (obs): μ = {obs_limit_dict[HNL_mass]:.4f}\")\n",
    "        print(f\"Upper limit {HNL_mass}MeV (exp): μ = {exp_limits_dict[HNL_mass][2]:.4f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a259396-713f-4435-99e8-870067d7c520",
   "metadata": {},
   "source": [
    "## Finished code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1787c6f6-155b-472d-8587-627baeaa4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def New_BR_limit(Old_theta_squared, HNL_mass):\n",
    "#     New_limit = Old_theta_squared*np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "#     print(\"Old limit was \" + str(Old_theta_squared))\n",
    "#     print(\"New limit is \" + str(New_limit))\n",
    "#     return New_limit\n",
    "\n",
    "# Old_limits = {20:0.00015681119188699395,\n",
    "#               50:9.92373190724973e-06,\n",
    "#               100:9.84200063077575e-07,\n",
    "#               150:1.2830647341135436e-07,\n",
    "#               180:7.604092480424186e-08}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15b7ad-a343-44ab-8045-dbd8ee626630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New_limits = []\n",
    "# for HNL_mass in list_test:\n",
    "#     New_limits.append(New_BR_limit(Old_limits[HNL_mass], HNL_mass))\n",
    "    \n",
    "# r = zip([20,50,100,150,180], New_limits)\n",
    "\n",
    "# with open(f'limit_files/New_BR_expected_mu.csv', \"w\") as s:\n",
    "#     w = csv.writer(s)\n",
    "#     for row in r:\n",
    "#         w.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656fea3-8b53-45fc-8cc6-914f6b9f8575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "#     new_scaling = np.sqrt(1/Constants.Old_gen_HNL_scalings[HNL_mass])\n",
    "#     print(\"New scaling is \" + str(new_scaling))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d35cf-1a71-42a7-ba8a-06a8fe0c30b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Total_dict[20][\"SIGNAL_ERR_dict\"])\n",
    "\n",
    "#print(Total_dict[20][\"SIGNAL_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5fe72-6da3-4db9-af90-38ee337aec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SIGNAL_dict = {}\n",
    "TOTAL_SIGNAL_ERR_dict = {}\n",
    "TOTAL_BKG_dict = {}\n",
    "TOTAL_BKG_ERR_dict = {}\n",
    "TOTAL_DATA_dict = {}\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    \n",
    "    r1_signal = hist_dict_run1[HNL_mass]['signal']\n",
    "    r1_EXT = hist_dict_run1[HNL_mass]['bkg_EXT']\n",
    "    r1_nu = hist_dict_run1[HNL_mass]['bkg_overlay']\n",
    "    r1_dirt = hist_dict_run1[HNL_mass]['bkg_dirt']\n",
    "    r1_data = hist_dict_run1[HNL_mass]['data']\n",
    "    \n",
    "    r3_signal = hist_dict_run3[HNL_mass]['signal']\n",
    "    r3_EXT = hist_dict_run3[HNL_mass]['bkg_EXT']\n",
    "    r3_nu = hist_dict_run3[HNL_mass]['bkg_overlay']\n",
    "    r3_dirt = hist_dict_run3[HNL_mass]['bkg_dirt']\n",
    "    r3_data = hist_dict_run3[HNL_mass]['data']\n",
    "    \n",
    "    r1_bkg_hists = [r1_EXT, r1_nu, r1_dirt]\n",
    "    r1_total_bkg = add_hists_vals(r1_bkg_hists)\n",
    "    \n",
    "    r3_bkg_hists = [r3_EXT, r3_nu, r3_dirt]\n",
    "    r3_total_bkg = add_hists_vals(r3_bkg_hists)\n",
    "    \n",
    "    if Params_pyhf[\"Stats_only\"] == True:\n",
    "        overlay_r1_err = get_stat_errors(r1_nu)\n",
    "        dirt_r1_err = get_stat_errors(r1_dirt)\n",
    "        \n",
    "        overlay_r3_err = get_stat_errors(r3_nu)\n",
    "        dirt_r3_err = get_stat_errors(r3_dirt)\n",
    "        \n",
    "        r1_sig_err = get_stat_errors(r1_signal)\n",
    "        r3_sig_err = get_stat_errors(r3_signal)\n",
    "        \n",
    "    elif Params_pyhf[\"Stats_only\"] == False:\n",
    "        overlay_r1_err = get_full_errors_nu_FLAT_INPUTS(r1_nu)\n",
    "        dirt_r1_err = get_full_errors_dirt(r1_dirt)\n",
    "      \n",
    "        overlay_r3_err = get_full_errors_nu_FLAT_INPUTS(r3_nu)\n",
    "        dirt_r3_err = get_full_errors_dirt(r3_dirt)\n",
    "        \n",
    "        r1_sig_err = get_full_errors_signal(r1_signal)\n",
    "        r3_sig_err = get_full_errors_signal(r3_signal)\n",
    "    \n",
    "    \n",
    "    r1_bkg_err_list = [overlay_r1_err, r1_EXT.errors(), dirt_r1_err]\n",
    "    r1_total_bkg_err = add_all_errors(r1_bkg_err_list)\n",
    "\n",
    "    r3_bkg_err_list = [overlay_r3_err, r3_EXT.errors(), dirt_r3_err]\n",
    "    r3_total_bkg_err = add_all_errors(r3_bkg_err_list)\n",
    "    \n",
    "    #Converting np.ndarrays to lists\n",
    "    SIGNAL_R1 = np.ndarray.tolist(r1_signal.values())\n",
    "    SIGNAL_ERR_R1 = np.ndarray.tolist(r1_sig_err)\n",
    "    BKG_R1 = np.ndarray.tolist(r1_total_bkg)\n",
    "    BKG_ERR_R1 = np.ndarray.tolist(r1_total_bkg_err)\n",
    "    DATA_R1 = np.ndarray.tolist(r1_data.values())\n",
    "    \n",
    "    SIGNAL_R3 = np.ndarray.tolist(r3_signal.values())\n",
    "    SIGNAL_ERR_R3 = np.ndarray.tolist(r3_sig_err)\n",
    "    BKG_R3 = np.ndarray.tolist(r3_total_bkg)\n",
    "    BKG_ERR_R3 = np.ndarray.tolist(r3_total_bkg_err)\n",
    "    DATA_R3 = np.ndarray.tolist(r3_data.values())\n",
    "    \n",
    "    list_of_lists = [SIGNAL_R1, SIGNAL_ERR_R1, BKG_R1, BKG_ERR_R1, DATA_R1, SIGNAL_R3, SIGNAL_ERR_R3, BKG_R3, BKG_ERR_R3, DATA_R3]\n",
    "    if Params_pyhf[\"Use_second_half_only\"] == True:\n",
    "        for n in range(len(list_of_lists)):\n",
    "            list_of_lists[n]=remove_first_half_hist(list_of_lists[n])\n",
    "    \n",
    "    TOTAL_SIGNAL_dict[HNL_mass] = append_r3_to_r1(list_of_lists[0], list_of_lists[5])\n",
    "    TOTAL_SIGNAL_ERR_dict[HNL_mass] = append_r3_to_r1(list_of_lists[1], list_of_lists[6])\n",
    "    TOTAL_BKG_dict[HNL_mass] = append_r3_to_r1(list_of_lists[2], list_of_lists[7])\n",
    "    TOTAL_BKG_ERR_dict[HNL_mass] = append_r3_to_r1(list_of_lists[3], list_of_lists[8])\n",
    "    TOTAL_DATA_dict[HNL_mass] = append_r3_to_r1(list_of_lists[4], list_of_lists[9])\n",
    "    \n",
    "    # print(f\"Total {HNL_mass}MeV signal is \")\n",
    "    # print(SIGNAL_R1)\n",
    "\n",
    "# print()\n",
    "# print(\"Total bkg is \")\n",
    "# print(r1_total_bkg)\n",
    "\n",
    "# print()\n",
    "# print(\"Total bkg error is  \")\n",
    "# print(r1_total_bkg_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce615d-8780-424f-9372-21a3c27a84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "# for HNL_mass in HNL_masses:\n",
    "#     model_dict[HNL_mass] = pyhf.simplemodels.uncorrelated_background(signal=TOTAL_SIGNAL_dict[HNL_mass],\n",
    "#                                                                      bkg=TOTAL_BKG_dict[HNL_mass], \n",
    "#                                                                      bkg_uncertainty=TOTAL_BKG_ERR_dict[HNL_mass])\n",
    "    \n",
    "for HNL_mass in HNL_masses:\n",
    "    model_dict[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "  \"channels\": [\n",
    "    {\n",
    "      \"name\": \"singlechannel\",\n",
    "      \"samples\": [\n",
    "        {\n",
    "          \"name\": \"signal\",\n",
    "          \"data\": TOTAL_SIGNAL_dict[HNL_mass],\n",
    "          \"modifiers\": [\n",
    "            {\n",
    "              \"name\": \"mu\",\n",
    "              \"type\": \"normfactor\",\n",
    "              \"data\": None\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"uncorr_siguncrt\",\n",
    "              \"type\": \"shapesys\",\n",
    "              \"data\": TOTAL_SIGNAL_ERR_dict[HNL_mass]  \n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"background\",\n",
    "          \"data\": TOTAL_BKG_dict[HNL_mass],\n",
    "          \"modifiers\": [\n",
    "            {\n",
    "              \"name\": \"uncorr_bkguncrt\",\n",
    "              \"type\": \"shapesys\",\n",
    "              \"data\": TOTAL_BKG_ERR_dict[HNL_mass]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e2625-7dc1-486d-9d33-a3c051f0584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_full = pyhf.model.uncorrelated_background(signal=TOTAL_SIGNAL, signal_uncertainty=TOTAL_SIGNAL_ERR, bkg=TOTAL_BKG, bkg_uncertainty=TOTAL_BKG_ERR)\n",
    "# model_full\n",
    "# print(json.dumps(model.spec, indent=2))\n",
    "# model.config.param_set(\"uncorr_bkguncrt\").n_parameters\n",
    "#model.config.param_set(\"uncorr_siguncrt\").n_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3cf9e-299d-42f0-afca-e541978bb7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(json.dumps(model.spec, indent=2))\n",
    "DATA_OBS_dict = {}\n",
    "for HNL_mass in HNL_masses:\n",
    "    init_pars = model_dict[HNL_mass].config.suggested_init()\n",
    "    model_dict[HNL_mass].expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "    bkg_pars = init_pars.copy()\n",
    "    bkg_pars[model_dict[HNL_mass].config.poi_index] = 0\n",
    "    model_dict[HNL_mass].expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "    DATA_OBS_dict[HNL_mass] = TOTAL_DATA_dict[HNL_mass]+model_dict[HNL_mass].config.auxdata\n",
    "\n",
    "    model_dict[HNL_mass].logpdf(pars=bkg_pars, data=DATA_OBS_dict[HNL_mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a5f893-b271-4da1-bf61-f16bd3fb3e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyhf.infer.mle.fit(data=DATA_OBS, pdf=model)\n",
    "for HNL_mass in HNL_masses:\n",
    "    CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "        1.0,  # null hypothesis\n",
    "        DATA_OBS_dict[HNL_mass],\n",
    "        model_dict[HNL_mass],\n",
    "        test_stat=\"qtilde\",\n",
    "        return_expected_set=True,\n",
    "        )\n",
    "    \n",
    "#print(f\"      Observed CLs: {CLs_obs:.4f}\")\n",
    "    print(f\"{HNL_mass}MeV\")\n",
    "    for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "        print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433df84e-f04e-45cf-9a8f-84f7cf889b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_limit_dict = {}\n",
    "exp_limits_dict = {}\n",
    "print(\"If the output of the following is equal to the lowest or highest value of poi, the range needs to be extended\")\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "\n",
    "    poi_values = np.linspace(0.001, 10, 100)\n",
    "    obs_limit_dict[HNL_mass], exp_limits_dict[HNL_mass], (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "        DATA_OBS_dict[HNL_mass], model_dict[HNL_mass], poi_values, level=0.1, return_results=True\n",
    "    )\n",
    "    print(f\"Upper limit {HNL_mass}MeV (obs): μ = {obs_limit_dict[HNL_mass]:.4f}\")\n",
    "    print(f\"Upper limit {HNL_mass}MeV (exp): μ = {exp_limits_dict[HNL_mass][2]:.4f}\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5451d-aad4-4dfb-87e6-fd111ea3bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bdt_output/new_theta_dict.pkl', 'rb') as handle:\n",
    "    new_theta_dict = pickle.load(handle)\n",
    "print(new_theta_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e89ee7-9741-419d-8dfa-faee7b318a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mass point\n",
    "with open('bdt_output/new_theta_dict.pkl', 'rb') as handle:\n",
    "    new_theta_dict = pickle.load(handle)\n",
    "#print(new_theta_dict)\n",
    "scaled_thetas = new_theta_dict #Saved in 3.5_BDT_Result\n",
    "\n",
    "exp_limit = []\n",
    "obs_limit = []\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    theta_squared = (scaled_thetas[HNL_mass])**2\n",
    "\n",
    "    EXP_LIMIT = np.sqrt(exp_limits_dict[HNL_mass][2])*theta_squared\n",
    "    LIMIT = np.sqrt(obs_limit_dict[HNL_mass])*theta_squared\n",
    "    print(f\"Expected {HNL_mass}MeV limit is \" + str(EXP_LIMIT))\n",
    "    print(f\"Observed {HNL_mass}MeV limit is \" + str(LIMIT)+ \"\\n\")\n",
    "    \n",
    "    exp_limit.append(EXP_LIMIT)\n",
    "# print()\n",
    "# print(\"Owen's expected limit is \" + str(Owen_exp_limit))\n",
    "# print(\"Owen's observed limit is \" + str(Owen_obs_limit))\n",
    "\n",
    "# print()\n",
    "# perc_diff_exp = (1-(EXP_LIMIT/Owen_exp_limit))*100\n",
    "# perc_diff_obs = (1-(LIMIT/Owen_obs_limit))*100\n",
    "\n",
    "# print(\"pyhf expected limit is \" + str(perc_diff_exp) + \" different from Owen's limit.\")\n",
    "# print(\"pyhf observed limit is \" + str(perc_diff_obs) + \" different from Owen's limit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69557119-e9c6-430c-a6b5-b6ddaea4613f",
   "metadata": {},
   "source": [
    "## Saving Limits as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197c589-b82b-4105-be66-ec902f4ffafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "masses = HNL_masses\n",
    "\n",
    "if Params_pyhf[\"Stats_only\"] == True:\n",
    "    stats =  \"Stats_only\"\n",
    "else:\n",
    "    stats = \"Owen_sys\"\n",
    "    \n",
    "if Params_pyhf[\"Use_second_half_only\"] == True:\n",
    "    half_hist = \"havled\"\n",
    "else:\n",
    "    half_hist = \"full_hist\"\n",
    "\n",
    "print(masses)\n",
    "print(exp_limit)\n",
    "\n",
    "r = zip(masses, exp_limit)\n",
    "\n",
    "with open(f'limit_files/{stats}_{half_hist}_expected_mu_COMBINED_highest_E.csv', \"w\") as s:\n",
    "    w = csv.writer(s)\n",
    "    for row in r:\n",
    "        w.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1405e88b-4862-4f4f-9aae-df0dc1924ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc7172-6236-477f-9d5c-1a50e81519ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 100\n",
    "print(json.dumps(model_dict[HNL_mass].spec, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01974e7f-3d17-4287-ba74-5328c603a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_full_sys = {}\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    model_dict_full_sys[HNL_mass] = pyhf.Model(\n",
    "        {\n",
    "  \"channels\": [\n",
    "    {\n",
    "      \"name\": \"singlechannel\",\n",
    "      \"samples\": [\n",
    "        {\n",
    "          \"name\": \"signal\",\n",
    "          \"data\": TOTAL_SIGNAL_dict[HNL_mass],\n",
    "          \"modifiers\": [\n",
    "            {\n",
    "              \"name\": \"mu\",\n",
    "              \"type\": \"normfactor\",\n",
    "              \"data\": None\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"uncorr_siguncrt\",\n",
    "              \"type\": \"shapesys\",\n",
    "              \"data\": TOTAL_SIGNAL_ERR_dict[HNL_mass]  \n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"background\",\n",
    "          \"data\": TOTAL_BKG_dict[HNL_mass],\n",
    "          \"modifiers\": [\n",
    "            {\n",
    "              \"name\": \"uncorr_bkguncrt\",\n",
    "              \"type\": \"shapesys\",\n",
    "              \"data\": TOTAL_BKG_ERR_dict[HNL_mass]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416cdf2-0f11-4ae6-ac1b-9260325f2396",
   "metadata": {},
   "source": [
    "# Adding in signal systematic uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d3408-b6f7-4cc9-bad2-86f8bb48220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Messing around with model\n",
    "\n",
    "full_model = pyhf.Model(\n",
    "    {\n",
    "  \"channels\": [\n",
    "    {\n",
    "      \"name\": \"singlechannel\",\n",
    "      \"samples\": [\n",
    "        {\n",
    "          \"name\": \"signal\",\n",
    "          \"data\": [\n",
    "            0.4354482889175415,\n",
    "            0.6531724333763123,\n",
    "            0.9367626905441284,\n",
    "            1.1947383880615234,\n",
    "            1.3539148569107056,\n",
    "            1.192908763885498,\n",
    "            0.6879351139068604,\n",
    "            0.2671237289905548,\n",
    "            0.8156560063362122,\n",
    "            1.649437665939331,\n",
    "            2.6418192386627197,\n",
    "            3.511852264404297,\n",
    "            3.4166924953460693,\n",
    "            2.6418192386627197,\n",
    "            1.114729881286621,\n",
    "            0.842844545841217\n",
    "          ],\n",
    "          \"modifiers\": [\n",
    "            {\n",
    "              \"name\": \"mu\",\n",
    "              \"type\": \"normfactor\",\n",
    "              \"data\": None\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"uncorr_siguncrt\",\n",
    "              \"type\": \"shapesys\",\n",
    "              \"data\": TOTAL_SIGNAL_ERR  \n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"background\",\n",
    "          \"data\": [\n",
    "            227.98190307617188,\n",
    "            185.65267944335938,\n",
    "            141.53671264648438,\n",
    "            83.10063171386719,\n",
    "            39.49835968017578,\n",
    "            20.065095901489258,\n",
    "            5.26054573059082,\n",
    "            0.7651026844978333,\n",
    "            385.53765869140625,\n",
    "            330.3393249511719,\n",
    "            241.39376831054688,\n",
    "            143.0430908203125,\n",
    "            55.337371826171875,\n",
    "            20.656126022338867,\n",
    "            7.634726524353027,\n",
    "            3.049088954925537\n",
    "          ],\n",
    "          \"modifiers\": [\n",
    "            {\n",
    "              \"name\": \"uncorr_bkguncrt\",\n",
    "              \"type\": \"shapesys\",\n",
    "              \"data\": [\n",
    "                34.158817291259766,\n",
    "                27.066844940185547,\n",
    "                22.60236358642578,\n",
    "                14.79345417022705,\n",
    "                6.955612659454346,\n",
    "                4.61644983291626,\n",
    "                1.6257153749465942,\n",
    "                0.48608535528182983,\n",
    "                73.31805419921875,\n",
    "                65.45207214355469,\n",
    "                51.50766372680664,\n",
    "                34.320030212402344,\n",
    "                10.886519432067871,\n",
    "                5.264797210693359,\n",
    "                2.1698012351989746,\n",
    "                1.1060731410980225\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6192c9b-f9ee-4407-be07-9f98aa29e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(json.dumps(full_model.spec, indent=2))\n",
    "init_pars = full_model.config.suggested_init()\n",
    "full_model.expected_actualdata(init_pars) #signal plus bkg\n",
    "\n",
    "bkg_pars = init_pars.copy()\n",
    "bkg_pars[model.config.poi_index] = 0\n",
    "full_model.expected_actualdata(bkg_pars) #bkg only\n",
    "\n",
    "DATA_OBS = TOTAL_DATA+full_model.config.auxdata\n",
    "\n",
    "full_model.logpdf(pars=bkg_pars, data=DATA_OBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01a2b1-3aa2-4348-9804-04c1f5937e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "    1.0,  # null hypothesis\n",
    "    DATA_OBS,\n",
    "    full_model,\n",
    "    test_stat=\"qtilde\",\n",
    "    return_expected_set=True,\n",
    ")\n",
    "print(f\"      Observed CLs: {CLs_obs:.4f}\")\n",
    "for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "    print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eddbe7e-3de0-4d2a-8729-49308c8ec4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_values = np.linspace(0.1, 10, 50)\n",
    "obs_limit, exp_limits, (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "    DATA_OBS, full_model, poi_values, level=0.1, return_results=True\n",
    ")\n",
    "print(f\"Upper limit (obs): μ = {obs_limit:.4f}\")\n",
    "print(f\"Upper limit (exp): μ = {exp_limits[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb56c7e-87ee-4eb0-8390-0ef5904cddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mass point\n",
    "EXP_LIMIT = np.sqrt(exp_limits[2])*theta_squared\n",
    "LIMIT = np.sqrt(obs_limit)*theta_squared\n",
    "print(\"Expected limit is \" + str(EXP_LIMIT))\n",
    "print(\"Observed limit is \" + str(LIMIT))\n",
    "print()\n",
    "print(\"Owen's expected limit is \" + str(Owen_exp_limit))\n",
    "print(\"Owen's observed limit is \" + str(Owen_obs_limit))\n",
    "\n",
    "print()\n",
    "perc_diff_exp = (1-(EXP_LIMIT/Owen_exp_limit))*100\n",
    "perc_diff_obs = (1-(LIMIT/Owen_obs_limit))*100\n",
    "\n",
    "# perc_diff_exp = (1-(Owen_exp_limit/EXP_LIMIT))*100\n",
    "# perc_diff_obs = (1-(Owen_obs_limit/LIMIT))*100\n",
    "\n",
    "print(\"pyhf expected limit is \" + str(perc_diff_exp) + \"% different from Owen's limit.\")\n",
    "print(\"pyhf observed limit is \" + str(perc_diff_obs) + \"% different from Owen's limit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608e383-7ca2-43da-99f1-4b5660318ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
