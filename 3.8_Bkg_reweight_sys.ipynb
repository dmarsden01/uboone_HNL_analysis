{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced5e536-c413-4447-b2d4-8b0ed1c50021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "import os,sys,string, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from platform import python_version\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import math\n",
    "from matplotlib.patches import Rectangle\n",
    "import pickle\n",
    "\n",
    "import Utilities.Plotter as PT\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Variables_list as Variables\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print ('Success')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce878578-1431-44ef-937e-f139e58caf9b",
   "metadata": {},
   "source": [
    "# Loading in the \"results\" dataframe after full selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a5f612-8b8c-427d-9201-adbecb1b2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = {\"Run\":\"run3\", #The run number, so far either \"run1\" or \"run3\"\n",
    "          \"Load_lepton_signal\":False, #Load ee BDTs\n",
    "          \"Load_pi0_signal\":True, #Load pi0 BDTs\n",
    "          \"Use_logit\":True} \n",
    "\n",
    "Run = Params[\"Run\"] #so far either \"run1\" or \"run3\"\n",
    "\n",
    "if Params[\"Load_lepton_signal\"] == True:\n",
    "    HNL_masses = Constants.HNL_mass_samples\n",
    "if Params[\"Load_pi0_signal\"] == True:\n",
    "    HNL_masses = Constants.HNL_mass_pi0_samples\n",
    "\n",
    "loc_pkl = f'pkl_files/{Run}/current_files/Results/'\n",
    "if Params[\"Load_pi0_signal\"] == True:loc_pkl+=\"pi0/\"\n",
    "\n",
    "filename = \"_EXT_full_Finished\"\n",
    "\n",
    "overlay_results = pd.read_pickle(loc_pkl+f\"overlay_results{filename}.pkl\") #This will contain all of the BDT output scores and rse_id\n",
    "\n",
    "loc_hists = 'bdt_output/'\n",
    "if Params[\"Load_pi0_signal\"] == True:loc_hists+=\"pi0/\"\n",
    "\n",
    "name_type = Functions.Get_signal_name_type(Params)\n",
    "root_filename = name_type+filename\n",
    "bins_dict = {}\n",
    "for HNL_mass in HNL_masses:\n",
    "    hist_placeholder = uproot.open(loc_hists+f'{Run}_{HNL_mass}_'+root_filename+\".root\")\n",
    "    bins_dict[HNL_mass] = hist_placeholder['bkg_overlay'].to_numpy()[1] #A tuple of bin edges\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3494dd8-3e2d-4d54-82f2-db5043f566e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## QUICK (not actually calculating to save time, but setting each to ~20% flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27bc9ea5-a3d8-49ee-81ff-b1ce6ae93441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "loc_hists = \"Uncertainties/\"\n",
    "if Params[\"Load_pi0_signal\"] == True:loc_hists+=\"pi0/\"\n",
    "\n",
    "name_type = Functions.Get_signal_name_type(Params)\n",
    "filename = name_type+'_EXT_full_Finished.root'\n",
    "\n",
    "frac_ppfx_flat = 0.2\n",
    "frac_genie_flat = 0.2\n",
    "frac_reweight_flat = 0.05\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    bins_cent=(bins_dict[HNL_mass][:-1]+bins_dict[HNL_mass][1:])/2\n",
    "    \n",
    "    score_hist = np.histogram(Functions.logit(overlay_results[f\"BDT_output_{HNL_mass}MeV\"]), weights=overlay_results[f\"weight\"], bins=bins_dict[HNL_mass], \n",
    "                              range=[bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1]])[0]\n",
    "\n",
    "    values_dict = {'ppfx_uncertainty': score_hist*frac_ppfx_flat, 'Genie_uncertainty':score_hist*frac_genie_flat,\n",
    "                   'Reinteraction_uncertainty': score_hist*frac_reweight_flat,\n",
    "                   'ppfx_uncertainty_frac': np.ones(len(score_hist))*frac_ppfx_flat, \n",
    "                   'Genie_uncertainty_frac':np.ones(len(score_hist))*frac_genie_flat,\n",
    "                   'Reinteraction_uncertainty_frac':np.ones(len(score_hist))*frac_reweight_flat} \n",
    "    hist_samples = {}\n",
    "\n",
    "    #make array with all values 1, then weight by value\n",
    "    for name in values_dict:\n",
    "        test_hist = np.histogram(bins_cent, weights=values_dict[name], bins=bins_dict[HNL_mass], \n",
    "                                 range=[bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1]])\n",
    "        hist_samples[name] = test_hist\n",
    "\n",
    "    stop_writing = False\n",
    "    dont_save = []\n",
    "    with uproot.open(loc_hists+Params[\"Run\"]+f'_{HNL_mass}_'+filename) as file:\n",
    "    # with uproot.open(loc_hists+f'{Run}_{HNL_mass}MeV_{filename}.root') as file: #Check what is already in the file (read-only)\n",
    "        for name in hist_samples:\n",
    "            if str(name)+\";1\" in file.keys():\n",
    "                # print(str(name) + f\" ALREADY EXISTS in {HNL_mass}MeV file, not re-saving\")\n",
    "                dont_save.append(name)\n",
    "\n",
    "    with uproot.update(loc_hists+f'{Run}_{HNL_mass}_{filename}') as file: #Add new hists into the file\n",
    "        # del file['ppfx_uncertainty']\n",
    "\n",
    "        for name in hist_samples:\n",
    "            # if stop_writing == True:\n",
    "            #     print(\"Not saving hist\")\n",
    "            #     break\n",
    "            if name in dont_save:\n",
    "                # print(f\"Not saving {name}\")\n",
    "                continue\n",
    "            else:\n",
    "                file[name] = hist_samples[name]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff28413-ae20-47cc-9464-4c260937d3e5",
   "metadata": {},
   "source": [
    "# Reading in the overlay .root file with reweight branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4c5de-0103-44bf-8404-7463d7a77ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_variables = Variables.sys_vars + Variables.event_vars + Variables.weight_related\n",
    "\n",
    "if Run == \"run1\":\n",
    "    NuMI_MC_overlay = uproot3.open('../NuMI_MC/SLIMMED_neutrinoselection_filt_run1_overlay.root')['nuselection/NeutrinoSelectionFilter']\n",
    "    Norm = Constants.SF_overlay_run1\n",
    "elif Run == \"run3\":\n",
    "    NuMI_MC_overlay = uproot3.open('../NuMI_MC/SLIMMED_neutrinoselection_filt_run3_overlay.root')['nuselection/NeutrinoSelectionFilter']\n",
    "    Norm = Constants.SF_overlay_run3\n",
    "\n",
    "df_overlay_weights = NuMI_MC_overlay.pandas.df(sys_variables, flatten=False) #Perhaps I can do this in a more clever way than just making a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c9948-3875-445d-aaaf-9c4e881d2969",
   "metadata": {},
   "source": [
    "# Keeping only the events which pass selection in the weight dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885864f-01e9-48e0-b041-1407ba8a5a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_unique_ev_id(df): #df must have 'run', 'sub' and 'evt' branches\n",
    "    if pd.Series(['run', 'sub', 'evt']).isin(df.columns).all():\n",
    "        rse_list = []\n",
    "        for entry in df.index: #Looping over all events in the dataframe\n",
    "            rse = str(df['run'][entry]) + \"_\" + str(df['sub'][entry]) + \"_\" + str(df['evt'][entry])\n",
    "            rse_list.append(rse)\n",
    "        df['rse_id'] = rse_list #Writing a new branch with the unique event id\n",
    "        return df.copy()\n",
    "    else:\n",
    "        print(\"Dataframe needs \\\"run\\\", \\\"sub\\\" and \\\"evt\\\" columns.\")\n",
    "        return 0\n",
    "    \n",
    "def check_duplicate_events(df):\n",
    "    rse_list = df['rse_id'].to_list()\n",
    "\n",
    "    seen = set()\n",
    "    dupes = []\n",
    "\n",
    "    for x in rse_list:\n",
    "        if x in seen:\n",
    "            dupes.append(x)\n",
    "        else:\n",
    "            seen.add(x)\n",
    "    print(\"Number of duplicates is \" + str(len(dupes)))\n",
    "    print(\"Number of unique events is \" + str(len(seen)))\n",
    "\n",
    "overlay_results_rse = make_unique_ev_id(overlay_results)\n",
    "df_overlay_weights_rse = make_unique_ev_id(df_overlay_weights)\n",
    "\n",
    "#Deleting any duplicates of events, should be able to avoid if correctly filtered for one event per row\n",
    "overlay_results_rse.drop_duplicates(subset=['rse_id'], keep='first', inplace=True)\n",
    "\n",
    "print(\"Number of events in weights file is \" + str(len(df_overlay_weights_rse)))\n",
    "print(\"Number of events in results file is \" + str(len(overlay_results_rse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00863fdf-66f9-4e0e-9e57-6e3db102152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeping only those events in the final selection\n",
    "filtered_weights = df_overlay_weights_rse.loc[(df_overlay_weights_rse['rse_id'].isin(overlay_results_rse['rse_id']))]\n",
    "\n",
    "print(\"Number of events in the filtered weights file is \" + str(len(filtered_weights)))\n",
    "print(\"Number of events in results file is \" + str(len(overlay_results_rse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9a5f9-bf48-45d7-a35b-909d3bc5eb88",
   "metadata": {},
   "source": [
    "## Calculating uncertainty for a BDT input variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4a8c2-831d-41ae-a044-cede9b82ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_weights.keys())\n",
    "print(overlay_results_rse.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c5f798-6355-4175-a963-48924bdcc2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BDT_score_list = []\n",
    "\n",
    "# HNL_masses = [20, 50, 100, 150, 180, 200] #Should get rid of this once made overlay branches with all results\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    BDT_score_list.append(f'BDT_output_{HNL_mass}MeV')\n",
    "\n",
    "just_score_df = overlay_results_rse[BDT_score_list + ['rse_id','weight']].copy()\n",
    "\n",
    "final_merged = pd.merge(filtered_weights,just_score_df, how='outer', on=['rse_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbe356-5c11-4e10-87a9-e92e37ed7464",
   "metadata": {},
   "source": [
    "## Merging into one dataframe with results and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcfd3df-03ff-4033-a11e-6ccd566b2d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BDT_score_list = []\n",
    "\n",
    "# HNL_masses = [20, 50, 100, 150, 180, 200] #Should get rid of this once made overlay branches with all results\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    BDT_score_list.append(f'BDT_output_{HNL_mass}MeV')\n",
    "\n",
    "just_score_df = overlay_results_rse[BDT_score_list + ['rse_id','weight']].copy()\n",
    "\n",
    "final_merged = pd.merge(filtered_weights,just_score_df, how='outer', on=['rse_id']) #This will have the reweighting branches AND BDT score branches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65b95e-e6a7-4d67-a8e3-84222bbaf6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = 150\n",
    "plt.hist(Functions.logit(final_merged[f'BDT_output_{mass}MeV']),bins=bins_dict[mass])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1cb880-1197-4d4e-b75e-5b93223a1e21",
   "metadata": {},
   "source": [
    "# Plotting the BDT score with all different multisims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b551c-b4ca-4171-bc17-c89a78fe522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def All_reweight_err(df, var_name, BINS, x_range, Norm, logit=True):\n",
    "    results_dict = {}\n",
    "    n_bins = len(BINS)-1\n",
    "    for Multisim in Constants.Multisim_univs:\n",
    "        Nuniverse = Constants.Multisim_univs[Multisim]\n",
    "        n_tot = np.empty([Nuniverse, n_bins])\n",
    "        n_cv_tot = np.empty(n_bins)\n",
    "        n_tot.fill(0)\n",
    "        n_cv_tot.fill(0)\n",
    "        \n",
    "        if logit == True:\n",
    "            variable = Functions.logit(df[var_name]) #The BDT output score\n",
    "        else:\n",
    "            variable = df[var_name] #The BDT output score\n",
    "        syst_weights = df[Multisim] #An array of length of the number of events, each entry is an array of length Nunivs\n",
    "        spline_fix_cv  = df[\"weight\"]*Norm\n",
    "        spline_fix_var = df[\"weight\"]*Norm\n",
    "        \n",
    "        s = syst_weights\n",
    "        df_weights = pd.DataFrame(s.values.tolist())\n",
    "        n_cv, bins = np.histogram(variable, range=x_range, bins=BINS, weights=spline_fix_cv)\n",
    "        n_cv_tot += n_cv\n",
    "        \n",
    "        if(Multisim == \"weightsGenie\"): #special treatment as [\"weightSplineTimesTune\"] is included in genie weights\n",
    "            if not df_weights.empty:\n",
    "                for i in range(Nuniverse):\n",
    "                    weight = df_weights[i].values / 1000.\n",
    "                    weight[weight == 1]= df[\"weightSplineTimesTune\"].iloc[weight == 1]\n",
    "                    weight[np.isnan(weight)] = df[\"weightSplineTimesTune\"].iloc[np.isnan(weight)]\n",
    "                    weight[weight > 50] = df[\"weightSplineTimesTune\"].iloc[weight > 50] # why 30 not 50?\n",
    "                    weight[weight <= 0] = df[\"weightSplineTimesTune\"].iloc[weight <= 0]\n",
    "                    weight[weight == np.inf] = df[\"weightSplineTimesTune\"].iloc[weight == np.inf]\n",
    "                \n",
    "                    n, bins = np.histogram(variable, \n",
    "                                           weights=np.nan_to_num(weight*spline_fix_var/df[\"weightSplineTimesTune\"]), range=x_range, bins=BINS)\n",
    "                    n_tot[i] += n\n",
    "                    \n",
    "        if(Multisim == \"weightsPPFX\"): #special treatment as [\"PPFXPcv\"] is included in ppfx weights\n",
    "            if not df_weights.empty:\n",
    "                for i in range(Nuniverse):\n",
    "                    weight = df_weights[i].values / 1000.\n",
    "                    weight[weight == 1]= df[\"ppfx_cv\"].iloc[weight == 1]\n",
    "                    weight[np.isnan(weight)] = df[\"ppfx_cv\"].iloc[np.isnan(weight)]\n",
    "                    weight[weight > 100] = df[\"ppfx_cv\"].iloc[weight > 100]\n",
    "                    weight[weight < 0] = df[\"ppfx_cv\"].iloc[weight < 0]\n",
    "                    weight[weight == np.inf] = df[\"ppfx_cv\"].iloc[weight == np.inf]\n",
    "                \n",
    "                    n, bins = np.histogram(variable, weights=weight*np.nan_to_num(spline_fix_var/df[\"ppfx_cv\"]), range=x_range, bins=BINS)\n",
    "                    n_tot[i] += n\n",
    "        \n",
    "        if(Multisim == \"weightsReint\"):\n",
    "            if not df_weights.empty:\n",
    "                for i in range(Nuniverse):\n",
    "                    weight = df_weights[i].values / 1000.\n",
    "                    weight[np.isnan(weight)] = 1\n",
    "                    weight[weight > 100] = 1\n",
    "                    weight[weight < 0] = 1\n",
    "                    weight[weight == np.inf] = 1\n",
    "                    n, bins = np.histogram(variable, weights=weight*spline_fix_var, range=x_range, bins=BINS)\n",
    "                    n_tot[i] += n\n",
    "        cov = np.empty([len(n_cv), len(n_cv)])\n",
    "        cov.fill(0)\n",
    "\n",
    "        for n in n_tot:\n",
    "            for i in range(len(n_cv)):\n",
    "                for j in range(len(n_cv)):\n",
    "                    cov[i][j] += (n[i] - n_cv_tot[i]) * (n[j] - n_cv_tot[j])\n",
    "\n",
    "        cov /= Nuniverse\n",
    "        results_dict[Multisim] = [cov,n_cv_tot,n_tot,bins]\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42735701-a7c6-42ff-992f-24424ae620ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HNL_mass = 50\n",
    "logit = True\n",
    "results_dict = {}\n",
    "for HNL_mass in HNL_masses:\n",
    "    print(f\"Calculating {HNL_mass} uncertainties.\")\n",
    "    results_dict[HNL_mass] = All_reweight_err(final_merged, f'BDT_output_{HNL_mass}MeV', bins_dict[HNL_mass],\n",
    "                                    [bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1]], Norm, logit=logit)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3f472-a3ae-40ad-8c0a-1473efb9b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_multisim(results_dict, HNL_mass, savefig=False, display=False, Params={}):\n",
    "    for Multisim in results_dict:\n",
    "        Nuniverse = Constants.Multisim_univs[Multisim]\n",
    "        cov = results_dict[Multisim][0]\n",
    "        cv = results_dict[Multisim][1]\n",
    "        n_tot = results_dict[Multisim][2]\n",
    "        bins = results_dict[Multisim][3]\n",
    "        xlims = [min(bins), max(bins)]\n",
    "        \n",
    "        fig,ax = plt.subplots(nrows=2, ncols=1, sharex=True, gridspec_kw={'height_ratios': [3, 1]}, figsize=[10,10],dpi=200)\n",
    "        plt.sca(ax[0])\n",
    "\n",
    "        # bins=np.linspace(0,1.0,21)\n",
    "\n",
    "        bins_cent=(bins[:-1]+bins[1:])/2\n",
    "        bins_centlong=np.tile(bins_cent,Nuniverse)\n",
    "\n",
    "        nybins=70\n",
    "\n",
    "        plt.title(Multisim + \" Variations\",fontsize=20)\n",
    "\n",
    "        plt.hist(bins_cent,bins,weights=cv,color=\"red\",histtype=\"step\",label=\"Central Value\",lw=2,linestyle='-')\n",
    "        plt.legend()\n",
    "        bins_cent=(bins[:-1]+bins[1:])/2\n",
    "        bins_centlong=np.tile(bins_cent,Nuniverse)\n",
    "\n",
    "        plt.ylabel(\"Events\")\n",
    "        plt.hist2d(bins_centlong,n_tot.flatten(),bins=[bins,nybins],cmin=1,range=[xlims,[0,max(cv)*1.4]],rasterized=True)\n",
    "\n",
    "        plt.colorbar(pad=0,use_gridspec=True)\n",
    "        #fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap),pad=0,use_gridspec=True)\n",
    "        plt.ylim(0,max(cv)*1.4)\n",
    "        # plt.yscale(\"log\")\n",
    "\n",
    "        plt.sca(ax[1])\n",
    "        \n",
    "        #ax[1].tick_params(labelright=False, length=0)\n",
    "        pos = ax[0].get_position()\n",
    "        pos2 = ax[1].get_position()\n",
    "        ax[1].set_position([pos.x0,pos2.y0,pos.width,pos2.height])\n",
    "        \n",
    "        plt.hist(bins_cent,bins,weights=np.sqrt(np.diag(cov))/cv*100,color=\"black\",histtype=\"step\",lw=3,linestyle='-')\n",
    "        maxy = 1.5*max(plt.hist(bins_cent,bins,weights=np.sqrt(np.diag(cov))/cv*100,color=\"black\",histtype=\"step\",lw=3,linestyle='-')[0])\n",
    "        plt.ylim(0,maxy)\n",
    "        plt.ylabel(\"% Uncertainity\")\n",
    "        #plt.yticks([])\n",
    "        plt.xlabel(f\"BDT score ({HNL_mass} MeV HNL) \",fontsize=25)\n",
    "        # plt.tight_layout()\n",
    "        \n",
    "        if Params[\"Load_lepton_signal\"] == True:sig_type = \"ee_\"\n",
    "        if Params[\"Load_pi0_signal\"] == True:sig_type = \"pi0_\"\n",
    "        save_loc = \"plots/Sys_uncertainty/Overlay/Multisim/bkg_multisim_\"\n",
    "        \n",
    "        if savefig == True:\n",
    "            plt.savefig(save_loc + Run + \"_\" + str(HNL_mass) + f\"_MeV_{sig_type}{Multisim}.pdf\")\n",
    "            plt.savefig(save_loc + Run + \"_\" + str(HNL_mass) + f\"_MeV_{sig_type}{Multisim}.png\")\n",
    "        if display == False:\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989da16-a516-4952-b1ea-19220aab0246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HNL_masses = Constants.HNL_mass_samples\n",
    "for HNL_mass in HNL_masses:\n",
    "    Plot_multisim(results_dict[HNL_mass], HNL_mass, savefig=False, display=True, Params=Params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b39f4-92e4-409a-a726-9c8af7380f5f",
   "metadata": {},
   "source": [
    "## Calculating and saving total uncertainty for each reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1dcc2d-2428-414e-820d-daec2265ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = 150\n",
    "print(len(results_dict[mass][\"weightsGenie\"][0]))\n",
    "print(results_dict[mass][\"weightsGenie\"][0][-1])\n",
    "\n",
    "print(results_dict[mass][\"weightsPPFX\"][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef060edf-eb74-44b1-8eab-e1530660d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.diag(results_dict[mass][\"weightsGenie\"][0]))\n",
    "print(results_dict[mass][\"weightsGenie\"][1])\n",
    "\n",
    "print(np.diag(results_dict[mass][\"weightsPPFX\"][0]))\n",
    "print(results_dict[mass][\"weightsPPFX\"][1])\n",
    "\n",
    "genie_unc = np.sqrt(np.diag(results_dict[mass][\"weightsGenie\"][0]))\n",
    "print(genie_unc)\n",
    "print(genie_unc/results_dict[mass][\"weightsGenie\"][1])\n",
    "\n",
    "print(type(genie_unc/results_dict[mass][\"weightsGenie\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a9093-1d2d-41c4-8e7f-4f73c10e9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppfx_unc_dict, ppfx_unc_frac_dict = {}, {}\n",
    "genie_unc_dict, genie_unc_frac_dict = {}, {}\n",
    "reint_unc_dict, reint_unc_frac_dict = {}, {}\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    \n",
    "    diagonal_ppfx = np.diag(results_dict[HNL_mass][\"weightsPPFX\"][0]).copy()\n",
    "    diagonal_genie = np.diag(results_dict[HNL_mass][\"weightsGenie\"][0]).copy()\n",
    "    diagonal_reint = np.diag(results_dict[HNL_mass][\"weightsReint\"][0]).copy()\n",
    "    \n",
    "    num_ppfx = results_dict[HNL_mass][\"weightsPPFX\"][1].copy()\n",
    "    num_genie = results_dict[HNL_mass][\"weightsGenie\"][1].copy()\n",
    "    num_reint = results_dict[HNL_mass][\"weightsReint\"][1].copy()\n",
    "    if np.any(diagonal_ppfx==0):\n",
    "        print(f\"{HNL_mass}MeV contains zeros, setting error to that of prev bin\")\n",
    "    \n",
    "    index = np.where(diagonal_ppfx == 0)\n",
    "    diagonal_ppfx[diagonal_ppfx==0] = 1\n",
    "    diagonal_genie[diagonal_genie==0] = 1\n",
    "    diagonal_reint[diagonal_reint==0] = 1\n",
    "    \n",
    "    num_ppfx[num_ppfx==0] = 1\n",
    "    num_genie[num_genie==0] = 1\n",
    "    num_reint[num_reint==0] = 1\n",
    "    \n",
    "    index_ppfx, index_genie, index_reint = [], [], []\n",
    "    \n",
    "    \n",
    "    ppfx_unc_dict[HNL_mass] = np.sqrt(diagonal_ppfx)\n",
    "    ppfx_unc_frac_dict[HNL_mass] = ppfx_unc_dict[HNL_mass]/num_ppfx\n",
    "    if np.any(ppfx_unc_dict[HNL_mass]/num_ppfx>0.40):\n",
    "        print(f\"{HNL_mass}MeV PPFX has large error, setting error to that of prev bin\")\n",
    "        index_ppfx = np.where(ppfx_unc_dict[HNL_mass]/num_ppfx>0.40)\n",
    "\n",
    "    genie_unc_dict[HNL_mass] = np.sqrt(diagonal_genie)\n",
    "    genie_unc_frac_dict[HNL_mass] = genie_unc_dict[HNL_mass]/num_genie\n",
    "    if np.any(genie_unc_dict[HNL_mass]/num_genie>0.40):\n",
    "        print(f\"{HNL_mass}MeV Genie has large error, setting error to that of prev bin\")\n",
    "        index_genie = np.where(genie_unc_dict[HNL_mass]/num_genie>0.40)\n",
    "\n",
    "    reint_unc_dict[HNL_mass] = np.sqrt(diagonal_reint)\n",
    "    reint_unc_frac_dict[HNL_mass] = reint_unc_dict[HNL_mass]/num_reint\n",
    "    if np.any(reint_unc_dict[HNL_mass]/num_reint>0.20):\n",
    "        print(f\"{HNL_mass}MeV reint has large error, setting error to that of prev bin\")\n",
    "        index_reint = np.where(reint_unc_dict[HNL_mass]/num_reint>0.20)\n",
    "    \n",
    "    #Fixing zero-bins\n",
    "    for i in index:\n",
    "        ppfx_unc_frac_dict[HNL_mass][i] = ppfx_unc_frac_dict[HNL_mass][i-1]\n",
    "        genie_unc_frac_dict[HNL_mass][i] = genie_unc_frac_dict[HNL_mass][i-1]\n",
    "        reint_unc_frac_dict[HNL_mass][i] = reint_unc_frac_dict[HNL_mass][i-1]\n",
    "        \n",
    "        ppfx_unc_dict[HNL_mass][i] = ppfx_unc_dict[HNL_mass][i-1]\n",
    "        genie_unc_dict[HNL_mass][i] = genie_unc_dict[HNL_mass][i-1]\n",
    "        reint_unc_dict[HNL_mass][i] = reint_unc_dict[HNL_mass][i-1]\n",
    "    \n",
    "    #Fixing high error bins\n",
    "    for i in index_ppfx:\n",
    "        ppfx_unc_frac_dict[HNL_mass][i] = ppfx_unc_frac_dict[HNL_mass][i-1]\n",
    "        ppfx_unc_dict[HNL_mass][i] = ppfx_unc_dict[HNL_mass][i-1]\n",
    "    for i in index_genie:\n",
    "        genie_unc_frac_dict[HNL_mass][i] = genie_unc_frac_dict[HNL_mass][i-1]\n",
    "        genie_unc_dict[HNL_mass][i] = genie_unc_dict[HNL_mass][i-1]\n",
    "    for i in index_reint:\n",
    "        reint_unc_frac_dict[HNL_mass][i] = reint_unc_frac_dict[HNL_mass][i-1]\n",
    "        reint_unc_dict[HNL_mass][i] = reint_unc_dict[HNL_mass][i-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2987cd53-e935-4752-b7ab-f04a621988a4",
   "metadata": {},
   "source": [
    "## Saving reweighting uncertainties as .root files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69c70f-afb2-40a5-8466-c07471150d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_hists = \"Uncertainties/\"\n",
    "if Params[\"Load_pi0_signal\"] == True:loc_hists+=\"pi0/\"\n",
    "\n",
    "name_type = Functions.Get_signal_name_type(Params)\n",
    "filename = name_type+'_EXT_full_Finished.root'\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    bins_cent=(bins_dict[HNL_mass][:-1]+bins_dict[HNL_mass][1:])/2\n",
    "\n",
    "    values_dict = {'ppfx_uncertainty': ppfx_unc_dict[HNL_mass], 'Genie_uncertainty':genie_unc_dict[HNL_mass],\n",
    "                   'Reinteraction_uncertainty':reint_unc_dict[HNL_mass],\n",
    "                   'ppfx_uncertainty_frac': ppfx_unc_frac_dict[HNL_mass], 'Genie_uncertainty_frac':genie_unc_frac_dict[HNL_mass],\n",
    "                   'Reinteraction_uncertainty_frac':reint_unc_frac_dict[HNL_mass]} \n",
    "    hist_samples = {}\n",
    "\n",
    "    #make array with all values 1, then weight by value\n",
    "    for name in values_dict:\n",
    "        test_hist = np.histogram(bins_cent, weights=values_dict[name], bins=bins_dict[HNL_mass], \n",
    "                                 range=[bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1]])\n",
    "        hist_samples[name] = test_hist\n",
    "\n",
    "    stop_writing = False\n",
    "    dont_save = []\n",
    "    with uproot.open(loc_hists+Params[\"Run\"]+f'_{HNL_mass}_'+filename) as file:\n",
    "    # with uproot.open(loc_hists+f'{Run}_{HNL_mass}MeV_{filename}.root') as file: #Check what is already in the file (read-only)\n",
    "        for name in hist_samples:\n",
    "            if str(name)+\";1\" in file.keys():\n",
    "                # print(str(name) + f\" ALREADY EXISTS in {HNL_mass}MeV file, not re-saving\")\n",
    "                dont_save.append(name)\n",
    "\n",
    "    with uproot.update(loc_hists+f'{Run}_{HNL_mass}_{filename}') as file: #Add new hists into the file\n",
    "        # del file['ppfx_uncertainty']\n",
    "\n",
    "        for name in hist_samples:\n",
    "            # if stop_writing == True:\n",
    "            #     print(\"Not saving hist\")\n",
    "            #     break\n",
    "            if name in dont_save:\n",
    "                # print(f\"Not saving {name}\")\n",
    "                continue\n",
    "            else:\n",
    "                file[name] = hist_samples[name]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98785248-cd13-45e8-b5f1-2e4aa371380b",
   "metadata": {},
   "source": [
    "## End of code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
