{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced5e536-c413-4447-b2d4-8b0ed1c50021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.24/06\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "import os,sys,string, time\n",
    "import ROOT\n",
    "from math import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from root_pandas import read_root\n",
    "from platform import python_version\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import math\n",
    "from matplotlib.patches import Rectangle\n",
    "import pickle\n",
    "\n",
    "import Utilities.Plotter as PT\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Variables_list as Variables\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print ('Success')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce878578-1431-44ef-937e-f139e58caf9b",
   "metadata": {},
   "source": [
    "# Loading in the \"results\" dataframe after full selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a5f612-8b8c-427d-9201-adbecb1b2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Run = \"run3\" #so far either \"run1\" or \"run3\"\n",
    "\n",
    "HNL_masses = Constants.HNL_mass_samples\n",
    "\n",
    "loc_pkl = f'pkl_files/{Run}/current_files/Results/'\n",
    "\n",
    "overlay_results = pd.read_pickle(loc_pkl+\"overlay_results_NEW.pkl\") #This will contain all of the BDT output scores and rse_id\n",
    "\n",
    "loc_hists = 'bdt_output/'\n",
    "\n",
    "filename = f'logit_FINAL_1.root' #CHANGE THIS IF CHANGING FILE INPUT\n",
    "\n",
    "bins_dict = {}\n",
    "for HNL_mass in HNL_masses:\n",
    "    hist_placeholder = uproot.open(loc_hists+f'{Run}_{HNL_mass}MeV_'+filename)\n",
    "    bins_dict[HNL_mass] = hist_placeholder['bkg_overlay'].to_numpy()[1] #A tuple of bin edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff28413-ae20-47cc-9464-4c260937d3e5",
   "metadata": {},
   "source": [
    "# Reading in the overlay .root file with reweight branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a4c5de-0103-44bf-8404-7463d7a77ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_variables = Variables.sys_vars + Variables.event_vars + Variables.weight_related\n",
    "\n",
    "if Run == \"run1\":\n",
    "    NuMI_MC_overlay = uproot3.open('../NuMI_MC/SLIMMED_neutrinoselection_filt_run1_overlay.root')['nuselection/NeutrinoSelectionFilter']\n",
    "    Norm = Constants.SF_overlay_run1\n",
    "elif Run == \"run3\":\n",
    "    NuMI_MC_overlay = uproot3.open('../NuMI_MC/SLIMMED_neutrinoselection_filt_run3_overlay.root')['nuselection/NeutrinoSelectionFilter']\n",
    "    Norm = Constants.SF_overlay_run3\n",
    "\n",
    "df_overlay_weights = NuMI_MC_overlay.pandas.df(sys_variables, flatten=False) #Perhaps I can do this in a more clever way than just making a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c9948-3875-445d-aaaf-9c4e881d2969",
   "metadata": {},
   "source": [
    "# Keeping only the events which pass selection in the weight dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8885864f-01e9-48e0-b041-1407ba8a5a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events in weights file is 748702\n",
      "Number of events in results file is 8849\n"
     ]
    }
   ],
   "source": [
    "def make_unique_ev_id(df): #df must have 'run', 'sub' and 'evt' branches\n",
    "    if pd.Series(['run', 'sub', 'evt']).isin(df.columns).all():\n",
    "        rse_list = []\n",
    "        for entry in df.index: #Looping over all events in the dataframe\n",
    "            rse = str(df['run'][entry]) + \"_\" + str(df['sub'][entry]) + \"_\" + str(df['evt'][entry])\n",
    "            rse_list.append(rse)\n",
    "        df['rse_id'] = rse_list #Writing a new branch with the unique event id\n",
    "        return df.copy()\n",
    "    else:\n",
    "        print(\"Dataframe needs \\\"run\\\", \\\"sub\\\" and \\\"evt\\\" columns.\")\n",
    "        return 0\n",
    "    \n",
    "def check_duplicate_events(df):\n",
    "    rse_list = df['rse_id'].to_list()\n",
    "\n",
    "    seen = set()\n",
    "    dupes = []\n",
    "\n",
    "    for x in rse_list:\n",
    "        if x in seen:\n",
    "            dupes.append(x)\n",
    "        else:\n",
    "            seen.add(x)\n",
    "    print(\"Number of duplicates is \" + str(len(dupes)))\n",
    "    print(\"Number of unique events is \" + str(len(seen)))\n",
    "\n",
    "overlay_results_rse = make_unique_ev_id(overlay_results)\n",
    "df_overlay_weights_rse = make_unique_ev_id(df_overlay_weights)\n",
    "\n",
    "#Deleting any duplicates of events, should be able to avoid if correctly filtered for one event per row\n",
    "overlay_results_rse.drop_duplicates(subset=['rse_id'], keep='first', inplace=True)\n",
    "\n",
    "print(\"Number of events in weights file is \" + str(len(df_overlay_weights_rse)))\n",
    "print(\"Number of events in results file is \" + str(len(overlay_results_rse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00863fdf-66f9-4e0e-9e57-6e3db102152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events in the filtered weights file is 8849\n",
      "Number of events in results file is 8849\n"
     ]
    }
   ],
   "source": [
    "#Keeping only those events in the final selection\n",
    "filtered_weights = df_overlay_weights_rse.loc[(df_overlay_weights_rse['rse_id'].isin(overlay_results_rse['rse_id']))]\n",
    "\n",
    "print(\"Number of events in the filtered weights file is \" + str(len(filtered_weights)))\n",
    "print(\"Number of events in results file is \" + str(len(overlay_results_rse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9a5f9-bf48-45d7-a35b-909d3bc5eb88",
   "metadata": {},
   "source": [
    "## Calculating uncertainty for a BDT input variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4db4a8c2-831d-41ae-a044-cede9b82ca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['weightsPPFX', 'weightsGenie', 'weightsReint', 'run', 'sub', 'evt',\n",
      "       'weightSplineTimesTune', 'ppfx_cv', 'npi0', 'rse_id'],\n",
      "      dtype='object')\n",
      "Index(['run', 'sub', 'evt', 'weightSplineTimesTune', 'ppfx_cv', 'npi0',\n",
      "       'weight', 'BDT_output_2MeV', 'BDT_output_10MeV', 'BDT_output_20MeV',\n",
      "       'BDT_output_50MeV', 'BDT_output_100MeV', 'BDT_output_150MeV',\n",
      "       'BDT_output_180MeV', 'BDT_output_200MeV', 'BDT_output_220MeV',\n",
      "       'BDT_output_240MeV', 'BDT_output_245MeV', 'rse_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(filtered_weights.keys())\n",
    "print(overlay_results_rse.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22c5f798-6355-4175-a963-48924bdcc2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BDT_score_list = []\n",
    "\n",
    "# HNL_masses = [20, 50, 100, 150, 180, 200] #Should get rid of this once made overlay branches with all results\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    BDT_score_list.append(f'BDT_output_{HNL_mass}MeV')\n",
    "\n",
    "just_score_df = overlay_results_rse[BDT_score_list + ['rse_id','weight']].copy()\n",
    "\n",
    "final_merged = pd.merge(filtered_weights,just_score_df, how='outer', on=['rse_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbe356-5c11-4e10-87a9-e92e37ed7464",
   "metadata": {},
   "source": [
    "## Merging into one dataframe with results and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efcfd3df-03ff-4033-a11e-6ccd566b2d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BDT_score_list = []\n",
    "\n",
    "# HNL_masses = [20, 50, 100, 150, 180, 200] #Should get rid of this once made overlay branches with all results\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    BDT_score_list.append(f'BDT_output_{HNL_mass}MeV')\n",
    "\n",
    "just_score_df = overlay_results_rse[BDT_score_list + ['rse_id','weight']].copy()\n",
    "\n",
    "final_merged = pd.merge(filtered_weights,just_score_df, how='outer', on=['rse_id']) #This will have the reweighting branches AND BDT score branches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1cb880-1197-4d4e-b75e-5b93223a1e21",
   "metadata": {},
   "source": [
    "# Plotting the BDT score with all different multisims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b44b551c-b4ca-4171-bc17-c89a78fe522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def All_reweight_err(df, var_name, BINS, x_range, Norm, logit=True):\n",
    "    results_dict = {}\n",
    "    n_bins = len(BINS)-1\n",
    "    for Multisim in Constants.Multisim_univs:\n",
    "        Nuniverse = Constants.Multisim_univs[Multisim]\n",
    "        n_tot = np.empty([Nuniverse, n_bins])\n",
    "        n_cv_tot = np.empty(n_bins)\n",
    "        n_tot.fill(0)\n",
    "        n_cv_tot.fill(0)\n",
    "        \n",
    "        if logit == True:\n",
    "            variable = Functions.logit(df[var_name]) #The BDT output score\n",
    "        else:\n",
    "            variable = df[var_name] #The BDT output score\n",
    "        syst_weights = df[Multisim] #An array of length of the number of events, each entry is an array of length Nunivs\n",
    "        spline_fix_cv  = df[\"weight\"]*Norm\n",
    "        spline_fix_var = df[\"weight\"]*Norm\n",
    "        \n",
    "        s = syst_weights\n",
    "        df_weights = pd.DataFrame(s.values.tolist())\n",
    "        n_cv, bins = np.histogram(variable, range=x_range, bins=BINS, weights=spline_fix_cv)\n",
    "        n_cv_tot += n_cv\n",
    "        \n",
    "        if(Multisim == \"weightsGenie\"): #special treatment as [\"weightSplineTimesTune\"] is included in genie weights\n",
    "            if not df_weights.empty:\n",
    "                for i in range(Nuniverse):\n",
    "                    weight = df_weights[i].values / 1000.\n",
    "                    weight[weight == 1]= df[\"weightSplineTimesTune\"].iloc[weight == 1]\n",
    "                    weight[np.isnan(weight)] = df[\"weightSplineTimesTune\"].iloc[np.isnan(weight)]\n",
    "                    weight[weight > 50] = df[\"weightSplineTimesTune\"].iloc[weight > 50] # why 30 not 50?\n",
    "                    weight[weight <= 0] = df[\"weightSplineTimesTune\"].iloc[weight <= 0]\n",
    "                    weight[weight == np.inf] = df[\"weightSplineTimesTune\"].iloc[weight == np.inf]\n",
    "                \n",
    "                    n, bins = np.histogram(variable, \n",
    "                                           weights=np.nan_to_num(weight*spline_fix_var/df[\"weightSplineTimesTune\"]), range=x_range, bins=BINS)\n",
    "                    n_tot[i] += n\n",
    "                    \n",
    "        if(Multisim == \"weightsPPFX\"): #special treatment as [\"PPFXPcv\"] is included in ppfx weights\n",
    "            if not df_weights.empty:\n",
    "                for i in range(Nuniverse):\n",
    "                    weight = df_weights[i].values / 1000.\n",
    "                    weight[weight == 1]= df[\"ppfx_cv\"].iloc[weight == 1]\n",
    "                    weight[np.isnan(weight)] = df[\"ppfx_cv\"].iloc[np.isnan(weight)]\n",
    "                    weight[weight > 100] = df[\"ppfx_cv\"].iloc[weight > 100]\n",
    "                    weight[weight < 0] = df[\"ppfx_cv\"].iloc[weight < 0]\n",
    "                    weight[weight == np.inf] = df[\"ppfx_cv\"].iloc[weight == np.inf]\n",
    "                \n",
    "                    n, bins = np.histogram(variable, weights=weight*np.nan_to_num(spline_fix_var/df[\"ppfx_cv\"]), range=x_range, bins=BINS)\n",
    "                    n_tot[i] += n\n",
    "        \n",
    "        if(Multisim == \"weightsReint\"):\n",
    "            if not df_weights.empty:\n",
    "                for i in range(Nuniverse):\n",
    "                    weight = df_weights[i].values / 1000.\n",
    "                    weight[np.isnan(weight)] = 1\n",
    "                    weight[weight > 100] = 1\n",
    "                    weight[weight < 0] = 1\n",
    "                    weight[weight == np.inf] = 1\n",
    "                    n, bins = np.histogram(variable, weights=weight*spline_fix_var, range=x_range, bins=BINS)\n",
    "                    n_tot[i] += n\n",
    "        cov = np.empty([len(n_cv), len(n_cv)])\n",
    "        cov.fill(0)\n",
    "\n",
    "        for n in n_tot:\n",
    "            for i in range(len(n_cv)):\n",
    "                for j in range(len(n_cv)):\n",
    "                    cov[i][j] += (n[i] - n_cv_tot[i]) * (n[j] - n_cv_tot[j])\n",
    "\n",
    "        cov /= Nuniverse\n",
    "        results_dict[Multisim] = [cov,n_cv_tot,n_tot,bins]\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42735701-a7c6-42ff-992f-24424ae620ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating 2MeV uncertainties.\n",
      "Calculating 10MeV uncertainties.\n",
      "Calculating 20MeV uncertainties.\n",
      "Calculating 50MeV uncertainties.\n",
      "Calculating 100MeV uncertainties.\n",
      "Calculating 150MeV uncertainties.\n",
      "Calculating 180MeV uncertainties.\n",
      "Calculating 200MeV uncertainties.\n",
      "Calculating 220MeV uncertainties.\n",
      "Calculating 240MeV uncertainties.\n",
      "Calculating 245MeV uncertainties.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# HNL_mass = 50\n",
    "logit = True\n",
    "results_dict = {}\n",
    "for HNL_mass in HNL_masses:\n",
    "    print(f\"Calculating {HNL_mass}MeV uncertainties.\")\n",
    "    results_dict[HNL_mass] = All_reweight_err(final_merged, f'BDT_output_{HNL_mass}MeV', bins_dict[HNL_mass],\n",
    "                                    [bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1]], Norm, logit=logit)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbc3f472-a3ae-40ad-8c0a-1473efb9b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_multisim(results_dict, HNL_mass, savefig=False, display=False):\n",
    "    for Multisim in results_dict:\n",
    "        Nuniverse = Constants.Multisim_univs[Multisim]\n",
    "        cov = results_dict[Multisim][0]\n",
    "        cv = results_dict[Multisim][1]\n",
    "        n_tot = results_dict[Multisim][2]\n",
    "        bins = results_dict[Multisim][3]\n",
    "        xlims = [min(bins), max(bins)]\n",
    "        \n",
    "        fig,ax = plt.subplots(nrows=2, ncols=1, sharex=True, gridspec_kw={'height_ratios': [3, 1]}, figsize=[10,10],dpi=200)\n",
    "        plt.sca(ax[0])\n",
    "\n",
    "        # bins=np.linspace(0,1.0,21)\n",
    "\n",
    "        bins_cent=(bins[:-1]+bins[1:])/2\n",
    "        bins_centlong=np.tile(bins_cent,Nuniverse)\n",
    "\n",
    "        nybins=70\n",
    "\n",
    "        plt.title(Multisim + \" Variations\",fontsize=20)\n",
    "\n",
    "        plt.hist(bins_cent,bins,weights=cv,color=\"red\",histtype=\"step\",label=\"Central Value\",lw=2,linestyle='-')\n",
    "        plt.legend()\n",
    "        bins_cent=(bins[:-1]+bins[1:])/2\n",
    "        bins_centlong=np.tile(bins_cent,Nuniverse)\n",
    "\n",
    "        plt.ylabel(\"Events\")\n",
    "        plt.hist2d(bins_centlong,n_tot.flatten(),bins=[bins,nybins],cmin=1,range=[xlims,[0,max(cv)*1.4]],rasterized=True)\n",
    "\n",
    "        plt.colorbar(pad=0,use_gridspec=True)\n",
    "        #fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap),pad=0,use_gridspec=True)\n",
    "        plt.ylim(0,max(cv)*1.4)\n",
    "        # plt.yscale(\"log\")\n",
    "\n",
    "        plt.sca(ax[1])\n",
    "        \n",
    "        #ax[1].tick_params(labelright=False, length=0)\n",
    "        pos = ax[0].get_position()\n",
    "        pos2 = ax[1].get_position()\n",
    "        ax[1].set_position([pos.x0,pos2.y0,pos.width,pos2.height])\n",
    "        \n",
    "        plt.hist(bins_cent,bins,weights=np.sqrt(np.diag(cov))/cv*100,color=\"black\",histtype=\"step\",lw=3,linestyle='-')\n",
    "        maxy = 1.5*max(plt.hist(bins_cent,bins,weights=np.sqrt(np.diag(cov))/cv*100,color=\"black\",histtype=\"step\",lw=3,linestyle='-')[0])\n",
    "        plt.ylim(0,maxy)\n",
    "        plt.ylabel(\"% Uncertainity\")\n",
    "        #plt.yticks([])\n",
    "        plt.xlabel(f\"BDT score ({HNL_mass} MeV HNL) \",fontsize=25)\n",
    "        # plt.tight_layout()\n",
    "        if savefig == True:\n",
    "            plt.savefig(\"plots/Sys_uncertainty/Overlay/Multisim/bkg_multisim_\" + Run + \"_\" + str(HNL_mass) + \"_MeV_\" + Multisim + \".pdf\")\n",
    "            plt.savefig(\"plots/Sys_uncertainty/Overlay/Multisim/bkg_multisim_\" + Run + \"_\" + str(HNL_mass) + \"_MeV_\" + Multisim + \".png\")\n",
    "        if display == False:\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1989da16-a516-4952-b1ea-19220aab0246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/david/jupyter/envs/ana/lib/python3.7/site-packages/ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "HNL_masses = Constants.HNL_mass_samples\n",
    "for HNL_mass in HNL_masses:\n",
    "    Plot_multisim(results_dict[HNL_mass], HNL_mass, savefig=False, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b39f4-92e4-409a-a726-9c8af7380f5f",
   "metadata": {},
   "source": [
    "## Calculating and saving total uncertainty for each reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f6a9093-1d2d-41c4-8e7f-4f73c10e9b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2MeV contains zeros, setting error to that of prev bin\n",
      "10MeV contains zeros, setting error to that of prev bin\n",
      "20MeV contains zeros, setting error to that of prev bin\n"
     ]
    }
   ],
   "source": [
    "ppfx_unc_dict, ppfx_unc_frac_dict = {}, {}\n",
    "genie_unc_dict, genie_unc_frac_dict = {}, {}\n",
    "reint_unc_dict, reint_unc_frac_dict = {}, {}\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    \n",
    "    diagonal_ppfx = np.diag(results_dict[HNL_mass][\"weightsPPFX\"][0]).copy()\n",
    "    diagonal_genie = np.diag(results_dict[HNL_mass][\"weightsGenie\"][0]).copy()\n",
    "    diagonal_reint = np.diag(results_dict[HNL_mass][\"weightsReint\"][0]).copy()\n",
    "    \n",
    "    num_ppfx = results_dict[HNL_mass][\"weightsPPFX\"][1].copy()\n",
    "    num_genie = results_dict[HNL_mass][\"weightsPPFX\"][1].copy()\n",
    "    num_reint = results_dict[HNL_mass][\"weightsReint\"][1].copy()\n",
    "    if np.any(diagonal_ppfx==0):\n",
    "        print(f\"{HNL_mass}MeV contains zeros, setting error to that of prev bin\")\n",
    "    index = np.where(diagonal_ppfx == 0)\n",
    "    diagonal_ppfx[diagonal_ppfx==0] = 1\n",
    "    diagonal_genie[diagonal_genie==0] = 1\n",
    "    diagonal_reint[diagonal_reint==0] = 1\n",
    "    \n",
    "    num_ppfx[num_ppfx==0] = 1\n",
    "    num_genie[num_genie==0] = 1\n",
    "    num_reint[num_reint==0] = 1\n",
    "    \n",
    "    ppfx_unc_dict[HNL_mass] = np.sqrt(diagonal_ppfx)\n",
    "    ppfx_unc_frac_dict[HNL_mass] = ppfx_unc_dict[HNL_mass]/num_ppfx\n",
    "\n",
    "    genie_unc_dict[HNL_mass] = np.sqrt(diagonal_genie)\n",
    "    genie_unc_frac_dict[HNL_mass] = genie_unc_dict[HNL_mass]/num_genie\n",
    "\n",
    "    reint_unc_dict[HNL_mass] = np.sqrt(diagonal_reint)\n",
    "    reint_unc_frac_dict[HNL_mass] = reint_unc_dict[HNL_mass]/num_reint\n",
    "    \n",
    "    for i in index:\n",
    "        ppfx_unc_frac_dict[HNL_mass][i] = ppfx_unc_frac_dict[HNL_mass][i-1]\n",
    "        genie_unc_frac_dict[HNL_mass][i] = genie_unc_frac_dict[HNL_mass][i-1]\n",
    "        reint_unc_frac_dict[HNL_mass][i] = reint_unc_frac_dict[HNL_mass][i-1]\n",
    "        \n",
    "        ppfx_unc_dict[HNL_mass][i] = ppfx_unc_dict[HNL_mass][i-1]\n",
    "        genie_unc_dict[HNL_mass][i] = genie_unc_dict[HNL_mass][i-1]\n",
    "        reint_unc_dict[HNL_mass][i] = reint_unc_dict[HNL_mass][i-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2987cd53-e935-4752-b7ab-f04a621988a4",
   "metadata": {},
   "source": [
    "## Saving reweighting uncertainties as .root files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f69c70f-afb2-40a5-8466-c07471150d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "loc_hists = \"Uncertainties/\"\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    bins_cent=(bins_dict[HNL_mass][:-1]+bins_dict[HNL_mass][1:])/2\n",
    "\n",
    "    values_dict = {'ppfx_uncertainty': ppfx_unc_dict[HNL_mass], 'Genie_uncertainty':genie_unc_dict[HNL_mass],\n",
    "                   'Reinteraction_uncertainty':reint_unc_dict[HNL_mass],\n",
    "                   'ppfx_uncertainty_frac': ppfx_unc_frac_dict[HNL_mass], 'Genie_uncertainty_frac':genie_unc_frac_dict[HNL_mass],\n",
    "                   'Reinteraction_uncertainty_frac':reint_unc_frac_dict[HNL_mass]} \n",
    "    hist_samples = {}\n",
    "\n",
    "    #make array with all values 1, then weight by value\n",
    "    for name in values_dict:\n",
    "        test_hist = np.histogram(bins_cent, weights=values_dict[name], bins=bins_dict[HNL_mass], \n",
    "                                 range=[bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1]])\n",
    "        hist_samples[name] = test_hist\n",
    "\n",
    "    stop_writing = False\n",
    "    dont_save = []\n",
    "    with uproot.open(loc_hists+f'{Run}_{HNL_mass}MeV_'+filename) as file: #Check what is already in the file (read-only)\n",
    "        for name in hist_samples:\n",
    "            if str(name)+\";1\" in file.keys():\n",
    "                # print(str(name) + f\" ALREADY EXISTS in {HNL_mass}MeV file, not re-saving\")\n",
    "                dont_save.append(name)\n",
    "\n",
    "    with uproot.update(loc_hists+f'{Run}_{HNL_mass}MeV_'+filename) as file: #Add new hists into the file\n",
    "        # del file['ppfx_uncertainty']\n",
    "\n",
    "        for name in hist_samples:\n",
    "            # if stop_writing == True:\n",
    "            #     print(\"Not saving hist\")\n",
    "            #     break\n",
    "            if name in dont_save:\n",
    "                # print(f\"Not saving {name}\")\n",
    "                continue\n",
    "            else:\n",
    "                file[name] = hist_samples[name]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54131b96-277e-404d-a540-a5222a087388",
   "metadata": {},
   "source": [
    "## Can't figure out how to delete hists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790c12f-cc6e-4f82-b692-e6f352522b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to delete\n",
    "# for HNL_mass in HNL_masses:\n",
    "HNL_mass = 220\n",
    "file = uproot.update(loc_hists+f'{Run}_{HNL_mass}MeV_'+filename) #Add new hists into the file\n",
    "        # del file['ppfx_uncertainty']\n",
    "print(file.keys())\n",
    "# del file['ppfx_uncertainty_frac']\n",
    "print(file.keys())\n",
    "\n",
    "# for name in hist_samples:\n",
    "#     del file[name]\n",
    "    \n",
    "        # for name in hist_samples:\n",
    "        #     if str(name)+\";1\" in file.keys():\n",
    "        #         print(\"Trying to delete \" + name+\";1\")\n",
    "        #         del file[name+\";1\"]\n",
    "        \n",
    "            # if stop_writing == True:\n",
    "            #     print(\"Not saving hist\")\n",
    "            #     break\n",
    "            # if name in dont_save:\n",
    "            #     print(f\"Not saving {name}\")\n",
    "            # else:\n",
    "            # del file[name]\n",
    "            # file[name] = hist_samples[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98785248-cd13-45e8-b5f1-2e4aa371380b",
   "metadata": {},
   "source": [
    "## End of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1c409d-2ad8-45d7-8bd5-5c5d95d11f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_one_hist(hist,name,nbins,xlims):\n",
    "    # tData = ROOT.TH1F(name,name,nbins,xlims[0],xlims[1])\n",
    "    tData = ROOT.TH1F(name,name,len(nbins)-1,array(\"d\",nbins))\n",
    "    for i in range(len(nbins)-1):\n",
    "        tData.SetBinContent(i+1,hist['hist'][i])\n",
    "        tData.SetBinError(i+1,hist['err'][i])\n",
    "    return tData\n",
    "    \n",
    "def SaveToRoot_new(nbins,xlims,hist_samples,fileName='test.root'): \n",
    "    rFile = ROOT.TFile(f'bdt_output/{fileName}','RECREATE')\n",
    "    for name in hist_samples:\n",
    "        tData = Save_one_hist(hist_samples[name],name,nbins,xlims)\n",
    "        rFile.Write()\n",
    "    #rFile.Write()\n",
    "    rFile.Close()\n",
    "\n",
    "def make_stat_err(hist, SF):\n",
    "    stat_err = []\n",
    "    for i in range(0,len(hist[0])):\n",
    "        error = np.sqrt(hist[0][i])*np.sqrt(SF)\n",
    "        stat_err.append(error)\n",
    "    return stat_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd99609-2216-4c00-982a-4d81c49eac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_writing = False\n",
    "with uproot.open(loc_hists+f'{Run}_{HNL_mass}MeV_logit_top_20_merged.root') as file: #Check what is already in the file (read-only)\n",
    "    print(type(file.keys()))\n",
    "    for name in hist_samples:\n",
    "        if str(name)+\";1\" in file.keys():\n",
    "            print(str(name) + \" ALREADY EXISTS, stopping\")\n",
    "            stop_writing = True\n",
    "            break\n",
    "        else:\n",
    "            print(name)\n",
    "            \n",
    "with uproot.update(loc_hists+f'{Run}_{HNL_mass}MeV_logit_top_20_merged.root') as file: #Add new hists into the file\n",
    "    # del file['ppfx_uncertainty']\n",
    "        \n",
    "    for name in hist_samples:\n",
    "        if stop_writing == True:\n",
    "            print(\"Not saving hist\")\n",
    "            break\n",
    "        file[name] = hist_samples[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f458655-5be5-404f-b0c3-ba1ecebd4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with uproot.update(loc_hists+f'{Run}_{HNL_mass}MeV_logit_top_20_merged.root') as file:\n",
    "# # file = uproot.update(loc_hists+f'{Run}_{HNL_mass}MeV_logit_top_20_merged.root')\n",
    "#     print(file.keys())\n",
    "#     del file['ppfx_uncertainty;1']\n",
    "#     print(file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafc945-a0ba-4234-beff-14d2658fcc9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a66c41-d9f4-44eb-8db2-c2a1db5aaef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveToRoot(nbins,xlims,uncertainty,name,fileName='test.root'):\n",
    "    nBins = nbins\n",
    "    binLimits = xlims\n",
    "  ### Save files \n",
    "    rFile = ROOT.TFile(f'Uncertainties/{fileName}','RECREATE')\n",
    "    tData1 = ROOT.TH1F(name,name,nBins,binLimits[0],binLimits[1])\n",
    "    for i in range(nBins):\n",
    "        tData1.SetBinContent(i+1,uncertainty[i])\n",
    "    rFile.Write()\n",
    "    rFile.Close()\n",
    "    \n",
    "def SaveToRoot_new(nbins,xlims,hist_samples,fileName='test.root'): \n",
    "    rFile = ROOT.TFile(f'Uncertainties/{fileName}','RECREATE')\n",
    "    for name in hist_samples:\n",
    "        tData1 = ROOT.TH1F(name,name,nbins,xlims[0],xlims[1])\n",
    "        for i in range(nbins): \n",
    "            tData1.SetBinContent(i+1,hist_samples[name][i])\n",
    "        rFile.Write()\n",
    "    rFile.Close()\n",
    "    \n",
    "def SaveToRoot_INPUT(bins_dict_single,hist_samples,fileName='test.root'): \n",
    "    rFile = ROOT.TFile(f'Uncertainties/{fileName}','RECREATE')\n",
    "    for name in hist_samples:\n",
    "        tData1 = ROOT.TH1F(name,name,len(bins_dict_single)-1,bins_dict_single[0],bins_dict_single[1])\n",
    "        for i in range(len(bins_dict_single)-1): \n",
    "            tData1.SetBinContent(i+1,hist_samples[name][i])\n",
    "        rFile.Write()\n",
    "    rFile.Close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cff32b-0081-49fd-a65d-31fb40563d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hists = {} #Full hists here, with all old entries and new uncertainties added\n",
    "\n",
    "SaveToRoot_INPUT(bins_dict_single,hist_samples,fileName=f'Test_3_{HNL_mass}MeV.root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd41779-5610-49cd-8118-026e38095258",
   "metadata": {},
   "outputs": [],
   "source": [
    "reweight_errs = {'ppfx_uncertainty': ppfx_unc, 'Genie_uncertainty':genie_unc, 'Reinteraction_uncertainty':reint_unc}\n",
    "reweight_errs_frac = {'ppfx_uncertainty_frac': ppfx_unc_frac, 'Genie_uncertainty_frac':genie_unc_frac, \n",
    "                      'Reinteraction_uncertainty_frac':reint_unc_frac}\n",
    "\n",
    "SaveToRoot(20,[0,1.0],ppfx_unc,'ppfx_uncertainty',fileName=f'Test_1_{HNL_mass}MeV.root')\n",
    "\n",
    "SaveToRoot_new(20,[0,1.0],reweight_errs,fileName=Run+f'_Test_2_{HNL_mass}MeV.root')\n",
    "SaveToRoot_new(20,[0,1.0],reweight_errs_frac,fileName=Run+f'_Test_2_frac_{HNL_mass}MeV.root')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9abae5-b7f7-474a-befc-60653f099ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
