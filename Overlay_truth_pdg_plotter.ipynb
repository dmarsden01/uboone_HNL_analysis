{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced5e536-c413-4447-b2d4-8b0ed1c50021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "import os,sys,string, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from platform import python_version\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import math\n",
    "from matplotlib.patches import Rectangle\n",
    "import pickle\n",
    "\n",
    "import Utilities.Plotter as PT\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Variables_list as Variables\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print ('Success')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce878578-1431-44ef-937e-f139e58caf9b",
   "metadata": {},
   "source": [
    "# Loading in the \"results\" dataframe after full selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a5f612-8b8c-427d-9201-adbecb1b2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = {\"Run\":\"run3\", #The run number, so far either \"run1\" or \"run3\"\n",
    "          \"Load_lepton_signal\":True, #Load ee BDTs\n",
    "          \"Load_pi0_signal\":False, #Load pi0 BDTs\n",
    "          \"Use_logit\":True} \n",
    "\n",
    "Run = Params[\"Run\"] #so far either \"run1\" or \"run3\"\n",
    "\n",
    "if Params[\"Load_lepton_signal\"] == True: HNL_masses = Constants.HNL_mass_samples\n",
    "if Params[\"Load_pi0_signal\"] == True: HNL_masses = Constants.HNL_mass_pi0_samples\n",
    "\n",
    "loc_pkl = f'pkl_files/{Run}/current_files/Results/'\n",
    "if Params[\"Load_pi0_signal\"] == True:loc_pkl+=\"pi0/\"\n",
    "\n",
    "\n",
    "BDT_name = \"_full_Finished_10\"\n",
    "filename = BDT_name\n",
    "\n",
    "overlay_results = pd.read_pickle(loc_pkl+f\"overlay_results{filename}.pkl\") #This will contain all of the BDT output scores and rse_id\n",
    "\n",
    "loc_hists = 'bdt_output/'\n",
    "if Params[\"Load_pi0_signal\"] == True:loc_hists+=\"pi0/\"\n",
    "\n",
    "name_type = Functions.Get_signal_name_type(Params)\n",
    "root_filename = name_type+filename\n",
    "bins_dict, bin_cents = {}, {}\n",
    "for HNL_mass in HNL_masses:\n",
    "    hist_placeholder = uproot.open(loc_hists+f'{Run}_{HNL_mass}_'+root_filename+\".root\")\n",
    "    bins_dict[HNL_mass] = hist_placeholder['bkg_overlay'].to_numpy()[1] #A tuple of bin edges\n",
    "    bin_cents[HNL_mass]=(bins_dict[HNL_mass][:-1]+bins_dict[HNL_mass][1:])/2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27bc9ea5-a3d8-49ee-81ff-b1ce6ae93441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "loc_hists = \"Uncertainties/\"\n",
    "if Params[\"Load_pi0_signal\"] == True:loc_hists+=\"pi0/\"\n",
    "\n",
    "name_type = Functions.Get_signal_name_type(Params)\n",
    "filename = name_type+'_EXT_full_Finished.root'\n",
    "\n",
    "frac_ppfx_flat = 0.2\n",
    "frac_genie_flat = 0.2\n",
    "frac_reweight_flat = 0.05\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    bins_cent=(bins_dict[HNL_mass][:-1]+bins_dict[HNL_mass][1:])/2\n",
    "    \n",
    "    score_hist = np.histogram(Functions.logit(overlay_results[f\"BDT_output_{HNL_mass}MeV\"]), weights=overlay_results[f\"weight\"], bins=bins_dict[HNL_mass], \n",
    "                              range=[bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1]])[0]\n",
    "\n",
    "    values_dict = {'ppfx_uncertainty': score_hist*frac_ppfx_flat, 'Genie_uncertainty':score_hist*frac_genie_flat,\n",
    "                   'Reinteraction_uncertainty': score_hist*frac_reweight_flat,\n",
    "                   'ppfx_uncertainty_frac': np.ones(len(score_hist))*frac_ppfx_flat, \n",
    "                   'Genie_uncertainty_frac':np.ones(len(score_hist))*frac_genie_flat,\n",
    "                   'Reinteraction_uncertainty_frac':np.ones(len(score_hist))*frac_reweight_flat} \n",
    "    hist_samples = {}\n",
    "\n",
    "    #make array with all values 1, then weight by value\n",
    "    for name in values_dict:\n",
    "        test_hist = np.histogram(bins_cent, weights=values_dict[name], bins=bins_dict[HNL_mass], \n",
    "                                 range=[bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1]])\n",
    "        hist_samples[name] = test_hist\n",
    "\n",
    "    stop_writing = False\n",
    "    dont_save = []\n",
    "    with uproot.open(loc_hists+Params[\"Run\"]+f'_{HNL_mass}_'+filename) as file:\n",
    "    # with uproot.open(loc_hists+f'{Run}_{HNL_mass}MeV_{filename}.root') as file: #Check what is already in the file (read-only)\n",
    "        for name in hist_samples:\n",
    "            if str(name)+\";1\" in file.keys():\n",
    "                # print(str(name) + f\" ALREADY EXISTS in {HNL_mass}MeV file, not re-saving\")\n",
    "                dont_save.append(name)\n",
    "\n",
    "    with uproot.update(loc_hists+f'{Run}_{HNL_mass}_{filename}') as file: #Add new hists into the file\n",
    "        # del file['ppfx_uncertainty']\n",
    "\n",
    "        for name in hist_samples:\n",
    "            # if stop_writing == True:\n",
    "            #     print(\"Not saving hist\")\n",
    "            #     break\n",
    "            if name in dont_save:\n",
    "                # print(f\"Not saving {name}\")\n",
    "                continue\n",
    "            else:\n",
    "                file[name] = hist_samples[name]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff28413-ae20-47cc-9464-4c260937d3e5",
   "metadata": {},
   "source": [
    "# Reading in the overlay .root file with reweight branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a4c5de-0103-44bf-8404-7463d7a77ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nu_pdg, ccnc, npi0\n",
    "sys_variables = Variables.event_vars + Variables.weight_related + [\"nu_pdg\", \"ccnc\", \"npi0\"]\n",
    "\n",
    "if Run == \"run1\":\n",
    "    NuMI_MC_overlay = uproot3.open('../NuMI_MC/SLIMMED_neutrinoselection_filt_run1_overlay.root')['nuselection/NeutrinoSelectionFilter']\n",
    "    Norm = Constants.SF_overlay_run1\n",
    "elif Run == \"run3\":\n",
    "    NuMI_MC_overlay = uproot3.open('../NuMI_MC/SLIMMED_neutrinoselection_filt_run3_overlay.root')['nuselection/NeutrinoSelectionFilter']\n",
    "    Norm = Constants.SF_overlay_run3\n",
    "\n",
    "df_overlay_weights = NuMI_MC_overlay.pandas.df(sys_variables, flatten=False) #Perhaps I can do this in a more clever way than just making a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c9948-3875-445d-aaaf-9c4e881d2969",
   "metadata": {},
   "source": [
    "# Keeping only the events which pass selection in the weight dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8885864f-01e9-48e0-b041-1407ba8a5a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events in weights file is 748702\n",
      "Number of events in results file is 14833\n"
     ]
    }
   ],
   "source": [
    "def make_unique_ev_id(df): #df must have 'run', 'sub' and 'evt' branches\n",
    "    if pd.Series(['run', 'sub', 'evt']).isin(df.columns).all():\n",
    "        rse_list = []\n",
    "        for entry in df.index: #Looping over all events in the dataframe\n",
    "            rse = str(df['run'][entry]) + \"_\" + str(df['sub'][entry]) + \"_\" + str(df['evt'][entry])\n",
    "            rse_list.append(rse)\n",
    "        df['rse_id'] = rse_list #Writing a new branch with the unique event id\n",
    "        return df.copy()\n",
    "    else:\n",
    "        print(\"Dataframe needs \\\"run\\\", \\\"sub\\\" and \\\"evt\\\" columns.\")\n",
    "        return 0\n",
    "    \n",
    "def check_duplicate_events(df):\n",
    "    rse_list = df['rse_id'].to_list()\n",
    "\n",
    "    seen = set()\n",
    "    dupes = []\n",
    "\n",
    "    for x in rse_list:\n",
    "        if x in seen:\n",
    "            dupes.append(x)\n",
    "        else:\n",
    "            seen.add(x)\n",
    "    print(\"Number of duplicates is \" + str(len(dupes)))\n",
    "    print(\"Number of unique events is \" + str(len(seen)))\n",
    "\n",
    "overlay_results_rse = make_unique_ev_id(overlay_results)\n",
    "df_overlay_weights_rse = make_unique_ev_id(df_overlay_weights)\n",
    "\n",
    "#Deleting any duplicates of events, should be able to avoid if correctly filtered for one event per row\n",
    "overlay_results_rse.drop_duplicates(subset=['rse_id'], keep='first', inplace=True)\n",
    "\n",
    "print(\"Number of events in weights file is \" + str(len(df_overlay_weights_rse)))\n",
    "print(\"Number of events in results file is \" + str(len(overlay_results_rse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00863fdf-66f9-4e0e-9e57-6e3db102152b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events in the filtered weights file is 14833\n",
      "Number of events in results file is 14833\n"
     ]
    }
   ],
   "source": [
    "#Keeping only those events in the final selection\n",
    "filtered_weights = df_overlay_weights_rse.loc[(df_overlay_weights_rse['rse_id'].isin(overlay_results_rse['rse_id']))]\n",
    "\n",
    "print(\"Number of events in the filtered weights file is \" + str(len(filtered_weights)))\n",
    "print(\"Number of events in results file is \" + str(len(overlay_results_rse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a9a5f9-bf48-45d7-a35b-909d3bc5eb88",
   "metadata": {},
   "source": [
    "## Calculating uncertainty for a BDT input variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db4a8c2-831d-41ae-a044-cede9b82ca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['run', 'sub', 'evt', 'weightSplineTimesTune', 'ppfx_cv', 'npi0',\n",
      "       'nu_pdg', 'ccnc', 'npi0', 'rse_id'],\n",
      "      dtype='object')\n",
      "Index(['run', 'sub', 'evt', 'weightSplineTimesTune', 'ppfx_cv', 'npi0',\n",
      "       'weight', 'BDT_output_2MeV', 'BDT_output_10MeV', 'BDT_output_20MeV',\n",
      "       'BDT_output_50MeV', 'BDT_output_100MeV', 'BDT_output_150MeV', 'rse_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(filtered_weights.keys())\n",
    "print(overlay_results_rse.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22c5f798-6355-4175-a963-48924bdcc2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BDT_score_list = []\n",
    "\n",
    "# HNL_masses = [20, 50, 100, 150, 180, 200] #Should get rid of this once made overlay branches with all results\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    BDT_score_list.append(f'BDT_output_{HNL_mass}MeV')\n",
    "\n",
    "just_score_df = overlay_results_rse[BDT_score_list + ['rse_id','weight']].copy()\n",
    "\n",
    "final_merged = pd.merge(filtered_weights,just_score_df, how='outer', on=['rse_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e1cf021-00ba-4d5e-afff-8e691e930abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['run', 'sub', 'evt', 'weightSplineTimesTune', 'ppfx_cv', 'npi0',\n",
       "       'nu_pdg', 'ccnc', 'npi0', 'rse_id', 'BDT_output_2MeV',\n",
       "       'BDT_output_10MeV', 'BDT_output_20MeV', 'BDT_output_50MeV',\n",
       "       'BDT_output_100MeV', 'BDT_output_150MeV', 'weight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_merged.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae7241c6-5a37-4da6-ad61-805cf7040428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal region is above 0.9933071490757152\n"
     ]
    }
   ],
   "source": [
    "HNL_mass = 10\n",
    "signal_region = 5\n",
    "\n",
    "var = f'BDT_output_{HNL_mass}MeV'\n",
    "overlay = final_merged\n",
    "\n",
    "signal_region_invlogit = Functions.invlogit(signal_region)\n",
    "print(f\"Signal region is above {signal_region_invlogit}\")\n",
    "\n",
    "overlay_signal_region = overlay.query(var+\">\"+str(signal_region_invlogit))\n",
    "\n",
    "numu_CC_npi0_ls = '((nu_pdg==14 or nu_pdg==-14) and ccnc==0 and npi0>=1)'\n",
    "numu_CC_0pi0 = '((nu_pdg==14 or nu_pdg==-14) and ccnc==0 and npi0==0)'\n",
    "numu_NC_npi0_ls = '((nu_pdg==14 or nu_pdg==-14) and ccnc==1 and npi0>=1)'\n",
    "numu_NC_0pi0 = '((nu_pdg==14 or nu_pdg==-14) and ccnc==1 and npi0==0)'\n",
    "nue_ccnc_ls = '(nu_pdg==12 or nu_pdg==-12)'\n",
    "other = 'nu_pdg != nu_pdg'\n",
    "\n",
    "NC_pi0_sample = overlay.query(numu_NC_npi0_ls)\n",
    "\n",
    "# stacked_hist = [Functions.logit(overlay.query(numu_CC_npi0_ls)[var]), #var is the BDT score\n",
    "#                 Functions.logit(overlay.query(numu_CC_0pi0)[var]),\n",
    "#                 Functions.logit(overlay.query(numu_NC_npi0_ls)[var]),\n",
    "#                 Functions.logit(overlay.query(numu_NC_0pi0)[var]),\n",
    "#                 Functions.logit(overlay.query(nue_ccnc_ls)[var])]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26962d57-250a-4ff0-9482-760205d8d079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3          17055_98_4930\n",
      "8          17048_51_2577\n",
      "14        16949_107_5370\n",
      "16       16874_324_16237\n",
      "20       15799_227_11363\n",
      "              ...       \n",
      "14800        14209_4_210\n",
      "14815        16432_3_198\n",
      "14816      16432_61_3072\n",
      "14821        15267_6_322\n",
      "14822        15267_6_337\n",
      "Name: rse_id, Length: 1714, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(NC_pi0_sample[\"rse_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94937e9b-c56e-4c30-85e9-fba66151d294",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(NC_pi0_sample['nu_pdg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f109105d-83db-426e-9975-edee92267dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signal region is above 0.9820137900379085\n"
     ]
    }
   ],
   "source": [
    "HNL_mass = 10\n",
    "signal_region = 4\n",
    "\n",
    "var = f'BDT_output_{HNL_mass}MeV'\n",
    "overlay = final_merged\n",
    "\n",
    "signal_region_invlogit = Functions.invlogit(signal_region)\n",
    "print(f\"Signal region is above {signal_region_invlogit}\")\n",
    "\n",
    "overlay_signal_region = overlay.query(var+\">\"+str(signal_region_invlogit))\n",
    "\n",
    "numu_CC_npi0_ls = '((nu_pdg==14 or nu_pdg==-14) and ccnc==0 and npi0>=1)'\n",
    "numu_CC_0pi0 = '((nu_pdg==14 or nu_pdg==-14) and ccnc==0 and npi0==0)'\n",
    "numu_NC_npi0_ls = '((nu_pdg==14 or nu_pdg==-14) and ccnc==1 and npi0>=1)'\n",
    "numu_NC_0pi0 = '((nu_pdg==14 or nu_pdg==-14) and ccnc==1 and npi0==0)'\n",
    "nue_ccnc_ls = '(nu_pdg==12 or nu_pdg==-12)'\n",
    "other = 'nu_pdg != nu_pdg'\n",
    "\n",
    "NC_pi0_sample_signal_like = overlay_signal_region.query(numu_NC_npi0_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c96dac3-aac1-413e-bcc3-5f8389b01cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(NC_pi0_sample_signal_like))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daa9e1f3-fb97-4479-aa9b-8d07b631b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e01b7b91-c4c4-490d-9670-0c9d4f361cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving NC pi0 event numbers\n",
    "csv_location = \"bdt_output/signal_like_rse_lists/\"\n",
    "signal_sample = \"10_ee\"\n",
    "\n",
    "overlay_run_sub_event = NC_pi0_sample_signal_like[[\"run\", \"sub\", \"evt\"]]\n",
    "csv_overlay_name = csv_location + Params[\"Run\"] + f\"_NC_pi0_overlay_{signal_sample}.csv\"\n",
    "overlay_run_sub_event.to_csv(csv_overlay_name, sep=\" \", header=False, index=False)\n",
    "\n",
    "csv_file = csv_overlay_name\n",
    "txt_file = csv_location + Params[\"Run\"] + f\"_NC_pi0_overlay_{signal_sample}.list\" #could save this as .txt, just following Owen's naming system so using .list\n",
    "with open(txt_file, \"w\") as my_output_file:\n",
    "    with open(csv_file, \"r\") as my_input_file:\n",
    "        [ my_output_file.write(\" \".join(row)+'\\n') for row in csv.reader(my_input_file)]\n",
    "    my_output_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbe356-5c11-4e10-87a9-e92e37ed7464",
   "metadata": {},
   "source": [
    "## Merging into one dataframe with results and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe6ac9-57d4-4107-9628-0c6937f0f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_overflow_bin(bins_dict, bins_cents_dict):\n",
    "    \"\"\"\n",
    "    For making the final \"overflow\" bin the same size as the previous bins, i.e one integer in width.\n",
    "    \"\"\"\n",
    "    bins_overflow, bins_cent_overflow = {}, {}\n",
    "    for HNL_mass in bins_dict:\n",
    "        overflow_bin = bins_cents_dict[HNL_mass][-2]+1 #Just adding one to the penultimate bin centre val. \n",
    "        bins_cent_overflow[HNL_mass] = bins_cents_dict[HNL_mass].copy()\n",
    "        bins_cent_overflow[HNL_mass][-1] = overflow_bin\n",
    "        bins_overflow[HNL_mass] = bins_dict[HNL_mass].copy()\n",
    "        bins_overflow[HNL_mass][-1] = bins_dict[HNL_mass][-2]+1 #Just adding one to the penultimate bin end val. \n",
    "    return bins_overflow, bins_cent_overflow\n",
    "\n",
    "bins_overflow, bins_cents_overflow = make_overflow_bin(bins_dict, bin_cents)\n",
    "\n",
    "def make_xlims_dict(bins_dict, spacing, lower = None):\n",
    "    \"\"\"\n",
    "    Making a dict of xlims for plotting several mass points at once.\n",
    "    Also returns a dict of xticks for the purpose of indicating the overflow.\n",
    "    \"\"\"\n",
    "    xlims_adjusted, xticks_adjusted = {}, {}\n",
    "    vals_dict={}\n",
    "    for HNL_mass in bins_dict:\n",
    "        if isinstance(lower,(int, float)): lower_val = lower\n",
    "        else: lower_val = bins_dict[HNL_mass][0]\n",
    "        xlims_adjusted[HNL_mass] = [lower_val,bins_dict[HNL_mass][-1]]\n",
    "        ticks = np.arange(bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1], spacing)\n",
    "        if ticks[-1] != bins_dict[HNL_mass][-2]: ticks = np.append(ticks, bins_dict[HNL_mass][-1]-1)\n",
    "        ticks_strings = []\n",
    "        vals = []\n",
    "        for val in ticks:\n",
    "            ticks_strings.append(str(int(val)))\n",
    "            vals.append(val)\n",
    "        ticks_strings[-1] = str(ticks_strings[-1])+\"+\"\n",
    "        xticks_adjusted[HNL_mass] = ticks_strings\n",
    "        vals_dict[HNL_mass] = vals\n",
    "        \n",
    "    return xlims_adjusted, xticks_adjusted, vals_dict\n",
    "\n",
    "xlims_dict, xticks_dict, vals_dict = make_xlims_dict(bins_overflow, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff3ffe-94dd-474f-ac1d-9bcf444ac25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xticks_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65b95e-e6a7-4d67-a8e3-84222bbaf6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass = 150\n",
    "plt.hist(Functions.logit(final_merged[f'BDT_output_{mass}MeV']),bins=bins_dict[mass])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30257e1f-4ef4-414d-bb54-87cc926fb6d4",
   "metadata": {},
   "source": [
    "## Plotting the different overlay contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13056495-dc9d-4dc8-b7c3-85c7dea92c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start of Aditya's code\n",
    "def Plot_overlay_contributions(df, HNL_mass, bins_dict, bins_original, bins_cent_dict, signal_region, xticks=[], vals_dict=[], xlims=[], ylims=[]):\n",
    "    \"\"\"\n",
    "    Given an overlay dataframe with the MC truth information, plots the BDT score with a breakdown of the overlay contributions.\n",
    "    \"\"\"\n",
    "    plt.rcParams.update({'font.size': 28})\n",
    "    \n",
    "    plt.figure(figsize=[10,7])\n",
    "    \n",
    "    var = f'BDT_output_{HNL_mass}MeV'\n",
    "    overlay = df\n",
    "    \n",
    "    signal_region_invlogit = Functions.invlogit(signal_region)\n",
    "    print(f\"Signal region is above {signal_region_invlogit}\")\n",
    "    print(type(signal_region_invlogit))\n",
    "    \n",
    "    overlay_signal_region = overlay.query(var+\">\"+str(signal_region_invlogit))\n",
    "    \n",
    "    numu_CC_npi0_ls = '((nu_pdg==14 or nu_pdg==-14) and ccnc==0 and npi0>=1)'\n",
    "    numu_CC_0pi0 = '((nu_pdg==14 or nu_pdg==-14) and ccnc==0 and npi0==0)'\n",
    "    numu_NC_npi0_ls = '((nu_pdg==14 or nu_pdg==-14) and ccnc==1 and npi0>=1)'\n",
    "    numu_NC_0pi0 = '((nu_pdg==14 or nu_pdg==-14) and ccnc==1 and npi0==0)'\n",
    "    nue_ccnc_ls = '(nu_pdg==12 or nu_pdg==-12)'\n",
    "    other = 'nu_pdg != nu_pdg'\n",
    "    \n",
    "    \n",
    "    stacked_hist = [Functions.logit(overlay.query(numu_CC_npi0_ls)[var]), #var is the BDT score\n",
    "                    Functions.logit(overlay.query(numu_CC_0pi0)[var]),\n",
    "                    Functions.logit(overlay.query(numu_NC_npi0_ls)[var]),\n",
    "                    Functions.logit(overlay.query(numu_NC_0pi0)[var]),\n",
    "                    Functions.logit(overlay.query(nue_ccnc_ls)[var])]\n",
    "    \n",
    "    WEIGHTS = [overlay.query(numu_CC_npi0_ls)[\"weight\"],\n",
    "               overlay.query(numu_CC_0pi0)[\"weight\"],\n",
    "               overlay.query(numu_NC_npi0_ls)[\"weight\"],\n",
    "               overlay.query(numu_NC_0pi0)[\"weight\"],\n",
    "               overlay.query(nue_ccnc_ls)[\"weight\"]]\n",
    "    \n",
    "    weights_signal_region = [overlay_signal_region.query(numu_CC_npi0_ls)[\"weight\"],\n",
    "                             overlay_signal_region.query(numu_CC_0pi0)[\"weight\"],\n",
    "                             overlay_signal_region.query(numu_NC_npi0_ls)[\"weight\"],\n",
    "                             overlay_signal_region.query(numu_NC_0pi0)[\"weight\"],\n",
    "                             overlay_signal_region.query(nue_ccnc_ls)[\"weight\"]]\n",
    "    \n",
    "    Total_overlay_weights = sum(overlay[\"weight\"])\n",
    "    Total_signal_region_weights = sum(overlay_signal_region[\"weight\"])\n",
    "    print(f\"Total overlay weights is {Total_overlay_weights}\")\n",
    "    sum_weights = []\n",
    "    signal_weights = []\n",
    "    Total_weights = 0\n",
    "    for i, weights in enumerate(WEIGHTS):\n",
    "        sum_weights.append(sum(weights))\n",
    "        signal_weights.append(sum(weights_signal_region[i]))\n",
    "        Total_weights += sum(weights)\n",
    "        \n",
    "    print(f\"Total contribution weights is {Total_weights}\")\n",
    "    fractions_each = []\n",
    "    frac_signal = []\n",
    "    for i, weights in enumerate(sum_weights):\n",
    "        frac = weights/Total_weights\n",
    "        frac_region = signal_weights[i]/Total_signal_region_weights\n",
    "        fractions_each.append(frac)\n",
    "        frac_signal.append(frac_region)\n",
    "    \n",
    "    COLOR = ['steelblue','palegreen','green','y','olive']\n",
    "    \n",
    "    varis=stacked_hist\n",
    "    weights=WEIGHTS\n",
    "    colors=COLOR\n",
    "    labels=[r\"$\\nu_{\\mu}$ CC $n\\pi^{0}$\", r\"$\\nu_{\\mu}$ CC $0\\pi^{0}$\", r\"$\\nu_{\\mu}$ NC $n\\pi^{0}$\", r\"$\\nu_{\\mu}$ NC $0\\pi^{0}$\", r\"$\\nu_{e}$\"] \n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        print(f\"{label} fraction is \" + str(fractions_each[i]))\n",
    "        \n",
    "    for i, label in enumerate(labels):\n",
    "        print(f\"{label} fraction in signal region is \" + str(frac_signal[i]))\n",
    "        \n",
    "    \n",
    "    all_vals = []\n",
    "    bins_cents = []\n",
    "    for i, label in enumerate(labels):\n",
    "        all_vals.append(np.histogram(stacked_hist[i], weights=WEIGHTS[i], bins=bins_original[HNL_mass])[0])\n",
    "        bins_cents.append(bins_cent_dict[HNL_mass])\n",
    "\n",
    "    # plot=plt.hist(varis,\n",
    "    #               bins=bins_dict[HNL_mass],\n",
    "    #               label=labels,\n",
    "    #               histtype=\"stepfilled\",\n",
    "    #               stacked=True,density=True,linewidth=2,edgecolor=\"darkblue\",\n",
    "    #               weights=weights, color=colors)\n",
    "    \n",
    "    plot=plt.hist(bins_cents,\n",
    "                  bins=bins_dict[HNL_mass],\n",
    "                  label=labels,\n",
    "                  histtype=\"stepfilled\",\n",
    "                  stacked=True,density=True,linewidth=2,edgecolor=\"black\",\n",
    "                  weights=all_vals, color=colors)\n",
    "    \n",
    "    if isinstance(xticks,dict):\n",
    "        plt.xticks(ticks=vals_dict[HNL_mass], labels=xticks[HNL_mass])\n",
    "    \n",
    "    if xlims!=[]:\n",
    "        plt.xlim(xlims)\n",
    "        \n",
    "    if ylims!=[]:\n",
    "        plt.ylim(ylims)\n",
    "    \n",
    "    plt.legend(fontsize=20)\n",
    "    \n",
    "    plt.xlabel(f'BDT Score '+r'($m_{\\mathrm{HNL}}=$'+f'{HNL_mass} MeV)', fontsize=30)\n",
    "    plt.ylabel(\"Fraction of events\", fontsize=30)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9e1e2-24f9-479d-9d51-bc58730a3e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Params[\"Load_lepton_signal\"] == True: HNL_mass = 100\n",
    "if Params[\"Load_pi0_signal\"] == True: HNL_mass = 200\n",
    "\n",
    "if Params[\"Load_lepton_signal\"] == True: decay_type = \"ee\"\n",
    "if Params[\"Load_pi0_signal\"] == True: decay_type = \"pi0\"\n",
    "\n",
    "Plot_overlay_contributions(final_merged, HNL_mass, bins_overflow, bins_dict, bins_cents_overflow, signal_region = 3, \n",
    "                           xticks=xticks_dict, vals_dict=vals_dict, \n",
    "                           xlims=[3, 6], ylims=[0, 0.004])\n",
    "save_fig = input(\"Do you want to save the new BDT scores to .pkl files? y/n \")\n",
    "if save_fig == \"y\":\n",
    "    plt.savefig(f\"plots/BDT_output/overlay_composition/overlay_score_breakdown_signal_region_{HNL_mass}_{decay_type}_model.pdf\")\n",
    "    plt.savefig(f\"plots/BDT_output/overlay_composition/overlay_score_breakdown_signal_region_{HNL_mass}_{decay_type}_model.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc3001-d33a-44be-b3f5-efebce66a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_overlay_contributions(final_merged, HNL_mass, bins_overflow, bins_dict, bins_cents_overflow, signal_region = 3, \n",
    "                           xticks=xticks_dict, vals_dict=vals_dict, \n",
    "                           xlims=[], ylims=[])\n",
    "save_fig = input(\"Do you want to save the new BDT scores to .pkl files? y/n \")\n",
    "if save_fig == \"y\":\n",
    "    plt.savefig(f\"plots/BDT_output/overlay_composition/overlay_score_breakdown_full_dist_{HNL_mass}_{decay_type}_model.pdf\")\n",
    "    plt.savefig(f\"plots/BDT_output/overlay_composition/overlay_score_breakdown_full_dist_{HNL_mass}_{decay_type}_model.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1cb880-1197-4d4e-b75e-5b93223a1e21",
   "metadata": {},
   "source": [
    "# Plotting the BDT score with all different multisims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b551c-b4ca-4171-bc17-c89a78fe522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def All_reweight_err(df, var_name, BINS, x_range, logit=True):\n",
    "    results_dict = {}\n",
    "    n_bins = len(BINS)-1\n",
    "    for Multisim in Constants.Multisim_univs:\n",
    "        Nuniverse = Constants.Multisim_univs[Multisim]\n",
    "        n_tot = np.empty([Nuniverse, n_bins])\n",
    "        n_cv_tot = np.empty(n_bins)\n",
    "        n_tot.fill(0)\n",
    "        n_cv_tot.fill(0)\n",
    "        \n",
    "        if logit == True:\n",
    "            variable = Functions.logit(df[var_name]) #The BDT output score\n",
    "        else:\n",
    "            variable = df[var_name] #The BDT output score\n",
    "        syst_weights = df[Multisim] #An array of length of the number of events, each entry is an array of length Nunivs\n",
    "        spline_fix_cv  = df[\"weight\"]#*Norm #This needs to be normalized by the POT scaling for correct values\n",
    "        spline_fix_var = df[\"weight\"]#*Norm\n",
    "        \n",
    "        s = syst_weights\n",
    "        df_weights = pd.DataFrame(s.values.tolist())\n",
    "        n_cv, bins = np.histogram(variable, range=x_range, bins=BINS, weights=spline_fix_cv)\n",
    "        n_cv_tot += n_cv\n",
    "        \n",
    "        if(Multisim == \"weightsGenie\"): #special treatment as [\"weightSplineTimesTune\"] is included in genie weights\n",
    "            if not df_weights.empty:\n",
    "                for i in range(Nuniverse):\n",
    "                    weight = df_weights[i].values / 1000.\n",
    "                    weight[weight == 1]= df[\"weightSplineTimesTune\"].iloc[weight == 1]\n",
    "                    weight[np.isnan(weight)] = df[\"weightSplineTimesTune\"].iloc[np.isnan(weight)]\n",
    "                    weight[weight > 50] = df[\"weightSplineTimesTune\"].iloc[weight > 50] # why 30 not 50?\n",
    "                    weight[weight <= 0] = df[\"weightSplineTimesTune\"].iloc[weight <= 0]\n",
    "                    weight[weight == np.inf] = df[\"weightSplineTimesTune\"].iloc[weight == np.inf]\n",
    "                \n",
    "                    n, bins = np.histogram(variable, \n",
    "                                           weights=np.nan_to_num(weight*spline_fix_var/df[\"weightSplineTimesTune\"]), range=x_range, bins=BINS)\n",
    "                    n_tot[i] += n\n",
    "                    \n",
    "        if(Multisim == \"weightsPPFX\"): #special treatment as [\"PPFXPcv\"] is included in ppfx weights\n",
    "            if not df_weights.empty:\n",
    "                for i in range(Nuniverse):\n",
    "                    weight = df_weights[i].values / 1000.\n",
    "                    weight[weight == 1]= df[\"ppfx_cv\"].iloc[weight == 1]\n",
    "                    weight[np.isnan(weight)] = df[\"ppfx_cv\"].iloc[np.isnan(weight)]\n",
    "                    weight[weight > 100] = df[\"ppfx_cv\"].iloc[weight > 100]\n",
    "                    weight[weight < 0] = df[\"ppfx_cv\"].iloc[weight < 0]\n",
    "                    weight[weight == np.inf] = df[\"ppfx_cv\"].iloc[weight == np.inf]\n",
    "                \n",
    "                    n, bins = np.histogram(variable, weights=weight*np.nan_to_num(spline_fix_var/df[\"ppfx_cv\"]), range=x_range, bins=BINS)\n",
    "                    n_tot[i] += n\n",
    "        \n",
    "        if(Multisim == \"weightsReint\"):\n",
    "            if not df_weights.empty:\n",
    "                for i in range(Nuniverse):\n",
    "                    weight = df_weights[i].values / 1000.\n",
    "                    weight[np.isnan(weight)] = 1\n",
    "                    weight[weight > 100] = 1\n",
    "                    weight[weight < 0] = 1\n",
    "                    weight[weight == np.inf] = 1\n",
    "                    n, bins = np.histogram(variable, weights=weight*spline_fix_var, range=x_range, bins=BINS)\n",
    "                    n_tot[i] += n\n",
    "        cov = np.empty([len(n_cv), len(n_cv)])\n",
    "        cov.fill(0)\n",
    "\n",
    "        for n in n_tot:\n",
    "            for i in range(len(n_cv)):\n",
    "                for j in range(len(n_cv)):\n",
    "                    cov[i][j] += (n[i] - n_cv_tot[i]) * (n[j] - n_cv_tot[j])\n",
    "\n",
    "        cov /= Nuniverse\n",
    "        results_dict[Multisim] = [cov,n_cv_tot,n_tot,bins]\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42735701-a7c6-42ff-992f-24424ae620ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HNL_mass = 50\n",
    "logit = True\n",
    "results_dict = {}\n",
    "for HNL_mass in HNL_masses:\n",
    "    print(f\"Calculating {HNL_mass} uncertainties.\")\n",
    "    results_dict[HNL_mass] = All_reweight_err(final_merged, f'BDT_output_{HNL_mass}MeV', bins_dict[HNL_mass],\n",
    "                                    [bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1]], logit=logit)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f70046-7a1a-4e66-a746-ef1d80469b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[HNL_mass].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3f472-a3ae-40ad-8c0a-1473efb9b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_multisim(results_dict, HNL_mass, bins_dict, bin_cents, xticks_dict, xticks_vals, \n",
    "                  savefig=False, display=False, Params={}):\n",
    "    translate_dict = {'weightsPPFX':\"Flux\", 'weightsGenie':\"Genie\", 'weightsReint':\"Reinteractions\"}\n",
    "    for Multisim in results_dict:\n",
    "        Multisim_name = translate_dict[Multisim]\n",
    "        Nuniverse = Constants.Multisim_univs[Multisim]\n",
    "        cov = results_dict[Multisim][0]\n",
    "        cv = results_dict[Multisim][1]\n",
    "        n_tot = results_dict[Multisim][2]\n",
    "        # bins = results_dict[Multisim][3]\n",
    "        bins = bins_dict[HNL_mass]\n",
    "        xlims = [min(bins), max(bins)]\n",
    "        \n",
    "        fig,ax = plt.subplots(nrows=2, ncols=1, sharex=True, gridspec_kw={'height_ratios': [3, 1]}, figsize=[10,10],dpi=200)\n",
    "        plt.sca(ax[0])\n",
    "\n",
    "        # bins=np.linspace(0,1.0,21)\n",
    "\n",
    "        # bins_cent=(bins[:-1]+bins[1:])/2\n",
    "        bins_cent=bin_cents[HNL_mass]\n",
    "        bins_centlong=np.tile(bins_cent,Nuniverse)\n",
    "\n",
    "        nybins=70\n",
    "\n",
    "        plt.title(Multisim_name + \" Variations\",fontsize=20)\n",
    "\n",
    "        plt.hist(bins_cent,bins,weights=cv,color=\"red\",histtype=\"step\",label=\"Central Value\",lw=2,linestyle='-')\n",
    "        plt.legend()\n",
    "        bins_cent=(bins[:-1]+bins[1:])/2\n",
    "        bins_centlong=np.tile(bins_cent,Nuniverse)\n",
    "\n",
    "        plt.ylabel(\"Events\")\n",
    "        plt.hist2d(bins_centlong,n_tot.flatten(),bins=[bins,nybins],cmin=1,range=[xlims,[0,max(cv)*1.4]],rasterized=True)\n",
    "\n",
    "        plt.colorbar(pad=0,use_gridspec=True)\n",
    "        #fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap),pad=0,use_gridspec=True)\n",
    "        plt.ylim(0,max(cv)*1.4)\n",
    "        # plt.yscale(\"log\")\n",
    "\n",
    "        plt.sca(ax[1])\n",
    "        \n",
    "        #ax[1].tick_params(labelright=False, length=0)\n",
    "        pos = ax[0].get_position()\n",
    "        pos2 = ax[1].get_position()\n",
    "        ax[1].set_position([pos.x0,pos2.y0,pos.width,pos2.height])\n",
    "        \n",
    "        plt.hist(bins_cent,bins,weights=np.sqrt(np.diag(cov))/cv*100,color=\"black\",histtype=\"step\",lw=3,linestyle='-')\n",
    "        maxy = 1.5*max(plt.hist(bins_cent,bins,weights=np.sqrt(np.diag(cov))/cv*100,color=\"black\",histtype=\"step\",lw=3,linestyle='-')[0])\n",
    "        plt.ylim(0,maxy)\n",
    "        plt.ylabel(\"% Uncertainity\")\n",
    "        #plt.yticks([])\n",
    "        plt.xlabel(f'BDT Score '+r'($m_{\\mathrm{HNL}}=$'+f'{HNL_mass} MeV)', fontsize=25)\n",
    "        # plt.tight_layout()\n",
    "        plt.xticks(ticks=xticks_vals[HNL_mass], labels=xticks_dict[HNL_mass])\n",
    "        \n",
    "        if Params[\"Load_lepton_signal\"] == True:sig_type = \"ee_\"\n",
    "        if Params[\"Load_pi0_signal\"] == True:sig_type = \"pi0_\"\n",
    "        save_loc = \"plots/Sys_uncertainty/Overlay/Multisim/bkg_multisim_\"\n",
    "        \n",
    "        if savefig == True:\n",
    "            plt.savefig(save_loc + Run + \"_\" + str(HNL_mass) + f\"_{sig_type}{Multisim}.pdf\")\n",
    "            plt.savefig(save_loc + Run + \"_\" + str(HNL_mass) + f\"_{sig_type}{Multisim}.png\")\n",
    "        if display == False:\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989da16-a516-4952-b1ea-19220aab0246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HNL_masses = Constants.HNL_mass_samples\n",
    "for HNL_mass in HNL_masses:\n",
    "    # Plot_multisim(results_dict[HNL_mass], HNL_mass, savefig=False, display=True, Params=Params)\n",
    "    Plot_multisim(results_dict[HNL_mass], HNL_mass, bins_overflow, bins_cents_overflow, xticks_dict, vals_dict, \n",
    "                  savefig=False, display=False, Params=Params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b39f4-92e4-409a-a726-9c8af7380f5f",
   "metadata": {},
   "source": [
    "## Calculating and saving total uncertainty for each reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1dcc2d-2428-414e-820d-daec2265ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mass = 150\n",
    "# print(len(results_dict[mass][\"weightsGenie\"][0]))\n",
    "# print(results_dict[mass][\"weightsGenie\"][0][-1])\n",
    "\n",
    "# print(results_dict[mass][\"weightsPPFX\"][0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef060edf-eb74-44b1-8eab-e1530660d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.diag(results_dict[mass][\"weightsGenie\"][0]))\n",
    "print(results_dict[mass][\"weightsGenie\"][1])\n",
    "\n",
    "print(np.diag(results_dict[mass][\"weightsPPFX\"][0]))\n",
    "print(results_dict[mass][\"weightsPPFX\"][1])\n",
    "\n",
    "genie_unc = np.sqrt(np.diag(results_dict[mass][\"weightsGenie\"][0]))\n",
    "print(genie_unc)\n",
    "print(genie_unc/results_dict[mass][\"weightsGenie\"][1])\n",
    "\n",
    "print(type(genie_unc/results_dict[mass][\"weightsGenie\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a9093-1d2d-41c4-8e7f-4f73c10e9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppfx_unc_dict, ppfx_unc_frac_dict = {}, {}\n",
    "genie_unc_dict, genie_unc_frac_dict = {}, {}\n",
    "reint_unc_dict, reint_unc_frac_dict = {}, {}\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    \n",
    "    diagonal_ppfx = np.diag(results_dict[HNL_mass][\"weightsPPFX\"][0]).copy()\n",
    "    diagonal_genie = np.diag(results_dict[HNL_mass][\"weightsGenie\"][0]).copy()\n",
    "    diagonal_reint = np.diag(results_dict[HNL_mass][\"weightsReint\"][0]).copy()\n",
    "    \n",
    "    num_ppfx = results_dict[HNL_mass][\"weightsPPFX\"][1].copy()\n",
    "    num_genie = results_dict[HNL_mass][\"weightsGenie\"][1].copy()\n",
    "    num_reint = results_dict[HNL_mass][\"weightsReint\"][1].copy()\n",
    "    if np.any(diagonal_ppfx==0):\n",
    "        print(f\"{HNL_mass}MeV contains zeros, setting error to that of prev bin\")\n",
    "    \n",
    "    index = np.where(diagonal_ppfx == 0)\n",
    "    diagonal_ppfx[diagonal_ppfx==0] = 1\n",
    "    diagonal_genie[diagonal_genie==0] = 1\n",
    "    diagonal_reint[diagonal_reint==0] = 1\n",
    "    \n",
    "    num_ppfx[num_ppfx==0] = 1\n",
    "    num_genie[num_genie==0] = 1\n",
    "    num_reint[num_reint==0] = 1\n",
    "    \n",
    "    index_ppfx, index_genie, index_reint = [], [], []\n",
    "    \n",
    "    \n",
    "    ppfx_unc_dict[HNL_mass] = np.sqrt(diagonal_ppfx)\n",
    "    ppfx_unc_frac_dict[HNL_mass] = ppfx_unc_dict[HNL_mass]/num_ppfx\n",
    "    if np.any(ppfx_unc_dict[HNL_mass]/num_ppfx>0.40):\n",
    "        print(f\"{HNL_mass}MeV PPFX has large error, setting error to that of prev bin\")\n",
    "        index_ppfx = np.where(ppfx_unc_dict[HNL_mass]/num_ppfx>0.40)\n",
    "\n",
    "    genie_unc_dict[HNL_mass] = np.sqrt(diagonal_genie)\n",
    "    genie_unc_frac_dict[HNL_mass] = genie_unc_dict[HNL_mass]/num_genie\n",
    "    if np.any(genie_unc_dict[HNL_mass]/num_genie>0.40):\n",
    "        print(f\"{HNL_mass}MeV Genie has large error, setting error to that of prev bin\")\n",
    "        index_genie = np.where(genie_unc_dict[HNL_mass]/num_genie>0.40)\n",
    "\n",
    "    reint_unc_dict[HNL_mass] = np.sqrt(diagonal_reint)\n",
    "    reint_unc_frac_dict[HNL_mass] = reint_unc_dict[HNL_mass]/num_reint\n",
    "    if np.any(reint_unc_dict[HNL_mass]/num_reint>0.20):\n",
    "        print(f\"{HNL_mass}MeV reint has large error, setting error to that of prev bin\")\n",
    "        index_reint = np.where(reint_unc_dict[HNL_mass]/num_reint>0.20)\n",
    "    \n",
    "    #Fixing zero-bins\n",
    "    for i in index:\n",
    "        ppfx_unc_frac_dict[HNL_mass][i] = ppfx_unc_frac_dict[HNL_mass][i-1]\n",
    "        genie_unc_frac_dict[HNL_mass][i] = genie_unc_frac_dict[HNL_mass][i-1]\n",
    "        reint_unc_frac_dict[HNL_mass][i] = reint_unc_frac_dict[HNL_mass][i-1]\n",
    "        \n",
    "        ppfx_unc_dict[HNL_mass][i] = ppfx_unc_dict[HNL_mass][i-1]\n",
    "        genie_unc_dict[HNL_mass][i] = genie_unc_dict[HNL_mass][i-1]\n",
    "        reint_unc_dict[HNL_mass][i] = reint_unc_dict[HNL_mass][i-1]\n",
    "    \n",
    "    #Fixing high error bins\n",
    "    for i in index_ppfx:\n",
    "        ppfx_unc_frac_dict[HNL_mass][i] = ppfx_unc_frac_dict[HNL_mass][i-1]\n",
    "        ppfx_unc_dict[HNL_mass][i] = ppfx_unc_dict[HNL_mass][i-1]\n",
    "    for i in index_genie:\n",
    "        genie_unc_frac_dict[HNL_mass][i] = genie_unc_frac_dict[HNL_mass][i-1]\n",
    "        genie_unc_dict[HNL_mass][i] = genie_unc_dict[HNL_mass][i-1]\n",
    "    for i in index_reint:\n",
    "        reint_unc_frac_dict[HNL_mass][i] = reint_unc_frac_dict[HNL_mass][i-1]\n",
    "        reint_unc_dict[HNL_mass][i] = reint_unc_dict[HNL_mass][i-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2987cd53-e935-4752-b7ab-f04a621988a4",
   "metadata": {},
   "source": [
    "## Saving reweighting uncertainties as .root files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69c70f-afb2-40a5-8466-c07471150d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_hists = \"Uncertainties/\"\n",
    "if Params[\"Load_pi0_signal\"] == True:loc_hists+=\"pi0/\"\n",
    "\n",
    "name_type = Functions.Get_signal_name_type(Params)\n",
    "filename=name_type+BDT_name+'.root'\n",
    "# filename = name_type+'_EXT_full_Finished.root'\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    bins_cent=(bins_dict[HNL_mass][:-1]+bins_dict[HNL_mass][1:])/2\n",
    "\n",
    "    values_dict = {'ppfx_uncertainty': ppfx_unc_dict[HNL_mass], 'Genie_uncertainty':genie_unc_dict[HNL_mass],\n",
    "                   'Reinteraction_uncertainty':reint_unc_dict[HNL_mass],\n",
    "                   'ppfx_uncertainty_frac': ppfx_unc_frac_dict[HNL_mass], 'Genie_uncertainty_frac':genie_unc_frac_dict[HNL_mass],\n",
    "                   'Reinteraction_uncertainty_frac':reint_unc_frac_dict[HNL_mass]} \n",
    "    hist_samples = {}\n",
    "\n",
    "    #make array with all values 1, then weight by value\n",
    "    for name in values_dict:\n",
    "        test_hist = np.histogram(bins_cent, weights=values_dict[name], bins=bins_dict[HNL_mass], \n",
    "                                 range=[bins_dict[HNL_mass][0], bins_dict[HNL_mass][-1]])\n",
    "        hist_samples[name] = test_hist\n",
    "\n",
    "    stop_writing = False\n",
    "    dont_save = []\n",
    "    with uproot.open(loc_hists+Params[\"Run\"]+f'_{HNL_mass}_'+filename) as file:\n",
    "    # with uproot.open(loc_hists+f'{Run}_{HNL_mass}MeV_{filename}.root') as file: #Check what is already in the file (read-only)\n",
    "        for name in hist_samples:\n",
    "            if str(name)+\";1\" in file.keys():\n",
    "                # print(str(name) + f\" ALREADY EXISTS in {HNL_mass}MeV file, not re-saving\")\n",
    "                dont_save.append(name)\n",
    "\n",
    "    with uproot.update(loc_hists+f'{Run}_{HNL_mass}_{filename}') as file: #Add new hists into the file\n",
    "        # del file['ppfx_uncertainty']\n",
    "\n",
    "        for name in hist_samples:\n",
    "            # if stop_writing == True:\n",
    "            #     print(\"Not saving hist\")\n",
    "            #     break\n",
    "            if name in dont_save:\n",
    "                # print(f\"Not saving {name}\")\n",
    "                continue\n",
    "            else:\n",
    "                file[name] = hist_samples[name]\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98785248-cd13-45e8-b5f1-2e4aa371380b",
   "metadata": {},
   "source": [
    "## End of code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
