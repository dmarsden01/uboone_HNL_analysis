{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab81f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,string, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from platform import python_version\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import math\n",
    "from matplotlib.patches import Rectangle\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import Utilities.Plotter as PT\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Variables_list as Variables\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print ('Success')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0048d55",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This notebook is for training BDT models using xgboost. The BDT models are then saved and will be loaded in notebook 3.5. <br>\n",
    "The notebook also contains some optimization and performance plotting. With the standard hyper-parameters used for the analysis there is some over-training but when I initially performed the analysis I didn't think it was severe enough to be concerned with. Since the test and training samples are completely separated then any overtraining does not make the result incorrect, it just reduced the separation power of the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a3504-392d-4b23-a225-0db439fcb0f5",
   "metadata": {},
   "source": [
    "## Reading in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e41222-cb45-4107-a206-3b3653d23ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = {\"Run\":\"run3\", #The run number, so far either \"run1\" or \"run3\"\n",
    "          \"Load_single_file\":False, #This will override everything else, put the desired file in the \"single_file\" line\n",
    "          \"single_file\":10,\n",
    "          \"Load_standard\":True,\n",
    "          \"Load_DetVars\":False,\n",
    "          \"Only_keep_common_DetVar_evs\":True,\n",
    "          \"Load_data\":False,\n",
    "          \"FLATTEN\":True, #Have one row per reconstructed object in the analysis dataframe\n",
    "          \"only_presel\":False, #Create small files containing only variables necessary for pre-selection, for making pre-selection plots\n",
    "          \"EXT_in_training\":True,\n",
    "          \"dirt_in_training\":True,\n",
    "          \"Load_pi0_signal\":True} #Otherwise loads e+e- samples by default\n",
    "\n",
    "loc_pkls = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/my_vars/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d786fc-af51-48e5-88d4-d0ee5bcd2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_samples_dict = {}\n",
    "# end_string = \"_FINAL\"\n",
    "end_string = \"_full_Finished\" #Is this the final BDT we used?\n",
    "\n",
    "Presel_overlay = pd.read_pickle(loc_pkls+\"Preselected_overlay_\"+Params[\"Run\"]+f\"_flattened{end_string}.pkl\")\n",
    "\n",
    "#Always load in EXT, dirt for pre-selection, even though might not be used in the training\n",
    "Presel_EXT = pd.read_pickle(loc_pkls+\"Preselected_beamoff_\"+Params[\"Run\"]+f\"_flattened{end_string}.pkl\")\n",
    "Presel_dirt = pd.read_pickle(loc_pkls+\"Preselected_dirtoverlay_\"+Params[\"Run\"]+f\"_flattened{end_string}.pkl\")\n",
    "\n",
    "start_load_str = loc_pkls\n",
    "if Params[\"Load_pi0_signal\"] == False: HNL_samples = Constants.HNL_ee_samples_names\n",
    "if Params[\"Load_pi0_signal\"] == True: \n",
    "    start_load_str += f\"pi0_selection/\"\n",
    "    HNL_samples = Constants.HNL_mass_pi0_samples_names\n",
    "if Params[\"Load_single_file\"] == True: HNL_samples = [Params[\"single_file\"]]\n",
    "    \n",
    "for HNL_mass in HNL_samples:\n",
    "    Presel_signal = pd.read_pickle(start_load_str+f\"Preselected_{HNL_mass}_\"+Params[\"Run\"]+f\"_flattened{end_string}.pkl\")\n",
    "    signal_samples_dict[HNL_mass] = Presel_signal\n",
    "\n",
    "print(\"overlay events: \"+str(len(Presel_overlay)))\n",
    "print(\"beamoff events: \"+str(len(Presel_EXT)))\n",
    "print(\"dirt events: \"+str(len(Presel_dirt)))\n",
    "print(Presel_overlay.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f4b792-4c16-486e-87bf-7c169e548c2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Calculating fractions of tracks and showers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4cb148-a31d-450b-a789-6a0ecf0006b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for HNL_mass in HNL_samples:\n",
    "    print(type(signal_samples_dict[HNL_mass]['shr_px_v'])) \n",
    "    \n",
    "#dataframe[dataframe['Percentage'] > 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a1164-da2a-4ce1-a45f-188ba77671a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mass = \"10_ee\"\n",
    "\n",
    "print(\"Total events: \" + str(len(signal_samples_dict[HNL_mass]['n_pfps'])))\n",
    "\n",
    "zero_showers = len(signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass]['n_showers']==0])\n",
    "zero_tracks = len(signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass]['n_tracks']==0])\n",
    "\n",
    "print(f\"Zero showers in: {zero_showers}\")\n",
    "print(f\"Zero tracks in: {zero_tracks}\")\n",
    "\n",
    "var_list = ['shr_theta_v', 'shr_phi_v', 'shr_pz_v', 'trk_theta_v', 'trk_phi_v', 'trk_dir_z_v']\n",
    "\n",
    "for var in var_list:\n",
    "    non_reco = signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass][var]==-9999.0]\n",
    "    print(var+\": \"+str(len(non_reco)))\n",
    "\n",
    "\n",
    "print()\n",
    "\n",
    "var_list_2 = ['shrclusdir2', 'shr_energy_tot','trk_energy', 'trk_energy_tot','shr_tkfit_dedx_max']\n",
    "\n",
    "for var in var_list_2:\n",
    "    non_reco = signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass][var]==-9999.0]\n",
    "    print(var+\": \"+str(len(non_reco)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203fef8c-1097-4920-9ad3-908ed1ce1a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(signal_samples_dict[HNL_mass]['rse_id'])\n",
    "zero_showers_df = signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass]['n_showers']==0]\n",
    "one_shower_df = signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass]['n_showers']==1]\n",
    "no_shr_energy_tot_df = signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass]['shr_energy_tot']==-9999]\n",
    "no_shr_tkfit_df = signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass]['shr_tkfit_dedx_max']==-9999]\n",
    "\n",
    "filtered_shrs = zero_showers_df.loc[(zero_showers_df['rse_id'].isin(no_shr_energy_tot_df['rse_id']))]\n",
    "filtered_shrs_inv = no_shr_energy_tot_df.loc[-(no_shr_energy_tot_df['rse_id'].isin(zero_showers_df['rse_id']))]\n",
    "\n",
    "filtered_shr_tkfit = zero_showers_df.loc[(zero_showers_df['rse_id'].isin(no_shr_tkfit_df['rse_id']))]\n",
    "\n",
    "print(len(filtered_shrs))\n",
    "print(len(filtered_shrs_inv))\n",
    "print(len(filtered_shr_tkfit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de593f-14e3-444a-8d63-c130c65014c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_shrs_inv[filtered_shrs_inv[\"n_showers\"]==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f878486-bb57-4bca-b962-2966c5bc5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_shrs_inv[filtered_shrs_inv[\"trk_score_v\"]>0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6494f59d-4225-4d64-aefb-9692bea2e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_reco_shr = signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass]['shr_phi_v']==-9999.0] # 'shr_pz_v'\n",
    "non_reco_trk = signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass]['trk_phi_v']==-9999.0]\n",
    "non_reco_energy = signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass]['NeutrinoEnergy2']==-9999.0]\n",
    "non_reco_shrclusdir = signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass]['shrclusdir2']==-9999.0] \n",
    "non_reco_trkcalo = signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass]['trk_calo_energy_u_v']==-9999.0] \n",
    "non_reco_trk_E_tot = signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass]['trk_energy_tot']==-9999.0] \n",
    "\n",
    "print(len(signal_samples_dict[HNL_mass]))\n",
    "print(len(non_reco_shr))\n",
    "print(len(non_reco_trk))\n",
    "print(len(non_reco_energy))\n",
    "print(len(non_reco_shrclusdir))\n",
    "print(len(non_reco_trkcalo))\n",
    "print(len(non_reco_trk_E_tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc7cfef-4df4-412e-b88c-49dc0cbf2bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass]['n_tracks']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94e54ed-cea1-4579-a01a-6cff613f35de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(signal_samples_dict[HNL_mass][signal_samples_dict[HNL_mass]['n_showers']==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb76888-7925-407b-9a82-b6a997a12e5e",
   "metadata": {},
   "source": [
    "## Splitting into test and training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9873ba6f-b41a-4eb2-9343-c0dfd7032455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#overlay_train_frac, signal_train_frac, EXT_train_frac\n",
    "overlay_train_frac = 0.5 \n",
    "signal_train_frac = 0.7\n",
    "EXT_train_frac = 0.5\n",
    "dirt_train_frac = 0.5\n",
    "\n",
    "split_dict = Functions.Split_samples(Presel_overlay, signal_samples_dict, Presel_EXT, Presel_dirt, \n",
    "                                     overlay_train_frac, signal_train_frac, EXT_train_frac, dirt_train_frac)\n",
    "\n",
    "overlay_test, overlay_train = split_dict[\"overlay_test\"], split_dict[\"overlay_train\"]\n",
    "signal_test_dict, signal_train_dict = split_dict[\"signal_test_dict\"], split_dict[\"signal_train_dict\"]\n",
    "EXT_test, EXT_train = split_dict[\"EXT_test\"], split_dict[\"EXT_train\"]\n",
    "dirt_test, dirt_train = split_dict[\"dirt_test\"], split_dict[\"dirt_train\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef77c7-0f1d-4d58-a0b8-7d6fb95fdf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ultimate_feature_list = ['n_pfps', 'n_tracks', 'shr_theta_v', 'shr_phi_v', 'shr_pz_v', 'shrclusdir2', 'shr_energy_tot',\n",
    "                         'trk_theta_v', 'trk_phi_v','trk_dir_z_v', 'trk_energy', 'trk_energy_tot', 'trk_score_v',\n",
    "                         'pfnplanehits_U', 'pfnplanehits_V', 'pfnplanehits_Y', 'NeutrinoEnergy2']#, 'nu_flashmatch_score']\n",
    "\n",
    "ultimate_feature_list += ['shr_tkfit_dedx_max', 'topological_score']\n",
    "if Params[\"Run\"]==\"run3\":\n",
    "    ultimate_feature_list += ['nu_flashmatch_score']\n",
    "print(f\"Number of features is {len(ultimate_feature_list)}\")\n",
    "\n",
    "bdt_vars = ultimate_feature_list #This is using just the most important variables list\n",
    "\n",
    "if Params[\"Load_pi0_signal\"] == False: BDT_name = f\"{end_string}\"\n",
    "if Params[\"Load_pi0_signal\"] == True: BDT_name = f\"{end_string}\"\n",
    "if Params[\"EXT_in_training\"] == True: BDT_name = f\"{end_string}\"\n",
    "\n",
    "BDT_name = \"_full_Finished_10\"\n",
    "# BDT_name = \"_full_Finished_8\" #For pi0 importances plot\n",
    "\n",
    "var_list = bdt_vars\n",
    "train_frac_dict = {\"overlay\":overlay_train_frac, \"signal\":signal_train_frac, \"beamoff\":EXT_train_frac, \"dirtoverlay\":dirt_train_frac}\n",
    "\n",
    "start_loc = f\"bdts/\"\n",
    "if Params[\"Load_pi0_signal\"] == True: start_loc += \"pi0_selection/\"\n",
    "\n",
    "save_vars_fracs = input(\"Do you want to save the variables and training fractions? y/n \")\n",
    "\n",
    "if save_vars_fracs == \"y\":\n",
    "    print(f\"Saving variables used as {BDT_name}_\"+Params[\"Run\"]+\".pkl\")\n",
    "    with open(start_loc+f\"input_vars/{BDT_name}_\"+Params[\"Run\"], \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(var_list, fp)\n",
    "\n",
    "    print(f\"Saving training fractions used as {BDT_name}_\"+Params[\"Run\"]+\".pkl\")\n",
    "    with open(start_loc+f\"Training_fractions/{BDT_name}_\"+Params[\"Run\"], \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(train_frac_dict, fp)\n",
    "\n",
    "signal_samples_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c2490-bb46-49dc-a1be-a3f0853f7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkls = input(\"Do you want to save the test sample pkls? y/n \")\n",
    "\n",
    "if save_pkls == \"y\":\n",
    "    Functions.Save_test_pkls(Params, loc_pkls, \"_full_Finished_10\", overlay_test, signal_test_dict, EXT_test, dirt_test)\n",
    "else: print(\"Not saved .pkls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f3f899-8eef-408e-b520-a2483ec89a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Functions)\n",
    "combined_dict, labels_dict, bkg_train = Functions.Make_train_labels_and_dicts(Params, bdt_vars, overlay_train, EXT_train, dirt_train, signal_train_dict)\n",
    "\n",
    "if Params[\"EXT_in_training\"] == True:\n",
    "    print(\"Adding EXT events into test set\")\n",
    "    bkg_test = pd.concat([overlay_test,EXT_test])\n",
    "else: bkg_test = overlay_test.copy()\n",
    "\n",
    "if Params[\"dirt_in_training\"] == True:\n",
    "    print(\"Adding dirt events into test set\")\n",
    "    bkg_test = pd.concat([bkg_test,EXT_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc7ce8-8cb0-469d-ab0e-815c25a9aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_vs_train_plots(test_sig, test_bkg, train_sig, train_bkg, bdt_dict, bins_dict, ymax=[], xlims=[], legsize=16, sig_region=3):\n",
    "    \"\"\"\n",
    "    Plotting the BDT score results for test and training sets to check for overtraining.\n",
    "    \"\"\"\n",
    "    test_results_sig_dict, test_results_bkg_dict = {}, {}\n",
    "    train_results_sig_dict, train_results_bkg_dict = {}, {}\n",
    "    \n",
    "    frac_test_signal, frac_train_signal = {}, {}\n",
    "    frac_test_bkg, frac_train_bkg = {}, {}\n",
    "    \n",
    "    for HNL_mass in test_sig:\n",
    "        results_sig = Functions.logit(bdt_dict[HNL_mass].predict(test_sig[HNL_mass]))\n",
    "        results_bkg = Functions.logit(bdt_dict[HNL_mass].predict(test_bkg))\n",
    "\n",
    "        train_results_sig = Functions.logit(bdt_dict[HNL_mass].predict(train_sig[HNL_mass]))\n",
    "        train_results_bkg = Functions.logit(bdt_dict[HNL_mass].predict(train_bkg))\n",
    "\n",
    "        test_results_sig_dict.update({HNL_mass:results_sig})\n",
    "        test_results_bkg_dict.update({HNL_mass:results_bkg})\n",
    "\n",
    "        train_results_sig_dict.update({HNL_mass:train_results_sig})\n",
    "        train_results_bkg_dict.update({HNL_mass:train_results_bkg})\n",
    "\n",
    "    save = input(f\"Do you want to save these plots? y/n\")\n",
    "    for HNL_mass in test_sig:\n",
    "        plt.figure(figsize=(10,7))\n",
    "        mass_label=HNL_mass.split(\"_\")[0]\n",
    "        sample_type = HNL_mass.split(\"_\")[1]\n",
    "        if sample_type == \"ee\": full_label = f\"{mass_label} MeV \" + r\"$e^{+}e^{-}$\"\n",
    "        if sample_type == \"pi0\": full_label = f\"{mass_label} MeV \" + r\"$\\nu \\pi^{0}$\"\n",
    "        weights_train = np.ones(len(train_results_sig_dict[HNL_mass]))/len(train_results_sig_dict[HNL_mass])\n",
    "        weights_test = np.ones(len(test_results_sig_dict[HNL_mass]))/len(test_results_sig_dict[HNL_mass])\n",
    "        # plt.hist(train_results_sig_dict[HNL_mass],bins=bins_dict[HNL_mass], density=True,alpha=0.4,color='red',label=f'Train '+full_label)\n",
    "        plt.hist(train_results_sig_dict[HNL_mass],bins=bins_dict[HNL_mass], density=False, weights=weights_train,\n",
    "                 alpha=0.35,color='red',label=f'Train '+full_label)\n",
    "        # counts,bin_edges = np.histogram(test_results_sig_dict[HNL_mass],bins=bins_dict[HNL_mass],density=True)\n",
    "        counts,bin_edges = np.histogram(test_results_sig_dict[HNL_mass],bins=bins_dict[HNL_mass],density=False, weights=weights_test)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:])/2.\n",
    "        \n",
    "        #For calculating the fraction in the signal region\n",
    "        x_train, y_train = np.histogram(train_results_sig_dict[HNL_mass],bins=bins_dict[HNL_mass],range=xlims,density=True)\n",
    "        \n",
    "        # Taken from plotting script\n",
    "        x,y=np.histogram(test_results_sig_dict[HNL_mass],bins=bins_dict[HNL_mass],range=xlims,density=True)\n",
    "        x1,y=np.histogram(test_results_sig_dict[HNL_mass],bins=bins_dict[HNL_mass],range=xlims)\n",
    "        bin_center = [(y[i] + y[i+1])/2. for i in range(len(y)-1)]\n",
    "        dat_val=x\n",
    "        # dat_err=np.sqrt(x1)*Functions.safe_div(x,x1) #need to write one for arrays instead of single values.\n",
    "        dat_err=np.sqrt(x1)*np.nan_to_num(x/x1)\n",
    "        \n",
    "        # plt.plot(bin_centers,counts,marker='o',linestyle=\"None\",color='red',label=f'Test '+full_label)\n",
    "        plt.errorbar(bin_center,dat_val,yerr=dat_err,fmt='.',color='red',\n",
    "                     markersize=9,lw=5,capsize=6,elinewidth=3,label=f'Test '+full_label)\n",
    "\n",
    "        plt.hist(train_results_bkg_dict[HNL_mass], bins=bins_dict[HNL_mass], density = True, alpha = 0.35, color = 'orange', label = r'Train background')\n",
    "        counts,bin_edges = np.histogram(test_results_bkg_dict[HNL_mass],bins=bins_dict[HNL_mass],density = True)\n",
    "        bin_centers = (bin_edges[:-1] +  bin_edges[1:])/2.\n",
    "        \n",
    "        x_bkg,y_bkg=np.histogram(test_results_bkg_dict[HNL_mass],bins=bins_dict[HNL_mass],range=xlims,density=True)\n",
    "        x1_bkg,y_bkg=np.histogram(test_results_bkg_dict[HNL_mass],bins=bins_dict[HNL_mass],range=xlims)\n",
    "        bin_center_bkg = [(y[i] + y[i+1])/2. for i in range(len(y)-1)]\n",
    "        dat_val_bkg=x_bkg\n",
    "        dat_err_bkg=np.sqrt(x1_bkg)*np.nan_to_num(x_bkg/x1_bkg)\n",
    "        \n",
    "        x_train_bkg, y_train_bkg = np.histogram(train_results_bkg_dict[HNL_mass],bins=bins_dict[HNL_mass],range=xlims,density=True)\n",
    "        \n",
    "        # plt.plot(bin_centers,counts,marker='o',linestyle =\"None\",color='orange',label = r'Test background')\n",
    "        plt.errorbar(bin_center_bkg,dat_val_bkg,yerr=dat_err_bkg,fmt='.',color='orange',\n",
    "                     markersize=9,lw=5,capsize=6,elinewidth=3,label=r'Test background')\n",
    "        \n",
    "        #calculating test and train fracs in signal region\n",
    "        frac_test_signal[HNL_mass] = sum(x[-1*sig_region:])\n",
    "        frac_train_signal[HNL_mass] = sum(x_train[-1*sig_region:])\n",
    "        frac_test_bkg[HNL_mass] = sum(x_bkg[-1*sig_region:])\n",
    "        frac_train_bkg[HNL_mass] = sum(x_train_bkg[-1*sig_region:])\n",
    "        \n",
    "        if ymax != []:\n",
    "            plt.ylim(0, ymax)\n",
    "        if xlims != []:\n",
    "            plt.xlim(xlims)\n",
    "        \n",
    "        plt.xlabel(f'BDT Score '+r'($m_{\\mathrm{HNL}}=$'+f'{mass_label} MeV)', fontsize=30)\n",
    "        plt.ylabel(\"Fraction of events\")\n",
    "        plt.legend(fontsize=legsize, ncol=2)\n",
    "        \n",
    "        if(save==\"y\"): \n",
    "            plt.savefig(f\"plots/BDT_output/test_vs_train/Test_vs_train_\" + Params[\"Run\"] + f\"_{HNL_mass}{BDT_name}.png\")\n",
    "            plt.savefig(f\"plots/BDT_output/test_vs_train/Test_vs_train_\" + Params[\"Run\"] + f\"_{HNL_mass}{BDT_name}.pdf\")\n",
    "            \n",
    "    return frac_test_signal, frac_train_signal, frac_test_bkg, frac_train_bkg\n",
    "    \n",
    "        \n",
    "def bkg_test_dists(overlay, beamoff, dirt, bdt_dict, bins_dict, sample_norms):\n",
    "    \"\"\"\n",
    "    Just plotting histograms of the output scores of test sets.\n",
    "    Correctly normalised by sample_norms.\n",
    "    \"\"\"\n",
    "    for HNL_mass in bdt_dict:\n",
    "        results_overlay = Functions.logit(bdt_dict[HNL_mass].predict(overlay))\n",
    "        results_beamoff = Functions.logit(bdt_dict[HNL_mass].predict(beamoff))\n",
    "        results_dirt = Functions.logit(bdt_dict[HNL_mass].predict(dirt))\n",
    "        \n",
    "        weights_overlay = np.ones(len(results_overlay))*sample_norms[\"overlay\"]\n",
    "        weights_beamoff = np.ones(len(results_beamoff))*sample_norms[\"beamoff\"]\n",
    "        weights_dirt = np.ones(len(results_dirt))*sample_norms[\"dirtoverlay\"]\n",
    "        \n",
    "        plt.figure(figsize=(10,7))\n",
    "        \n",
    "        plt.hist(results_overlay,bins=bins_dict[HNL_mass], weights=weights_overlay, density=False,alpha=0.4,label=f'Overlay')\n",
    "        plt.hist(results_beamoff,bins=bins_dict[HNL_mass], weights=weights_beamoff,density=False,alpha=0.4,label=f'Beamoff')\n",
    "        plt.hist(results_dirt,bins=bins_dict[HNL_mass], weights=weights_dirt,density=False,alpha=0.4,label=f'dirt')\n",
    "        \n",
    "        plt.legend()\n",
    "    print(\"overlay sum weights: \"+str(sum(weights_overlay)))\n",
    "    print(\"beamoff sum weights: \"+str(sum(weights_beamoff)))\n",
    "    print(\"dirt sum weights: \"+str(sum(weights_dirt)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159feff4-fafc-4770-b656-26e86f0c7d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_dict, xgb_sig_train_dict, xgb_sig_test_dict, xgb_train_bkg, xgb_test_bkg = Functions.Prepare_for_xgb(Params, bdt_vars, combined_dict, \n",
    "                                                                                                     signal_train_dict, bkg_train,  signal_test_dict, \n",
    "                                                                                                     bkg_test, labels_dict, missing=-9999.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7e9ed-286e-4580-959a-4460ee9dab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test_dict, labels_test_dict={}, {}\n",
    "for HNL_mass in xgb_sig_train_dict:\n",
    "    combined_test_dict[HNL_mass] = pd.concat([signal_test_dict[HNL_mass][bdt_vars], bkg_test[bdt_vars]])\n",
    "    labels_test_dict[HNL_mass] = [1]*len(signal_test_dict[HNL_mass]) + [0]*len(bkg_test)\n",
    "\n",
    "xgb_combined_test_dict = {}\n",
    "for HNL_mass in xgb_sig_train_dict:\n",
    "    xgb_combined_test_dict[HNL_mass] = xgboost.DMatrix(combined_test_dict[HNL_mass][bdt_vars], label=labels_test_dict[HNL_mass], \n",
    "                                                   missing=-9999.0, feature_names=bdt_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689a4ae-9dcf-4d9c-9963-0501e234ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param = {'booster': 'dart',\n",
    "             'max_depth':6,\n",
    "             'eta':0.3,\n",
    "             'objective':'binary:logistic',\n",
    "             'eval_metric':'auc', #area under ROC curve\n",
    "             # 'eval_metric':'logloss',\n",
    "             # 'eval_metric':'mae',\n",
    "             'tree_method':'hist',\n",
    "             'rate_drop': 0.1,\n",
    "             'skip_drop': 0.5,\n",
    "             # 'subsample':0.5,\n",
    "             'scale_pos_weight': float(len(bkg_train))/float(len(signal_train_dict[HNL_mass]))\n",
    "            }\n",
    "progress = dict()\n",
    "\n",
    "print(f\"BDT name is {BDT_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f578c614-7042-4a20-af02-511d25607f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_BDTs = input(f\"Do you want to train new BDT models with names {BDT_name}? y/n \")\n",
    "\n",
    "if train_BDTs == \"y\":\n",
    "    Functions.Train_BDTs(Params, bdt_vars, BDT_name, xgb_train_dict, xgb_sig_test_dict, xgb_test_bkg, xgb_param, xgb_combined_test_dict, \n",
    "               progress, num_round = 150, early_stop=10)\n",
    "    print(\"Finished all!\")\n",
    "else: print(\"Not retraining BDTs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75338d5-1ab1-49c6-8e22-62a6ed99056b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Trying to save and load params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04e54d-5f29-492f-a9dc-757d6dfb22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param = {'booster': 'dart',\n",
    "             'max_depth':7,\n",
    "             'eta':0.3,\n",
    "             'objective':'binary:logistic',\n",
    "             'eval_metric':'auc', #area under ROC curve\n",
    "             # 'eval_metric':'logloss',\n",
    "             # 'eval_metric':'mae',\n",
    "             'tree_method':'hist',\n",
    "             'rate_drop': 0.1,\n",
    "             'skip_drop': 0.8,\n",
    "#        'subsample':0.5,\n",
    "             'scale_pos_weight': float(len(bkg_train))/float(len(signal_train_dict[HNL_mass]))\n",
    "            }\n",
    "\n",
    "BDT_name = \"Testing_saving_2\"\n",
    "\n",
    "watchlist = [(xgb_train_dict[HNL_mass], 'train'), (xgb_combined_test_dict[HNL_mass], 'test_combined')]\n",
    "\n",
    "print(f\"Training {HNL_mass} BDT\" + \"\\n\")\n",
    "# bdt = xgboost.train(xgb_param, xgb_train_dict[HNL_mass], num_round, watchlist, evals_result=progress, verbose_eval=False)\n",
    "individual_progress=dict()\n",
    "bdt_test_NEW = xgboost.train(params=xgb_param, dtrain=xgb_train_dict[HNL_mass], num_boost_round=150, evals=watchlist, \n",
    "                         early_stopping_rounds=20, evals_result=individual_progress, verbose_eval=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e189284-f21c-47dc-9925-7d67e1a718cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using .json\n",
    "bdt_test_NEW.save_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}{BDT_name}.json\")\n",
    "# config = bdt_test_NEW.save_config()\n",
    "\n",
    "bdt_json_NEW = xgboost.Booster()\n",
    "bdt_json_NEW.load_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}{BDT_name}.json\")\n",
    "\n",
    "json_booster_saved = json.loads(bdt_json_NEW.save_config())\n",
    "print(json_booster_saved['learner']['gradient_booster']['dart_train_param'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c7c0ce-03c2-4e1c-9c2a-9fb51c152dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using .pkl\n",
    "file_name = f\"bdts/xgb_reg_{BDT_name}.pkl\"\n",
    "\n",
    "# save\n",
    "pickle.dump(bdt_test_NEW, open(file_name, \"wb\"))\n",
    "\n",
    "# load\n",
    "xgb_model_loaded = pickle.load(open(file_name, \"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "pkl_loaded_booster = json.loads(xgb_model_loaded.save_config())\n",
    "\n",
    "print(pkl_loaded_booster['learner']['gradient_booster']['dart_train_param'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61425b5-bb71-4386-8c0f-1f36c8f49b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(xgb_model_loaded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deefb3e-b46c-4c29-bc2a-674116e53e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test=xgb_model_loaded.predict(xgb_sig_test_dict[HNL_mass])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab34fd-9db4-4568-9f53-f5fea90b0404",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(progress[\"10_ee\"]['test_combined'][\"auc\"]))\n",
    "\n",
    "# boost_rounds = progress[\"test-auc-mean\"].argmax()\n",
    "# boost_rounds = progress[\"test-logloss-mean\"].argmin()\n",
    "bdt = xgboost.Booster()\n",
    "bdt.load_model(loc+Params[\"Run\"]+f\"_{HNL_mass}{BDT_name}.json\")\n",
    "\n",
    "test_json = json.loads(bdt.save_config())\n",
    "print(test_json['learner']['gradient_booster']['dart_train_param'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f85ce2-bb05-4bd8-a69b-4760a1044102",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa12cc4-ba42-4182-a457-8fe53a924161",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized params = {'booster': 'dart', 'max_depth': 7, 'eta': 0.3, 'objective': 'binary:logistic', 'eval_metric': 'logloss',\n",
    "                    'tree_method': 'hist', 'scale_pos_weight': 3.923306148055207,\n",
    "                    'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 1.0, 'rate_drop': 0.1, 'skip_drop': 0.3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15850a-deca-4470-9541-82272e6cc275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZING\n",
    "\n",
    "HNL_mass = \"100_ee\"\n",
    "\n",
    "old_params = {'booster': 'dart',\n",
    "             'max_depth':6,\n",
    "             'eta':0.3,\n",
    "             'objective':'binary:logistic',\n",
    "             'tree_method':'hist',\n",
    "             'rate_drop': 0.1,\n",
    "             'skip_drop': 0.5#,\n",
    "#        'subsample':0.5,\n",
    "#        'scale_pos_weight': float(len(data_bkg_train))/float(len(data_sig_train)),\n",
    "            }\n",
    "\n",
    "old_watchlist = [(xgb_train_dict[HNL_mass], 'train'), (xgb_sig_test_dict[HNL_mass], 'test_sig'), (xgb_test_bkg,'test_bkg')]\n",
    "\n",
    "cv_results = xgboost.cv(\n",
    "    xgb_param,\n",
    "    xgb_train_dict[HNL_mass],\n",
    "    num_boost_round=100,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    metrics={'auc'},\n",
    "    early_stopping_rounds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e5374-46c9-461a-a747-9f76bd3a57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results\n",
    "\n",
    "print(cv_results[\"test-auc-mean\"].max())\n",
    "print(cv_results[\"test-auc-mean\"].argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200c716-27ee-4814-ba18-63e0dd80f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(4,10)\n",
    "    for min_child_weight in range(1,6)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83afd19-5f88-4011-b44a-76286d62fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial best params and MAE\n",
    "max_auc = 0.0\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))    # Update our parameters\n",
    "    xgb_param['max_depth'] = max_depth\n",
    "    xgb_param['min_child_weight'] = min_child_weight    # Run CV\n",
    "    cv_results = xgboost.cv(\n",
    "        xgb_param,\n",
    "        xgb_train_dict[HNL_mass],\n",
    "        num_boost_round=100,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'auc'},\n",
    "        early_stopping_rounds=10\n",
    "    )    # Update best eval metric\n",
    "    mean_auc = cv_results[\"test-auc-mean\"].max()\n",
    "    boost_rounds = cv_results[\"test-auc-mean\"].argmax()\n",
    "    print(\"\\tAUC {} for {} rounds\".format(mean_auc, boost_rounds))\n",
    "    if mean_auc > max_auc:\n",
    "        max_auc = mean_auc\n",
    "        best_params = (max_depth, min_child_weight)\n",
    "    \n",
    "print(\"Best params: {}, {}, AUC: {}\".format(best_params[0], best_params[1], max_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58435900-1996-4ff5-842d-78ca15c84dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = xgboost.cv(\n",
    "    xgb_param,\n",
    "    xgb_train_dict[HNL_mass],\n",
    "    num_boost_round=100,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    metrics={'logloss'},\n",
    "    early_stopping_rounds=5\n",
    ")\n",
    "\n",
    "cv_results\n",
    "\n",
    "print(cv_results[\"test-logloss-mean\"].min())\n",
    "print(cv_results[\"test-logloss-mean\"].argmin())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054b914-0224-42b9-b8e9-160b30c8c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial best params and MAE\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))    # Update our parameters\n",
    "    xgb_param['max_depth'] = max_depth\n",
    "    xgb_param['min_child_weight'] = min_child_weight    # Run CV\n",
    "    cv_results = xgboost.cv(\n",
    "        xgb_param,\n",
    "        xgb_train_dict[HNL_mass],\n",
    "        num_boost_round=100,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'logloss'},\n",
    "        early_stopping_rounds=10\n",
    "    )    # Update best eval metric\n",
    "    mean_logloss = cv_results[\"test-logloss-mean\"].min()\n",
    "    boost_rounds = cv_results[\"test-logloss-mean\"].argmin()\n",
    "    print(\"\\tlogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (max_depth, min_child_weight)\n",
    "    \n",
    "print(\"Best params: {}, {}, logloss: {}\".format(best_params[0], best_params[1], min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93dc32-2183-4ddd-aa70-799ad63f131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param['max_depth'] = 7\n",
    "xgb_param['min_child_weight'] = 1\n",
    "xgb_param['eval_metric'] = \"logloss\"\n",
    "\n",
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(7,11)]\n",
    "    for colsample in [i/10. for i in range(7,11)]\n",
    "]\n",
    "\n",
    "print(gridsearch_params)\n",
    "print(xgb_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54614689-69f0-44da-93b4-5870203bdb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "for subsample, colsample in gridsearch_params:\n",
    "    print(\"CV with subsample={}, colsample={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))    # Update our parameters\n",
    "    xgb_param['subsample'] = subsample\n",
    "    xgb_param['colsample_bytree'] = colsample    # Run CV\n",
    "    cv_results = xgboost.cv(\n",
    "        xgb_param,\n",
    "        xgb_train_dict[HNL_mass],\n",
    "        num_boost_round=100,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'logloss'},\n",
    "        early_stopping_rounds=10\n",
    "    )    # Update best eval metric\n",
    "    mean_logloss = cv_results[\"test-logloss-mean\"].min()\n",
    "    boost_rounds = cv_results[\"test-logloss-mean\"].argmin()\n",
    "    print(\"\\tlogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (subsample, rate_drop)\n",
    "    \n",
    "print(\"Best params: {}, {}, logloss: {}\".format(best_params[0], best_params[1], min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11066beb-282f-4994-aa82-0ec60f5bbc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param['subsample'] = 1.0\n",
    "xgb_param['colsample_bytree'] = 1.0\n",
    "\n",
    "print(xgb_param)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b2fed-2bd1-4e99-b4ab-857e83e3a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.20378282252634553 for 0.0 rate drop and skip drop\n",
    "\n",
    "gridsearch_params = [\n",
    "    (rate_drop, skip_drop)\n",
    "    for rate_drop in [i/10. for i in range(1,8)]\n",
    "    for skip_drop in [i/10. for i in range(1,11)]\n",
    "]\n",
    "\n",
    "print(gridsearch_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64754ed-d361-43d1-8e53-6c8425fab605",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "for rate_drop, skip_drop in gridsearch_params:\n",
    "    print(\"CV with rate_drop={}, skip_drop={}\".format(\n",
    "                             rate_drop,\n",
    "                             skip_drop))    # Update our parameters\n",
    "    xgb_param['rate_drop'] = rate_drop\n",
    "    xgb_param['skip_drop'] = skip_drop    # Run CV\n",
    "    cv_results = xgboost.cv(\n",
    "        xgb_param,\n",
    "        xgb_train_dict[HNL_mass],\n",
    "        num_boost_round=100,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'logloss'},\n",
    "        early_stopping_rounds=10\n",
    "    )    # Update best eval metric\n",
    "    mean_logloss = cv_results[\"test-logloss-mean\"].min()\n",
    "    boost_rounds = cv_results[\"test-logloss-mean\"].argmin()\n",
    "    print(\"\\tlogloss {} for {} rounds\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = (rate_drop, skip_drop)\n",
    "    \n",
    "print(\"Best params: {}, {}, logloss: {}\".format(best_params[0], best_params[1], min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928cefb6-81bf-4b0b-9439-1170d48a4707",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param['rate_drop'] = 0.1\n",
    "xgb_param['skip_drop'] = 0.3\n",
    "\n",
    "print(xgb_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc5b52-ac6d-47dc-afda-e84de3cc101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_logloss = float(\"Inf\")\n",
    "best_params = None\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))    # We update our parameters\n",
    "    xgb_param['eta'] = eta    # Run and time CV\n",
    "    cv_results = xgboost.cv(\n",
    "            xgb_param,\n",
    "            xgb_train_dict[HNL_mass],\n",
    "            num_boost_round=300,\n",
    "            seed=42,\n",
    "            nfold=5,\n",
    "            metrics=['logloss'],\n",
    "            early_stopping_rounds=10\n",
    "            )    # Update best score\n",
    "    mean_logloss = cv_results[\"test-logloss-mean\"].min()\n",
    "    boost_rounds = cv_results[\"test-logloss-mean\"].argmin()\n",
    "    print(\"\\tlogloss {} for {} rounds\\n\".format(mean_logloss, boost_rounds))\n",
    "    if mean_logloss < min_logloss:\n",
    "        min_logloss = mean_logloss\n",
    "        best_params = eta\n",
    "        \n",
    "print(\"Best params: {}, logloss: {}\".format(best_params, min_logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41018d83-9a4f-4f4d-bd30-a1bea2b4bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param['eta'] = 0.3\n",
    "\n",
    "print(xgb_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d8146e-c3fc-4e76-a608-273bcc7cd5f4",
   "metadata": {},
   "source": [
    "## Test vs. train results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9637a803-c855-4248-b9da-1fb61c9e784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading BDT models named {BDT_name}\")\n",
    "\n",
    "sample_names = xgb_train_dict.keys()\n",
    "bdt_model_dict = {}\n",
    "sig_results_test_dict, bkg_results_test_dict = {}, {}\n",
    "sig_results_train_dict, bkg_results_train_dict = {}, {}\n",
    "\n",
    "max_sig_results, max_bkg_results = {}, {}\n",
    "min_sig_results, min_bkg_results = {}, {}\n",
    "\n",
    "bins_dict={}\n",
    "\n",
    "if Params[\"Load_pi0_signal\"]==False: sample_type = \"ee\"\n",
    "if Params[\"Load_pi0_signal\"]==True: sample_type = \"pi0\"\n",
    "\n",
    "for HNL_mass in sample_names:\n",
    "    # bdt = xgboost.Booster()\n",
    "    if HNL_mass.split(\"_\")[1] == \"pi0\": loc = \"bdts/pi0_selection/\"\n",
    "    if HNL_mass.split(\"_\")[1] == \"ee\": loc = \"bdts/\"\n",
    "    # bdt.load_model(loc+Params[\"Run\"]+f\"_{HNL_mass}{BDT_name}.json\")\n",
    "    file_name = loc+Params[\"Run\"]+f\"_{HNL_mass}{BDT_name}.pkl\"\n",
    "    bdt = pickle.load(open(file_name, \"rb\"))\n",
    "    \n",
    "    bdt_model_dict[HNL_mass] = bdt\n",
    "    \n",
    "    sig_results_test_dict[HNL_mass] = Functions.logit(bdt.predict(xgb_sig_test_dict[HNL_mass]))\n",
    "    bkg_results_test_dict[HNL_mass] = Functions.logit(bdt.predict(xgb_test_bkg))\n",
    "    \n",
    "    sig_results_train_dict[HNL_mass] = Functions.logit(bdt.predict(xgb_sig_train_dict[HNL_mass]))\n",
    "    bkg_results_train_dict[HNL_mass] = Functions.logit(bdt.predict(xgb_train_bkg))\n",
    "    \n",
    "    max_sig_results[HNL_mass] = max([max(sig_results_test_dict[HNL_mass]), max(sig_results_train_dict[HNL_mass])])\n",
    "    max_bkg_results[HNL_mass] = max([max(bkg_results_test_dict[HNL_mass]), max(bkg_results_train_dict[HNL_mass])])\n",
    "    \n",
    "    min_sig_results[HNL_mass] = min([min(sig_results_test_dict[HNL_mass]), min(sig_results_train_dict[HNL_mass])])\n",
    "    min_bkg_results[HNL_mass] = min([min(bkg_results_test_dict[HNL_mass]), min(bkg_results_train_dict[HNL_mass])])\n",
    "    \n",
    "    low_edge_sig, low_edge_bkg = np.floor(min_sig_results[HNL_mass]), np.floor(min_bkg_results[HNL_mass])\n",
    "    high_edge_sig, high_edge_bkg = np.ceil(max_sig_results[HNL_mass]), np.ceil(max_bkg_results[HNL_mass])\n",
    "\n",
    "    bins_dict[HNL_mass] = np.arange(low_edge_bkg, high_edge_sig)\n",
    "    \n",
    "\n",
    "with open(loc+f\"Training_fractions/{BDT_name}_\"+Params[\"Run\"], \"rb\") as fp:   # Unpickling\n",
    "    saved_train_fracs = pickle.load(fp)\n",
    "    \n",
    "print(saved_train_fracs)\n",
    "    \n",
    "param_info = json.loads(bdt.save_config()) #Assuming all BDTs trained with same params (should have been)\n",
    "print(\"Params used were: \")\n",
    "print(param_info['learner']['gradient_booster']['dart_train_param'])\n",
    "print(param_info['learner']['metrics'])\n",
    "print(param_info['learner']['objective'])\n",
    "print(bdt_model_dict.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce69e5-3d7b-4967-912c-3315cc6161aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Functions)\n",
    "sample_norms = Functions.Get_sample_norms(Params, sample_names, saved_train_fracs[\"signal\"], saved_train_fracs[\"overlay\"], \n",
    "                                          saved_train_fracs[\"beamoff\"], saved_train_fracs[\"dirtoverlay\"])\n",
    "\n",
    "print(sample_norms)\n",
    "\n",
    "xgb_overlay = xgboost.DMatrix(overlay_test[bdt_vars], label=[0]*len(overlay_test), missing=-9999.0, feature_names=bdt_vars)\n",
    "xgb_beamoff = xgboost.DMatrix(EXT_test[bdt_vars], label=[0]*len(EXT_test), missing=-9999.0, feature_names=bdt_vars)\n",
    "xgb_dirt = xgboost.DMatrix(Presel_dirt[bdt_vars], label=[0]*len(Presel_dirt), missing=-9999.0, feature_names=bdt_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71a1ac-ecc3-41c4-9d39-5c997abea491",
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_test_signal, frac_train_signal, frac_test_bkg, frac_train_bkg=Test_vs_train_plots(xgb_sig_test_dict, xgb_test_bkg, xgb_sig_train_dict, xgb_train_bkg,bdt_model_dict, bins_dict, \n",
    "                    ymax=0.28, xlims=[-13,8], legsize=19, sig_region=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e274f1-8ea8-4342-ab44-06857bc21a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mass=\"150_pi0\"\n",
    "\n",
    "print(\"test frac:\")\n",
    "print(frac_test_signal[test_mass])\n",
    "print(\"train frac:\")\n",
    "print(frac_train_signal[test_mass])\n",
    "print(\"test bkg frac:\")\n",
    "print(frac_test_bkg[test_mass])\n",
    "print(\"train bkg frac:\")\n",
    "print(frac_train_bkg[test_mass])\n",
    "# frac_test_signal, frac_train_signal, frac_test_bkg, frac_train_bkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040394b4-b43a-4709-bddb-82a180276833",
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_test_dists(xgb_overlay, xgb_beamoff, xgb_dirt, bdt_model_dict, bins_dict, sample_norms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff24f79-31f9-4fc7-9b33-af88795a4be8",
   "metadata": {},
   "source": [
    "## ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4059cd97-bbb7-4f20-8810-2fbdd38904bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(overlay_test))\n",
    "print(len(EXT_test))\n",
    "print(len(dirt_test))\n",
    "\n",
    "print(BDT_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917adb1c-6040-4b6d-88c7-786f730ea1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BDT_name = \"_full_Finished\"\n",
    "\n",
    "sample_names = xgb_train_dict.keys()\n",
    "sig_results_test_dict, bkg_results_test_dict = {}, {}\n",
    "sig_results_train_dict, bkg_results_train_dict = {}, {}\n",
    "overlay_test_results, beamoff_test_results, dirt_test_results = {}, {}, {}\n",
    "\n",
    "if Params[\"Load_pi0_signal\"]==False: sample_type = \"ee\"\n",
    "if Params[\"Load_pi0_signal\"]==True: sample_type = \"pi0\"\n",
    "\n",
    "# xgb_overlay \n",
    "# xgb_beamoff \n",
    "# xgb_dirt\n",
    "\n",
    "for HNL_mass in sample_names:\n",
    "    # bdt = xgboost.Booster()\n",
    "    if HNL_mass.split(\"_\")[1] == \"pi0\": loc = \"bdts/pi0_selection/\"\n",
    "    if HNL_mass.split(\"_\")[1] == \"ee\": loc = \"bdts/\"\n",
    "    # bdt.load_model(loc+Params[\"Run\"]+f\"_{HNL_mass}{BDT_name}.json\")\n",
    "    \n",
    "    bdt = bdt_model_dict[HNL_mass]\n",
    "    \n",
    "    sig_results_test_dict[HNL_mass] = bdt.predict(xgb_sig_test_dict[HNL_mass])\n",
    "    bkg_results_test_dict[HNL_mass] = bdt.predict(xgb_test_bkg)\n",
    "    \n",
    "    sig_results_train_dict[HNL_mass] = bdt.predict(xgb_sig_train_dict[HNL_mass])\n",
    "    bkg_results_train_dict[HNL_mass] = bdt.predict(xgb_train_bkg)\n",
    "    \n",
    "    overlay_test_results[HNL_mass] = bdt.predict(xgb_overlay)\n",
    "    beamoff_test_results[HNL_mass] = bdt.predict(xgb_beamoff)\n",
    "    dirt_test_results[HNL_mass] = bdt.predict(xgb_dirt)\n",
    "    \n",
    "print(bdt_model_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb72cd-8b87-40dc-b7b8-878d62197665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = xgboost.load_config(bdt_model_dict[\"10_ee\"])\n",
    "for HNL_mass in bdt_model_dict:\n",
    "    print(HNL_mass)\n",
    "    print(bdt_model_dict[HNL_mass].num_boosted_rounds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04e982-bb46-4845-9806-ae1c89168913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RocCurves_test(test_sig,test_bkg,mass_label):\n",
    "    \"\"\"\n",
    "    For plotting ROC curves of test samples only.\n",
    "    \"\"\"\n",
    "    \n",
    "    def PosRate(cut_val,var):\n",
    "        return len(var[var>cut_val])/len(var)\n",
    "    \n",
    "    def NegRate(cut_val,var):\n",
    "        return len(var[var<cut_val])/len(var)\n",
    "    \n",
    "    def CalcRate(sig,bkg):\n",
    "        cuts=np.linspace(0,1,200)\n",
    "        FPs=[]\n",
    "        TPs=[]\n",
    "\n",
    "\n",
    "        for cut in cuts:\n",
    "            FPs.append(NegRate(cut,bkg))\n",
    "            TPs.append(PosRate(cut,sig))\n",
    "\n",
    "        FPs=np.array(FPs)\n",
    "        TPs=np.array(TPs)\n",
    "\n",
    "        AUC=-1.0*sum(((TPs[:-1]+TPs[1:])/2)*(FPs[:-1]-FPs[1:]))\n",
    "        rounded_AUC = round(AUC,3)\n",
    "\n",
    "        plt.plot(FPs,TPs,label=f'{mass_label} MeV, AUC={AUC:.{3}g}')\n",
    "        # plt.plot(FPs,TPs,label=f'{mass_label} MeV, AUC={rounded_AUC}')\n",
    "        \n",
    "    CalcRate(test_sig,test_bkg)\n",
    "    \n",
    "def RocCurves_normalised(test_sig, sample_norms, test_overlay, test_beamoff, test_dirt, mass_label):\n",
    "    \"\"\"\n",
    "    For plotting ROC curves of test samples only. \n",
    "    Correctly accounted for POT normalisation but not individual weights (effect minimal).\n",
    "    \"\"\"\n",
    "    \n",
    "    def PosRate(cut_val,var): #Fraction of signal events ABOVE cut_val\n",
    "        return len(var[var>cut_val])/len(var)\n",
    "    \n",
    "    def NegRate(cut_val,overlay,beamoff,dirt): #Fraction of all bkg events BELOW cut_val (normalised, not weighted)\n",
    "        num=(len(overlay[overlay<cut_val])*sample_norms[\"overlay\"])+(len(beamoff[beamoff<cut_val])*sample_norms[\"beamoff\"])+(len(dirt[dirt<cut_val])*sample_norms[\"dirtoverlay\"])\n",
    "        dom=(len(overlay)*sample_norms[\"overlay\"])+(len(beamoff)*sample_norms[\"beamoff\"])+(len(dirt)*sample_norms[\"dirtoverlay\"])\n",
    "        return num/dom\n",
    "    \n",
    "    def CalcRate(sig,overlay,beamoff,dirt):\n",
    "        cuts=np.linspace(0,1,50)\n",
    "        FPs=[]\n",
    "        TPs=[]\n",
    "        \n",
    "        for cut in cuts:\n",
    "            FPs.append(NegRate(cut,overlay,beamoff,dirt))\n",
    "            TPs.append(PosRate(cut,sig))\n",
    "\n",
    "        FPs=np.array(FPs)\n",
    "        TPs=np.array(TPs)\n",
    "\n",
    "        AUC=-1.0*sum(((TPs[:-1]+TPs[1:])/2)*(FPs[:-1]-FPs[1:]))\n",
    "        rounded_AUC = round(AUC,3)\n",
    "\n",
    "        plt.plot(FPs,TPs,label=f'{mass_label} MeV, AUC={AUC:.{3}g}')\n",
    "        \n",
    "    CalcRate(test_sig, test_overlay, test_beamoff, test_dirt)\n",
    "    \n",
    "def RocCurves_test_train(test_sig,test_bkg,train_sig,train_bkg, mass_label):\n",
    "    \"\"\"\n",
    "    For plotting ROC curves of test samples only.\n",
    "    \"\"\"\n",
    "    \n",
    "    def PosRate(cut_val,var):\n",
    "        return len(var[var>cut_val])/len(var)\n",
    "    \n",
    "    def NegRate(cut_val,var):\n",
    "        return len(var[var<cut_val])/len(var)\n",
    "    \n",
    "    def CalcRate(sig,bkg,linestyle):\n",
    "        cuts=np.linspace(0,1,200)\n",
    "        FPs=[]\n",
    "        TPs=[]\n",
    "\n",
    "\n",
    "        for cut in cuts:\n",
    "            FPs.append(NegRate(cut,bkg))\n",
    "            TPs.append(PosRate(cut,sig))\n",
    "\n",
    "        FPs=np.array(FPs)\n",
    "        TPs=np.array(TPs)\n",
    "\n",
    "\n",
    "        AUC=-1.0*sum(((TPs[:-1]+TPs[1:])/2)*(FPs[:-1]-FPs[1:]))\n",
    "        rounded_AUC = round(AUC,3)\n",
    "        \n",
    "        plt.plot(FPs,TPs,label=f'{mass_label} MeV, AUC={AUC:.{3}g}',linestyle=linestyle)\n",
    "        # plt.plot(FPs,TPs,label=f'{mass_label} MeV, AUC={rounded_AUC}')\n",
    "        plt.xlabel(\"False Positives\")\n",
    "        plt.ylabel(\"True Positives\")\n",
    "        plt.legend()\n",
    "        \n",
    "        \n",
    "    CalcRate(test_sig,test_bkg,linestyle=\"solid\")\n",
    "    CalcRate(train_sig,train_bkg,linestyle=\"dashed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb63ea-beab-4141-9ec1-01331afd160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,8])\n",
    "\n",
    "save_fig = input(\"Do you want to save the Figure? y/n \")\n",
    "\n",
    "for HNL_mass in sig_results_test_dict:\n",
    "    mass_label = HNL_mass.split(\"_\")[0]\n",
    "    if mass_label == \"2\": continue\n",
    "    # RocCurves_test(sig_results_test_dict[HNL_mass],bkg_results_test_dict[HNL_mass], mass_label)\n",
    "    RocCurves_normalised(sig_results_test_dict[HNL_mass], sample_norms, overlay_test_results[HNL_mass], beamoff_test_results[HNL_mass], \n",
    "                         dirt_test_results[HNL_mass], mass_label)\n",
    "    \n",
    "x_flat_line=np.linspace(0,1,50)\n",
    "y_flat_line = []\n",
    "for i, x in enumerate(x_flat_line):\n",
    "    y_flat_line.append(1-x)\n",
    "    \n",
    "plt.plot(x_flat_line, y_flat_line, linestyle=\"dashed\", color=\"black\")\n",
    "    \n",
    "plt.xlabel(\"Signal Efficiency\")\n",
    "plt.ylabel(\"Background Rejection\")\n",
    "plt.legend(fontsize=18)\n",
    "\n",
    "if save_fig == \"y\":\n",
    "    plt.savefig(f\"plots/BDT_output/BDT_performance/Test_set_normalised_ROC_\" + Params[\"Run\"] + f\"_{sample_type}{BDT_name}.png\")\n",
    "    plt.savefig(f\"plots/BDT_output/BDT_performance/Test_set_normalised_ROC_\" + Params[\"Run\"] + f\"_{sample_type}{BDT_name}.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81bca8-0b4c-4c8c-ae4c-87f813b66184",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,8])\n",
    "\n",
    "save_fig = input(\"Do you want to save the Figure? y/n \")\n",
    "\n",
    "for HNL_mass in sig_results_test_dict:\n",
    "    mass_label = HNL_mass.split(\"_\")[0]\n",
    "    if mass_label == \"2\": continue\n",
    "    RocCurves_test_train(sig_results_test_dict[HNL_mass],bkg_results_test_dict[HNL_mass],\n",
    "                         sig_results_train_dict[HNL_mass],bkg_results_train_dict[HNL_mass], mass_label)\n",
    "    \n",
    "x_flat_line=np.linspace(0,1,200)\n",
    "y_flat_line = []\n",
    "for i, x in enumerate(x_flat_line):\n",
    "    y_flat_line.append(1-x)\n",
    "    \n",
    "plt.plot(x_flat_line, y_flat_line, linestyle=\"dashed\", color=\"black\")\n",
    "    \n",
    "plt.xlabel(\"False Positives\")\n",
    "plt.ylabel(\"True Positives\")\n",
    "plt.legend(fontsize=19)\n",
    "\n",
    "if save_fig == \"y\":\n",
    "    plt.savefig(f\"plots/BDT_output/BDT_performance/Test_and_train_ROC_\" + Params[\"Run\"] + f\"_{sample_type}{BDT_name}.png\")\n",
    "    plt.savefig(f\"plots/BDT_output/BDT_performance/Test_and_train_ROC_\" + Params[\"Run\"] + f\"_{sample_type}{BDT_name}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5a815-d856-4b76-86af-1abd2b38f6d7",
   "metadata": {},
   "source": [
    "# Finished Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293380da-31c0-4d08-b5f5-7ac71386a1ef",
   "metadata": {},
   "source": [
    "## Checking variable correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ce0d0-4404-4a8c-9acd-642687cf8462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bdt_vars = feature_names\n",
    "HNL_mass = \"150_ee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad1892-8b6b-48f2-af3b-02622353d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from Luis' code\n",
    "# for HNL_mass in HNL_masses:\n",
    "method = 'kendall'\n",
    "correlations = signal_samples_dict[HNL_mass][bdt_vars].astype(np.float64).corr(method=method)\n",
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(correlations,vmin=-1,annot=False,square=True,cbar_kws={'label':method+' correlation'},cmap = 'RdBu_r')\n",
    "plt.title('Input Variable Correlations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd62bb6-b92f-448f-8514-9d5243a9aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just looking at most correlated \n",
    "corr=signal_samples_dict[HNL_mass][bdt_vars].corr()\n",
    "high_corr_var=np.where(corr>0.999)\n",
    "high_corr_var=[(corr.columns[x],corr.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\n",
    "#high_corr_var\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f50740e-dd5e-4cb7-ac69-396d483a0428",
   "metadata": {},
   "source": [
    "## Looking at feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e3e499-e7d3-4b3f-a6b5-00cdab18dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_feature_importances(model_dict, subset_samples, importance_type = \"gain\"):\n",
    "    \"\"\"\n",
    "    Takes a dict of BDT models and plots feature importances for a subset of samples.\n",
    "    \"\"\"\n",
    "    importance_dict = {}\n",
    "    for HNL_mass in model_dict:\n",
    "        importance = model_dict[HNL_mass].get_score(importance_type=importance_type)\n",
    "        #sort them here\n",
    "        sorted_importance = dict(sorted(importance.items(), key=lambda item: item[1]))\n",
    "        sorted_importance_list = list(sorted_importance.values())\n",
    "        sorted_importance_keys= list(sorted_importance.keys())\n",
    "        importance_dict[HNL_mass] = importance\n",
    "        \n",
    "    color_cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    plt.figure(figsize=[20,8])\n",
    "    \n",
    "    for i, HNL_mass in enumerate(subset_samples):\n",
    "        mass_label=HNL_mass.split(\"_\")[0]\n",
    "        sample_type = HNL_mass.split(\"_\")[1]\n",
    "        if sample_type == \"ee\": model_label = f\"{mass_label} MeV \" + r\"$\\nu e^{+}e^{-}$\" + \" model\"\n",
    "        if sample_type == \"pi0\": model_label = f\"{mass_label} MeV \" + r\"$\\nu \\pi^{0}$\" + \" model\"\n",
    "        plt.bar(importance_dict[HNL_mass].keys(),importance_dict[HNL_mass].values(), label=model_label, \n",
    "        fill=False,linewidth=3, edgecolor=color_cycle[i])\n",
    "        \n",
    "    correct_names = []\n",
    "    for var in importance_dict[HNL_mass].keys():\n",
    "        correct_names.append(Constants.variable_no_units[var])\n",
    "\n",
    "    # plt.xticks(np.array(range(0, len(importance_dict[HNL_mass].keys()))),importance_dict[HNL_mass].keys(),rotation=80)\n",
    "    plt.xticks(np.array(range(0, len(importance_dict[HNL_mass].keys()))),correct_names,rotation=80)\n",
    "    plt.ylabel(\"Importance\")\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.tight_layout()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2417cc-f0e2-411f-ad6e-c7ed9cea6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(Constants)\n",
    "if sample_type == \"ee\": subset_samples = [\"10_ee\", \"50_ee\", \"150_ee\"]\n",
    "if sample_type == \"pi0\": subset_samples = [\"150_pi0\", \"200_pi0\", \"245_pi0\"]\n",
    "Plot_feature_importances(bdt_model_dict, subset_samples, importance_type = \"gain\")\n",
    "\n",
    "save_fig = input(\"Do you want to save the Figure? y/n \")\n",
    "\n",
    "if save_fig==\"y\":\n",
    "    plt.savefig(\"plots/BDT_output/variable_importance/\"+ Params[\"Run\"]+f\"_importances_{sample_type}{BDT_name}.pdf\")\n",
    "    plt.savefig(\"plots/BDT_output/variable_importance/\"+ Params[\"Run\"]+f\"_importances_{sample_type}{BDT_name}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380877bb-f8ed-4648-88fd-3e4eec8c6c1d",
   "metadata": {},
   "source": [
    "# Finished code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
