{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab81f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.24/06\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "import os,sys,string, time\n",
    "import ROOT\n",
    "from math import *\n",
    "from ROOT import gPad, TTree, TObject, TFile, gDirectory, TH1D, TH2D, TH3D, TCanvas, gROOT, TGaxis, gStyle, TColor, TLegend, THStack, TChain, TLatex, TText, TCollection, kRed, kBlue\n",
    "from array import array\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from root_pandas import read_root\n",
    "from platform import python_version\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import math\n",
    "from matplotlib.patches import Rectangle\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "import pickle\n",
    "\n",
    "import Utilities.Plotter as PT\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Variables_list as Variables\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print ('Success')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a3504-392d-4b23-a225-0db439fcb0f5",
   "metadata": {},
   "source": [
    "## Reading in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e41222-cb45-4107-a206-3b3653d23ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = {\"Run\":\"run3\", #The run number, so far either \"run1\" or \"run3\"\n",
    "          \"Load_single_file\":False, #This will override everything else, put the desired file in the \"single_file\" line\n",
    "          \"single_file\":10,\n",
    "          \"Load_standard\":True,\n",
    "          \"Load_DetVars\":False,\n",
    "          \"Only_keep_common_DetVar_evs\":True,\n",
    "          \"Load_data\":False,\n",
    "          \"FLATTEN\":True, #Have one row per reconstructed object in the analysis dataframe\n",
    "          \"only_presel\":False, #Create small files containing only variables necessary for pre-selection, for making pre-selection plots\n",
    "          \"EXT_in_training\":False,\n",
    "          \"Load_pi0_signal\":True} \n",
    "\n",
    "feature_names = Variables.First_pass_vars_for_BDT #All variables\n",
    "feature_names_MC = feature_names + [\"weight\"]\n",
    "loc_pkls = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/my_vars/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d786fc-af51-48e5-88d4-d0ee5bcd2bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['run', 'sub', 'evt', 'nslice', 'n_pfps', 'n_tracks', 'n_showers',\n",
      "       'swtrig_pre', 'swtrig_post', 'trk_sce_start_x_v', 'trk_sce_start_y_v',\n",
      "       'trk_sce_start_z_v', 'trk_sce_end_x_v', 'trk_sce_end_y_v',\n",
      "       'trk_sce_end_z_v', 'shr_theta_v', 'shr_phi_v', 'shr_px_v', 'shr_py_v',\n",
      "       'shr_pz_v', 'shrclusdir0', 'shrclusdir1', 'shrclusdir2',\n",
      "       'shr_energy_tot', 'trk_theta_v', 'trk_phi_v', 'trk_dir_x_v',\n",
      "       'trk_dir_y_v', 'trk_dir_z_v', 'trk_energy', 'trk_energy_hits_tot',\n",
      "       'trk_energy_tot', 'trk_score_v', 'trk_calo_energy_u_v', 'trk_end_x_v',\n",
      "       'trk_chipr_best', 'pfnplanehits_U', 'pfnplanehits_V', 'pfnplanehits_Y',\n",
      "       'NeutrinoEnergy2', 'SliceCaloEnergy2', 'nu_flashmatch_score',\n",
      "       'contained_sps_ratio', 'flash_time', 'contained_fraction', 'trk_score',\n",
      "       'crtveto', 'weightSplineTimesTune', 'ppfx_cv', 'npi0', 'weight',\n",
      "       'min_x', 'max_x', 'min_y', 'max_y', 'min_z', 'max_z', 'highest_E'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "signal_samples_dict = {}\n",
    "\n",
    "Presel_overlay = pd.read_pickle(loc_pkls+\"Preselected_overlay_\"+Params[\"Run\"]+\"_my_vars_flattened_FINAL.pkl\")\n",
    "\n",
    "if Params[\"Load_pi0_signal\"] == False:\n",
    "    if Params[\"Load_single_file\"] == True:\n",
    "        HNL_mass = Params[\"single_file\"]\n",
    "        Presel_signal = pd.read_pickle(loc_pkls+f\"Preselected_{HNL_mass}_\"+Params[\"Run\"]+\"_my_vars_flattened_FINAL.pkl\")\n",
    "        signal_samples_dict[HNL_mass] = Presel_signal\n",
    "    else:\n",
    "        for HNL_mass in Constants.HNL_mass_samples:\n",
    "            Presel_signal = pd.read_pickle(loc_pkls+f\"Preselected_{HNL_mass}_\"+Params[\"Run\"]+\"_my_vars_flattened_FINAL.pkl\")\n",
    "            signal_samples_dict[HNL_mass] = Presel_signal\n",
    "    \n",
    "if Params[\"Load_pi0_signal\"] == True:\n",
    "    for HNL_mass in Constants.HNL_mass_pi0_samples:\n",
    "        Presel_signal = pd.read_pickle(loc_pkls+f\"pi0_selection/Preselected_\"+Params[\"Run\"]+f\"_{HNL_mass}_pi0_FINAL.pkl\")\n",
    "        # Presel_signal = pd.read_pickle(loc_pkls+f\"pi0_selection/Preselected_\"+Params[\"Run\"]+f\"_{HNL_mass}_pi0_ultimate.pkl\")\n",
    "        signal_samples_dict[HNL_mass] = Presel_signal\n",
    "    \n",
    "if Params[\"EXT_in_training\"] == True:\n",
    "    Presel_EXT = pd.read_pickle(loc_pkls+\"Preselected_EXT_\"+Params[\"Run\"]+\"_my_vars_flattened_ultimate.pkl\")\n",
    "\n",
    "print(Presel_overlay.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb76888-7925-407b-9a82-b6a997a12e5e",
   "metadata": {},
   "source": [
    "## Splitting into test and training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d1638c-9af1-4f64-aed1-16affdd7aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features is 19\n",
      "Total length of overlay file is 28355\n",
      "Total length of overlay TRAIN file is 19848\n",
      "Total length of overlay TEST file is 8507\n"
     ]
    }
   ],
   "source": [
    "ultimate_feature_list = ['n_pfps', 'n_tracks', 'shr_theta_v', 'shr_phi_v', 'shr_pz_v', 'shrclusdir2', 'shr_energy_tot',\n",
    "                         'trk_theta_v', 'trk_phi_v','trk_dir_z_v', 'trk_energy', 'trk_energy_tot', 'trk_calo_energy_u_v', 'trk_score_v',\n",
    "                         'pfnplanehits_U', 'pfnplanehits_V', 'pfnplanehits_Y', 'NeutrinoEnergy2']#, 'nu_flashmatch_score']\n",
    "if Params[\"Run\"]==\"run3\":\n",
    "    ultimate_feature_list += ['nu_flashmatch_score']\n",
    "print(f\"Number of features is {len(ultimate_feature_list)}\")\n",
    "\n",
    "bdt_vars = ultimate_feature_list #This is using just the most important variables list\n",
    "\n",
    "new_value = -9999 #This tells XGB what number refers to missing data for all variables\n",
    "\n",
    "signal_train_dict = {}\n",
    "signal_test_dict = {}\n",
    "labels_dict = {} \n",
    "\n",
    "train_vs_test_fraction = 0.7 #This is the fraction used for training\n",
    "\n",
    "print(f\"Total length of overlay file is {len(Presel_overlay)}\")\n",
    "\n",
    "overlay_train = Presel_overlay[:int(len(Presel_overlay)*train_vs_test_fraction)]\n",
    "overlay_test = Presel_overlay[int(len(Presel_overlay)*train_vs_test_fraction):]\n",
    "\n",
    "print(f\"Total length of overlay TRAIN file is {len(overlay_train)}\")\n",
    "print(f\"Total length of overlay TEST file is {len(overlay_test)}\")\n",
    "\n",
    "# overlay_train = Presel_overlay[int(len(Presel_overlay)*train_vs_test_fraction):] #OLD WRONG WAY, i.e 70% test\n",
    "# overlay_test = Presel_overlay[:int(len(Presel_overlay)*train_vs_test_fraction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f2f4b4-2c9c-4987-9919-904f2d9d36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Params[\"Load_pi0_signal\"] == False: BDT_name = \"ee_FINAL_2\"\n",
    "if Params[\"Load_pi0_signal\"] == True: BDT_name = \"pi0_FINAL_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8dfcb7-34ab-4420-adb5-14410c9f717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_files = False\n",
    "\n",
    "overlay_test.to_pickle(loc_pkls+\"BDT_Test_dfs/Test_overlay_\"+Params[\"Run\"]+\"_FINAL.pkl\")\n",
    "if Params[\"Load_pi0_signal\"] == False:\n",
    "    # overlay_test.to_pickle(loc_pkls+\"BDT_Test_dfs/Test_overlay_\"+Params[\"Run\"]+\"_my_vars_flattened_ultimate.pkl\")\n",
    "    for HNL_mass in signal_samples_dict:\n",
    "        signal_train_dict[HNL_mass] = signal_samples_dict[HNL_mass][:int(len(signal_samples_dict[HNL_mass])*train_vs_test_fraction)]\n",
    "        signal_test_dict[HNL_mass] = signal_samples_dict[HNL_mass][int(len(signal_samples_dict[HNL_mass])*train_vs_test_fraction):]\n",
    "        if pickle_files == True:\n",
    "            print(f\"Pickling {HNL_mass}MeV HNL test sample\")\n",
    "            signal_test_dict[HNL_mass].to_pickle(loc_pkls+f\"BDT_Test_dfs/Test_signal_{HNL_mass}_\"+Params[\"Run\"]+\"_FINAL.pkl\")\n",
    "    \n",
    "        labels_dict[HNL_mass] = [1]*len(signal_train_dict[HNL_mass][bdt_vars]) + [0]*len(overlay_train[bdt_vars])\n",
    "if Params[\"Load_pi0_signal\"] == True:\n",
    "    overlay_test.to_pickle(loc_pkls+\"BDT_Test_dfs/pi0_selection/Test_overlay_\"+Params[\"Run\"]+\".pkl\")\n",
    "\n",
    "    for HNL_mass in signal_samples_dict:\n",
    "        signal_train_dict[HNL_mass] = signal_samples_dict[HNL_mass][:int(len(signal_samples_dict[HNL_mass])*train_vs_test_fraction)]\n",
    "        signal_test_dict[HNL_mass] = signal_samples_dict[HNL_mass][int(len(signal_samples_dict[HNL_mass])*train_vs_test_fraction):]\n",
    "        if pickle_files == True:\n",
    "            print(f\"Pickling {HNL_mass}MeV HNL pi0 test sample\")\n",
    "            signal_test_dict[HNL_mass].to_pickle(loc_pkls+f\"BDT_Test_dfs/pi0_selection/Test_signal_{HNL_mass}_pi0_\"+\n",
    "                                                 Params[\"Run\"]+\"_FINAL.pkl\")\n",
    "    \n",
    "        labels_dict[HNL_mass] = [1]*len(signal_train_dict[HNL_mass][bdt_vars]) + [0]*len(overlay_train[bdt_vars])\n",
    "    \n",
    "if Params[\"EXT_in_training\"] == True:\n",
    "    frac_EXT = 0.1\n",
    "    EXT_train = Presel_EXT[:int(len(Presel_EXT)*frac_EXT)]\n",
    "    print(\"Number of EXT to train: \" + str(len(EXT_train)))\n",
    "    overlay_plus_EXT = pd.concat([overlay_train[bdt_vars],EXT_train[bdt_vars]])\n",
    "    for HNL_mass in Constants.HNL_mass_samples:\n",
    "        labels_dict[HNL_mass] = labels_dict[HNL_mass] + [0]*len(EXT_train[bdt_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e387da2f-dfb1-4433-a85a-5b98d40611d3",
   "metadata": {},
   "source": [
    "## BDT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f0a87a-aad9-4ec4-98ba-517bfe7b7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_dict = {}\n",
    "xgb_test_dict = {}\n",
    "\n",
    "xgb_sig_train_dict = {}\n",
    "xgb_bkg_train_dict = {}\n",
    "\n",
    "xgb_test_bkg = xgboost.DMatrix(overlay_test[bdt_vars], label=[0]*len(overlay_test[bdt_vars]), missing=new_value, feature_names=bdt_vars)\n",
    "\n",
    "xgb_param = {'booster': 'dart',\n",
    "        'max_depth':6,\n",
    "        'eta':0.3,\n",
    "        'objective':'binary:logistic',\n",
    "#        'eval_metric':'auc', \n",
    "#        'subsample':0.5,\n",
    "        'tree_method':'hist',\n",
    "#        'scale_pos_weight': float(len(data_bkg_train))/float(len(data_sig_train)),\n",
    "        'rate_drop': 0.1,\n",
    "        'skip_drop': 0.5 }\n",
    "num_round = 150\n",
    "progress = dict()\n",
    "\n",
    "for HNL_mass in signal_train_dict:\n",
    "    if Params[\"EXT_in_training\"] == False:\n",
    "        xgb_train_dict[HNL_mass] = xgboost.DMatrix(pd.concat([signal_train_dict[HNL_mass][bdt_vars], #This is both signal and bkg combined into one\n",
    "                                                               overlay_train[bdt_vars]]), \n",
    "                                               label=labels_dict[HNL_mass], \n",
    "                                                    missing=new_value, feature_names=bdt_vars)\n",
    "    if Params[\"EXT_in_training\"] == True:\n",
    "        xgb_train_dict[HNL_mass] = xgboost.DMatrix(pd.concat([signal_train_dict[HNL_mass][bdt_vars], #This is both signal and bkg combined into one\n",
    "                                                               overlay_plus_EXT[bdt_vars]]), \n",
    "                                               label=labels_dict[HNL_mass], \n",
    "                                                    missing=new_value, feature_names=bdt_vars)\n",
    "    xgb_test_dict[HNL_mass] = xgboost.DMatrix(signal_test_dict[HNL_mass][bdt_vars], label=[1]*len(signal_test_dict[HNL_mass][bdt_vars]), #Just signal test\n",
    "                                              missing=new_value, feature_names=bdt_vars)\n",
    "    \n",
    "    xgb_sig_train_dict[HNL_mass] = xgboost.DMatrix(signal_train_dict[HNL_mass][bdt_vars],label=[1]*len(signal_train_dict[HNL_mass][bdt_vars]), #Signal training\n",
    "                                                  missing=new_value, feature_names=bdt_vars)\n",
    "    xgb_bkg_train_dict[HNL_mass] = xgboost.DMatrix(overlay_train[bdt_vars],label=[0]*len(overlay_train[bdt_vars]), #Just background training\n",
    "                                                  missing=new_value, feature_names=bdt_vars)\n",
    "\n",
    "    #watchlist so that you can monitor the performance of the training by iterations\n",
    "    watchlist = [(xgb_train_dict[HNL_mass], 'train'), (xgb_test_dict[HNL_mass], 'test_sig'), (xgb_test_bkg,'test_bkg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd8965e-42ff-466b-9971-24bb5e0cb0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 150MeV BDT\n",
      "\n",
      "[17:31:46] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 180MeV BDT\n",
      "\n",
      "[17:32:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 200MeV BDT\n",
      "\n",
      "[17:32:37] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 220MeV BDT\n",
      "\n",
      "[17:33:14] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 240MeV BDT\n",
      "\n",
      "[17:33:42] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 245MeV BDT\n",
      "\n",
      "[17:34:09] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "# if Params[\"Load_pi0_signal\"] == False: BDT_name = \"ee_FINAL\"\n",
    "# if Params[\"Load_pi0_signal\"] == True: BDT_name = \"pi0_FINAL\"\n",
    "\n",
    "for HNL_mass in signal_train_dict:\n",
    "    print(f\"Training {HNL_mass}MeV BDT\" + \"\\n\")\n",
    "    bdt = xgboost.train(xgb_param, xgb_train_dict[HNL_mass], num_round, watchlist, evals_result=progress, verbose_eval=False)\n",
    "    # doesnt like watchlist/eval_result if using AOC\n",
    "    # save model so you can load it later\n",
    "    if Params[\"Load_pi0_signal\"] == False:\n",
    "        bdt.save_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "    if Params[\"Load_pi0_signal\"] == True:\n",
    "        bdt.save_model(\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ed366-ed68-4d56-b969-0887b09bc6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BDT_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa754b8-6edb-43bf-8ed4-5864a4c4a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = bdt_vars\n",
    "\n",
    "# BDT_name = \"New_20_variables_FIXED\"\n",
    "if Params[\"Load_pi0_signal\"] == False:\n",
    "    with open(f\"bdts/input_vars/{BDT_name}_\"+Params[\"Run\"], \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(var_list, fp)\n",
    "elif Params[\"Load_pi0_signal\"] == True:\n",
    "    with open(f\"bdts/pi0_selection/input_vars/{BDT_name}_\"+Params[\"Run\"], \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(var_list, fp)\n",
    "\n",
    "# with open(\"bdts/input_vars/\"+BDT_name, \"rb\") as fp:   # Unpickling\n",
    "#     b = pickle.load(fp)\n",
    "    \n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5a815-d856-4b76-86af-1abd2b38f6d7",
   "metadata": {},
   "source": [
    "# Finished Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293380da-31c0-4d08-b5f5-7ac71386a1ef",
   "metadata": {},
   "source": [
    "## Checking variable correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ce0d0-4404-4a8c-9acd-642687cf8462",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_vars = feature_names\n",
    "HNL_mass = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad1892-8b6b-48f2-af3b-02622353d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from Luis' code\n",
    "# for HNL_mass in HNL_masses:\n",
    "method = 'kendall'\n",
    "correlations = signal_samples_dict[HNL_mass][bdt_vars].astype(np.float64).corr(method=method)\n",
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(correlations,vmin=-1,annot=False,square=True,cbar_kws={'label':method+' correlation'},cmap = 'RdBu_r')\n",
    "plt.title('Input Variable Correlations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd62bb6-b92f-448f-8514-9d5243a9aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just looking at most correlated \n",
    "corr=signal_samples_dict[HNL_mass][bdt_vars].corr()\n",
    "high_corr_var=np.where(corr>0.999)\n",
    "high_corr_var=[(corr.columns[x],corr.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\n",
    "#high_corr_var\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f50740e-dd5e-4cb7-ac69-396d483a0428",
   "metadata": {},
   "source": [
    "## Looking at feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351d3da-76cc-4079-9036-548307e99353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make an importances dict   \n",
    "def Top_N_vars(bdt_model, N_vars):\n",
    "    importance = bdt.get_score(importance_type=\"gain\")\n",
    "    # for key in importance.keys():\n",
    "    #     importance[key] = round(importance[key],1)\n",
    "    sorted_importance = dict(sorted(importance.items(), key=lambda item: item[1]))\n",
    "    sorted_importance_list = list(sorted_importance.values())\n",
    "    sorted_importance_keys= list(sorted_importance.keys())\n",
    "    top_N = sorted_importance_keys[-N_vars:]\n",
    "    \n",
    "    return top_N\n",
    "\n",
    "def Sorted_importance(bdt_model):\n",
    "    importance = bdt.get_score(importance_type=\"gain\")\n",
    "    return importance\n",
    "\n",
    "top_N_dict = {}\n",
    "importance_dict = {}\n",
    "list_of_lists = []\n",
    "BDT_name = \"pi0_ultimate_full_reduced_benchmark\"\n",
    "# BDT_name = \"pi0\"\n",
    "\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in Constants.HNL_mass_pi0_samples:\n",
    "    bdt = xgboost.Booster()\n",
    "    # bdt.load_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_ultimate.json\")\n",
    "    # bdt.load_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "    bdt.load_model(\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "    # print(\"Number of entries in top20 is \" + str(len(Top_N_vars(bdt, 20))))\n",
    "    # top_N_dict[HNL_mass] = Top_N_vars(bdt, 20)\n",
    "    top_N = Top_N_vars(bdt, 50)\n",
    "    top_N_dict[HNL_mass] = Top_N_vars(bdt, 50)\n",
    "    list_of_lists.append(Top_N_vars(bdt, 50))\n",
    "    importance_dict[HNL_mass] = Sorted_importance(bdt)\n",
    "    \n",
    "elements_in_all = list(set.intersection(*map(set, list_of_lists)))\n",
    "print(len(elements_in_all))\n",
    "print(elements_in_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be7c30-7f83-4734-b9d7-98bab2c7556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "highets_imps = ['n_showers', 'NeutrinoEnergy2', 'secondshower_Y_charge', 'trk_chipr_best', 'trk_energy_hits_tot', 'shr_energy_tot', \n",
    "                'contained_sps_ratio', 'pfnplanehits_Y', 'shrclusdir2', 'trk_dir_y_v', 'SliceCaloEnergy2', 'trk_theta_v', 'trk_start_x_v', \n",
    "                'pi0_radlen2', 'pfnplanehits_V', 'trk_energy', 'trk_score_v', 'shrclusdir0', 'trk_bragg_mip_v', 'shr_py_v', \n",
    "                'nu_flashmatch_score', 'n_pfps', 'CosmicIPAll3D', 'pi0_dir2_z']\n",
    "\n",
    "ultimate_feature_list = ['n_pfps', 'n_tracks', 'shr_theta_v', 'shr_phi_v', 'shr_pz_v', 'shrclusdir2', 'shr_energy_tot',\n",
    "                         'trk_theta_v', 'trk_phi_v','trk_dir_z_v', 'trk_energy', 'trk_energy_tot', 'trk_calo_energy_u_v', 'trk_score_v',\n",
    "                         'pfnplanehits_U', 'pfnplanehits_V', 'pfnplanehits_Y', 'NeutrinoEnergy2', 'nu_flashmatch_score']\n",
    "\n",
    "elements_in_all = list(set.intersection(*map(set, [highets_imps,ultimate_feature_list])))\n",
    "print(len(elements_in_all))\n",
    "print(elements_in_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd2dc0-fc9a-4d54-b256-3785ba5e6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importance_dict[245].keys())\n",
    "print(len(importance_dict[245].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de628a-83ec-4505-8147-3a1e92e5b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "plt.figure(figsize=[20,8])\n",
    "colours = {2:\"coral\", 100:\"cornflowerblue\", 245:\"olivedrab\"}\n",
    "for HNL_mass in [2, 100, 245]:\n",
    "    plt.bar(importance_dict[HNL_mass].keys(),importance_dict[HNL_mass].values(), label=f\"{HNL_mass}MeV model\", \n",
    "            fill=False,linewidth=3, edgecolor=colours[HNL_mass], color=colours[HNL_mass])\n",
    "# plt.bar(importance_dict[245].keys(),importance_dict[245].values())\n",
    "plt.xticks(np.array(range(0, len(importance_test.keys()))),importance_test.keys(),rotation=80)\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.legend(fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"plots/BDT_output/variable_importance/\"+ Params[\"Run\"]+f\"_importances_{BDT_name}.pdf\")\n",
    "plt.savefig(\"plots/BDT_output/variable_importance/\"+ Params[\"Run\"]+f\"_importances_{BDT_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979aa2f-7260-47da-a99c-766ceabfc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20,8])\n",
    "colours = {150:\"coral\", 200:\"cornflowerblue\", 245:\"olivedrab\"}\n",
    "for HNL_mass in [245]:\n",
    "    plt.bar(importance_dict[HNL_mass].keys(),importance_dict[HNL_mass].values(), label=f\"{HNL_mass}MeV model\", \n",
    "            fill=False,linewidth=3, edgecolor=colours[HNL_mass], color=colours[HNL_mass])\n",
    "# plt.bar(importance_dict[245].keys(),importance_dict[245].values())\n",
    "plt.xticks(np.array(range(0, len(importance_dict[HNL_mass].keys()))),importance_dict[HNL_mass].keys(),rotation=80)\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.legend(fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"plots/BDT_output/variable_importance/\"+ Params[\"Run\"]+f\"_importances_{BDT_name}.pdf\")\n",
    "# plt.savefig(\"plots/BDT_output/variable_importance/\"+ Params[\"Run\"]+f\"_importances_{BDT_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea472ee-95b7-425f-9eaa-845b4da3425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[20,20])\n",
    "ax1 = fig.add_subplot(projection='3d')\n",
    "\n",
    "mass_list = [\"2MeV\", \"100MeV\", \"245MeV\"]\n",
    "x = importance_dict[2].keys()\n",
    "y = mass_list\n",
    "data = np.array([list(importance_dict[2].values()),\n",
    "                 list(importance_dict[100].values()),\n",
    "                 list(importance_dict[245].values())])\n",
    "\n",
    "numOfCols = len(x)\n",
    "numOfRows = len(y)\n",
    "\n",
    "xpos = np.arange(0, numOfCols, 1)\n",
    "ypos = np.arange(0, numOfRows, 1)\n",
    "xpos, ypos = np.meshgrid(xpos + 0.5, ypos + 0.5)\n",
    "\n",
    "xpos = xpos.flatten()\n",
    "ypos = ypos.flatten()\n",
    "zpos = np.zeros(numOfCols * numOfRows)\n",
    "\n",
    "dx = np.ones(numOfRows * numOfCols) * 0.5\n",
    "dy = np.ones(numOfCols * numOfRows) * 0.5\n",
    "dz = data.flatten()\n",
    "\n",
    "mass_list_labels = [\" \",\" \", \"2MeV\",\" \", \"100MeV\",\" \", \"245MeV\"]\n",
    "\n",
    "ax1.bar3d(xpos, ypos, zpos, dx, dy, dz)\n",
    "ax1.set_xticklabels(list(importance_dict[2].keys()),rotation=45)\n",
    "# ax1.set_xticklabels(list(importance_dict[2].keys()))\n",
    "ax1.set_yticklabels(mass_list_labels)\n",
    "\n",
    "ax1.set_xlabel('Variable')\n",
    "ax1.set_ylabel('Model')\n",
    "ax1.set_zlabel('Importance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380877bb-f8ed-4648-88fd-3e4eec8c6c1d",
   "metadata": {},
   "source": [
    "# Finished code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28366e0-1cea-490c-840e-bcccdfd88413",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966bb616-3075-4722-bbad-57a1e9afb281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirt_matrix = xgboost.DMatrix(dirt_BDT[bdt_vars])\n",
    "# EXT_matrix = xgboost.DMatrix(EXT_BDT[bdt_vars])\n",
    "\n",
    "test_results_sig_dict = {}\n",
    "test_results_bkg_dict = {}\n",
    "\n",
    "train_results_sig_dict = {}\n",
    "train_results_bkg_dict = {}\n",
    "\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "\n",
    "    bdt = xgboost.Booster()\n",
    "    bdt.load_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}_MeV_New_20_variables_FIXED.json\")\n",
    "    #bdt.load_model(f'bdts/{Run}_{HNL_mass}_MeV_REDUCED_variables_flattened_highest_E_2.json')\n",
    "    \n",
    "    importance = bdt.get_score(importance_type=\"gain\")\n",
    "    \n",
    "    for key in importance.keys():\n",
    "        importance[key] = round(importance[key],1)\n",
    "        \n",
    "    #importance_dict[HNL_mass] = importance\n",
    "\n",
    "    results_sig = bdt.predict(xgb_test_dict[HNL_mass])\n",
    "    results_bkg = bdt.predict(xgb_test_bkg)\n",
    "    \n",
    "    train_results_sig = bdt.predict(xgb_sig_train_dict[HNL_mass])\n",
    "    train_results_bkg = bdt.predict(xgb_bkg_train_dict[HNL_mass])\n",
    "    \n",
    "    test_results_sig_dict.update({HNL_mass:results_sig})\n",
    "    test_results_bkg_dict.update({HNL_mass:results_bkg})\n",
    "    \n",
    "    train_results_sig_dict.update({HNL_mass:train_results_sig})\n",
    "    train_results_bkg_dict.update({HNL_mass:train_results_bkg})\n",
    "\n",
    "    # results_dirt = bdt.predict(dirt_matrix)\n",
    "    # results_EXT = bdt.predict(EXT_matrix)\n",
    "    \n",
    "    # dirt_BDT[f'BDT_output_{HNL_mass}MeV'] = results_dirt\n",
    "    # EXT_BDT[f'BDT_output_{HNL_mass}MeV'] = results_EXT\n",
    "\n",
    "    # overlay_test_BDT[f'BDT_output_{HNL_mass}MeV'] = results_bkg\n",
    "    # #Can add in a second loop over HNL_masses so that I predict each signal mass point with every other mass point bdt\n",
    "    # signal_test_BDT_dict[HNL_mass][f'BDT_output'] = results_sig\n",
    "    \n",
    "    #Plotting importances of variables\n",
    "    plt.figure(figsize=(12,12),facecolor='white')\n",
    "    print(f\"Plotting {HNL_mass}MeV importances:\")\n",
    "    a = xgboost.plot_importance(importance,max_num_features=10,importance_type='gain')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91c75b-3da4-4991-8cdb-aa6644bba871",
   "metadata": {},
   "source": [
    "## Test vs. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab0164d-636e-4e96-9b92-56e108ccae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_range=[0,1.0]\n",
    "n_bins=20\n",
    "\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.hist(train_results_sig_dict[HNL_mass],bins=n_bins, range=hist_range, density=True,alpha=0.4,color='red',label=f'Train {HNL_mass}MeV HNL' )\n",
    "    counts,bin_edges = np.histogram(test_results_sig_dict[HNL_mass],bins=n_bins,range=hist_range,density=True)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:])/2.\n",
    "    plt.plot(bin_centers,counts,marker='o',linestyle=\"None\",color='red',label=f'Test {HNL_mass}MeV HNL')\n",
    "\n",
    "    plt.hist(train_results_bkg_dict[HNL_mass], bins = n_bins, range = hist_range, density = True, alpha = 0.4, color = 'orange', label = r'Train overlay')\n",
    "    counts,bin_edges = np.histogram(test_results_bkg_dict[HNL_mass],bins = n_bins, range= hist_range,density = True)\n",
    "    bin_centers = (bin_edges[:-1] +  bin_edges[1:])/2.\n",
    "    plt.plot(bin_centers,counts,marker='o',linestyle =\"None\",color='orange',label = r'Test overlay')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5106d9-d72f-432e-a947-3c022798f773",
   "metadata": {},
   "source": [
    "## Plotting BDT Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafc2434-7e9f-4318-97ab-54f9c4a1f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting BDT output after only keeping highest bdt score per event\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    # plt.figure(figsize=(12,8),facecolor='white')\n",
    "    numdecays_signal = len(cleaned_signal_dict_highest_E[HNL_mass])\n",
    "    numdecays_bkg = len(Cleaned_Presel_overlay_highest_E)\n",
    "    SF_sig = numdecays_signal/len(signal_test_dict[HNL_mass][f'BDT_output'])\n",
    "    SF_overlay = (numdecays_bkg)/len(data_test_bkg[f'BDT_output_{HNL_mass}MeV'])\n",
    "    \n",
    "    # print(\"Scale factor for signal is \" + str(SF_sig)) #Should have a dict of SF_sig and SF_overlay\n",
    "    # print(\"Scale factor for overlay is \" + str(SF_overlay))\n",
    "    \n",
    "    if Run == \"run1\":\n",
    "        SF_overlay_run1_corrected = Constants.SF_overlay_run1*SF_overlay #Necessary to account for the events used for training\n",
    "        SF_signal_run1_corrected = SF_sig #Necessary to account for the events used for training\n",
    "        EXT_scale_list = np.ones(len(Cleaned_Presel_EXT_highest_E[f'BDT_output_{HNL_mass}MeV']))*Constants.SF_EXT_run1\n",
    "        EXT_scale = Constants.SF_EXT_run1\n",
    "        dirt_scale = Constants.SF_dirt_run1\n",
    "        signal_scale_list = np.ones(len(signal_test_dict[HNL_mass][f'BDT_output']))*SF_signal_run1_corrected\n",
    "        overlay_scale = SF_overlay_run1_corrected\n",
    "        \n",
    "    elif Run == \"run3\":\n",
    "        SF_overlay_run3_corrected = Constants.SF_overlay_run3*SF_overlay #Necessary to account for the events used for training\n",
    "        SF_signal_run3_corrected = SF_sig #Necessary to account for the events used for training\n",
    "        EXT_scale_list = np.ones(len(Cleaned_Presel_EXT_highest_E[f'BDT_output_{HNL_mass}MeV']))*Constants.SF_EXT_run3\n",
    "        EXT_scale = Constants.SF_EXT_run3\n",
    "        dirt_scale = Constants.SF_dirt_run3\n",
    "        signal_scale_list = np.ones(len(signal_test_dict[HNL_mass][f'BDT_output']))*SF_signal_run3_corrected\n",
    "        overlay_scale = SF_overlay_run3_corrected\n",
    "\n",
    "print(\"Creating the sample and normalisation dictionaries\")    \n",
    "samples={'overlay_test':data_test_bkg,\n",
    "         'dirtoverlay':Cleaned_Presel_dirt_highest_E,\n",
    "         'beamoff':Cleaned_Presel_EXT_highest_E}\n",
    "\n",
    "sample_norms={'overlay_test':np.array(data_test_bkg[\"weight\"]*overlay_scale),\n",
    "         'dirtoverlay':np.array(Cleaned_Presel_dirt_highest_E[\"weight\"]*dirt_scale),\n",
    "         'beamoff':EXT_scale_list}\n",
    "\n",
    "print(\"Adding signal samples to sample dictionary\")\n",
    "for HNL_mass in HNL_masses:\n",
    "    signal_scale_list = np.ones(len(signal_test_dict[HNL_mass][f'BDT_output']))*SF_sig\n",
    "    \n",
    "    sample_placeholder = {HNL_mass:signal_test_dict[HNL_mass]}\n",
    "    norm_placeholder = {HNL_mass:signal_scale_list}\n",
    "    samples.update(sample_placeholder)\n",
    "    sample_norms.update(norm_placeholder)\n",
    "    print(len(sample_norms[HNL_mass]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab964df9-eed6-4151-86a1-251ae059b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT.Plot_BDT_output(HNL_masses=HNL_masses, samples=samples, sample_norms=sample_norms, colours={}, xlims=[0,1.0],\n",
    "                bins=20,figsize=[12,8], MergeBins=False, density=False, legloc=\"upper center\",logy=True, savefig=False, Run=Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fe26e-28b5-4cd3-9f18-200d99f0e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the histograms for detector variations\n",
    "HNL_mass = 20\n",
    "if Load_DetVars == True:\n",
    "    for DetVar in DetVar_BDT_dict:\n",
    "        linewidth = 1\n",
    "        if DetVar == \"CV\":\n",
    "            linewidth = 3\n",
    "        plt.hist(DetVar_BDT_dict[DetVar][f'BDT_output_{HNL_mass}MeV'], weights=DetVar_BDT_dict[DetVar][\"weight\"], bins=20,range=[0,1.0],label=f'{DetVar}',\n",
    "                 lw=linewidth,histtype=\"step\")\n",
    "    plt.xlabel(f'BDT_output_{HNL_mass}MeV')\n",
    "    plt.legend()\n",
    "    plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca8068-554c-43c3-82aa-6b93f3e06fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DetVar_percentages_dict = {}\n",
    "if Load_DetVars == True:\n",
    "    cv_hist, cv_bins = np.histogram(\n",
    "        DetVar_BDT_dict[\"CV\"][f'BDT_output_{HNL_mass}MeV'],\n",
    "        range=[0,1.0],\n",
    "        bins=20,\n",
    "        weights=DetVar_BDT_dict[\"CV\"][\"weight\"])\n",
    "    bins_cent=(cv_bins[:-1]+cv_bins[1:])/2\n",
    "    for DetVar in DetVar_BDT_dict:\n",
    "        if DetVar == \"CV\":\n",
    "            continue\n",
    "        perc_list = []\n",
    "        detvar_hist, bins = np.histogram(\n",
    "        DetVar_BDT_dict[DetVar][f'BDT_output_{HNL_mass}MeV'],\n",
    "        range=[0,1.0],\n",
    "        bins=20,\n",
    "        weights=DetVar_BDT_dict[DetVar][\"weight\"])\n",
    "        for i in range(len(detvar_hist)):\n",
    "            frac = detvar_hist[i]/cv_hist[i]\n",
    "            frac_diff = frac - 1.0\n",
    "            perc_list.append(frac_diff*100)\n",
    "        DetVar_percentages_dict[DetVar] = perc_list\n",
    "        plt.hist(bins_cent,weights=DetVar_percentages_dict[DetVar], bins=20,range=[0,1.0],label=f'{DetVar}',\n",
    "                lw=linewidth,histtype=\"step\") #just 1 entry for each bin, then \"weight\" becomes what the percentage is (hacky way, could do something nicer)\n",
    "    plt.legend(loc='upper left',frameon=True)\n",
    "    plt.ylim([-150,150])\n",
    "    plt.xlabel(f'BDT Score {HNL_mass} MeV', fontsize=30)\n",
    "    plt.ylabel('% Difference', fontsize=30)\n",
    "    #plt.yscale()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f34d7-408f-40ce-9ca6-2d74dce1e6f1",
   "metadata": {},
   "source": [
    "## Checking variable correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a4680-42f1-4f9b-954d-1197dd26917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from Luis' code\n",
    "# for HNL_mass in HNL_masses:\n",
    "method = 'kendall'\n",
    "correlations = cleaned_signal_dict[100][bdt_vars].astype(np.float64).corr(method=method)\n",
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(correlations,vmin=-1,annot=False,square=True,cbar_kws={'label':method+' correlation'},cmap = 'RdBu_r')\n",
    "plt.title('Input Variable Correlations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a32c08-e4b9-4d6c-8037-4aa3bc074521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just looking at most correlated \n",
    "corr=cleaned_signal_dict[100][bdt_vars].corr()\n",
    "high_corr_var=np.where(corr>0.95)\n",
    "high_corr_var=[(corr.columns[x],corr.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\n",
    "high_corr_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7d7ce-cb79-4e2f-a35d-88717247cdf0",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057e26b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 200\n",
    "\n",
    "first_model = xgboost.Booster()\n",
    "first_model.load_model(f'bdts/{Run}_{HNL_mass}_MeV_My_variables_flattened_highest_E_1.json')\n",
    "\n",
    "plt.figure(figsize=(12,12),facecolor='white')\n",
    "\n",
    "a = xgboost.plot_importance(first_model,max_num_features=15,importance_type='gain')\n",
    "\n",
    "print(type(a))  \n",
    "\n",
    "#Owen's dropdupes from the plotting function\n",
    "# overlay_post_bdt=data_test_bkg.sort_values('BDT_output',ascending=False).drop_duplicates(subset=[\"run\",\"evt\",\"sub\"])\n",
    "# dirt_post_bdt=data_test_bkg.sort_values('BDT_output',ascending=False).drop_duplicates(subset=[\"run\",\"evt\",\"sub\"])\n",
    "# EXT_post_bdt=data_test_bkg.sort_values('BDT_output',ascending=False).drop_duplicates(subset=[\"run\",\"evt\",\"sub\"])\n",
    "# signal_post_bdt=data_test_bkg.sort_values('BDT_output',ascending=False).drop_duplicates(subset=[\"run\",\"evt\",\"sub\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039f659-41d1-45f9-8233-6f395c25012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_dict = first_model.get_score(importance_type='gain')\n",
    "\n",
    "value = 15\n",
    "highest_imp = []\n",
    "\n",
    "for k in imp_dict:\n",
    "    if imp_dict[k] > value:\n",
    "        highest_imp.append(k)\n",
    "    \n",
    "print(highest_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77f5798-7c92-4033-b797-5a1e667db56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting unweighted\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    \n",
    "    numdecays_signal = len(signal_samples_dict[HNL_mass])\n",
    "    #numdecays_bkg = len(Presel_overlay)\n",
    "    numdecays_bkg = len(Cleaned_Presel_overlay_highest_E)\n",
    "    plt.figure(figsize=(12,8),facecolor='white')\n",
    "    SF_sig = numdecays_signal/len(signal_test_dict[HNL_mass][f'BDT_output'])\n",
    "    SF_overlay = (numdecays_bkg)/len(data_test_bkg[f'BDT_output_{HNL_mass}MeV'])\n",
    "    print(len(signal_test_dict[HNL_mass][f'BDT_output'])*SF_sig)\n",
    "    print(len(data_test_bkg[f'BDT_output_{HNL_mass}MeV'])*SF_overlay)\n",
    "\n",
    "    plt.hist(data_test_bkg[f'BDT_output_{HNL_mass}MeV'],bins=20,range = [0.0,1.0], color='blue', label='Neutrino bkg', alpha=0.8)\n",
    "    plt.hist(signal_test_dict[HNL_mass][f'BDT_output'],bins=20,range = [0.0,1.0], color='red', label=f'HNL {HNL_mass} MeV', alpha=0.8)\n",
    "    plt.hist(Cleaned_Presel_dirt_highest_E[f'BDT_output_{HNL_mass}MeV'],bins=20,range = [0.0,1.0], color='orange', label='Dirt bkg', alpha=0.8)\n",
    "    plt.hist(Cleaned_Presel_EXT_highest_E[f'BDT_output_{HNL_mass}MeV'],bins=20,range = [0.0,1.0], color='green', label=f'EXT bkg', alpha=0.8)\n",
    "\n",
    "    plt.xlabel('BDT score', fontsize=30)\n",
    "    plt.ylabel('Entries', fontsize=30)\n",
    "    plt.rcParams.update({'font.size': 30})\n",
    "    plt.legend()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
