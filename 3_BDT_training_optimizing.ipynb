{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab81f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,string, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from platform import python_version\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import math\n",
    "from matplotlib.patches import Rectangle\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "import pickle\n",
    "\n",
    "import Utilities.Plotter as PT\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Variables_list as Variables\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print ('Success')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a3504-392d-4b23-a225-0db439fcb0f5",
   "metadata": {},
   "source": [
    "## Reading in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e41222-cb45-4107-a206-3b3653d23ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = {\"Run\":\"run1\", #The run number, so far either \"run1\" or \"run3\"\n",
    "          \"Load_single_file\":False, #This will override everything else, put the desired file in the \"single_file\" line\n",
    "          \"single_file\":10,\n",
    "          \"Load_standard\":True,\n",
    "          \"Load_DetVars\":False,\n",
    "          \"Only_keep_common_DetVar_evs\":True,\n",
    "          \"Load_data\":False,\n",
    "          \"FLATTEN\":True, #Have one row per reconstructed object in the analysis dataframe\n",
    "          \"only_presel\":False, #Create small files containing only variables necessary for pre-selection, for making pre-selection plots\n",
    "          \"EXT_in_training\":True,\n",
    "          \"Load_pi0_signal\":False} #Otherwise loads e+e- samples by default\n",
    "\n",
    "loc_pkls = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/my_vars/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d786fc-af51-48e5-88d4-d0ee5bcd2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_samples_dict = {}\n",
    "# end_string = \"_FINAL\"\n",
    "end_string = \"_full_Finished\"\n",
    "\n",
    "Presel_overlay = pd.read_pickle(loc_pkls+\"Preselected_overlay_\"+Params[\"Run\"]+f\"_flattened{end_string}.pkl\")\n",
    "\n",
    "if Params[\"Load_pi0_signal\"] == False:\n",
    "    if Params[\"Load_single_file\"] == True:\n",
    "        HNL_mass = Params[\"single_file\"]\n",
    "        Presel_signal = pd.read_pickle(loc_pkls+f\"Preselected_{HNL_mass}_\"+Params[\"Run\"]+f\"_flattened{end_string}.pkl\")\n",
    "        signal_samples_dict[HNL_mass] = Presel_signal\n",
    "    else:\n",
    "        for HNL_mass in Constants.HNL_ee_samples_names:\n",
    "            Presel_signal = pd.read_pickle(loc_pkls+f\"Preselected_{HNL_mass}_\"+Params[\"Run\"]+f\"_flattened{end_string}.pkl\")\n",
    "            signal_samples_dict[HNL_mass] = Presel_signal\n",
    "    \n",
    "if Params[\"Load_pi0_signal\"] == True:\n",
    "    for HNL_mass in Constants.HNL_mass_pi0_samples_names:\n",
    "        Presel_signal = pd.read_pickle(loc_pkls+f\"pi0_selection/Preselected_{HNL_mass}_\"+Params[\"Run\"]+f\"_flattened{end_string}.pkl\")\n",
    "        signal_samples_dict[HNL_mass] = Presel_signal\n",
    "    \n",
    "if Params[\"EXT_in_training\"] == True:\n",
    "    Presel_EXT = pd.read_pickle(loc_pkls+\"Preselected_beamoff_\"+Params[\"Run\"]+f\"_flattened{end_string}.pkl\")\n",
    "\n",
    "print(Presel_overlay.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb76888-7925-407b-9a82-b6a997a12e5e",
   "metadata": {},
   "source": [
    "## Splitting into test and training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1638c-9af1-4f64-aed1-16affdd7aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ultimate_feature_list = ['n_pfps', 'n_tracks', 'shr_theta_v', 'shr_phi_v', 'shr_pz_v', 'shrclusdir2', 'shr_energy_tot',\n",
    "#                          'trk_theta_v', 'trk_phi_v','trk_dir_z_v', 'trk_energy', 'trk_energy_tot', 'trk_calo_energy_u_v', 'trk_score_v',\n",
    "#                          'pfnplanehits_U', 'pfnplanehits_V', 'pfnplanehits_Y', 'NeutrinoEnergy2']#, 'nu_flashmatch_score']\n",
    "\n",
    "ultimate_feature_list = ['n_pfps', 'n_tracks', 'shr_theta_v', 'shr_phi_v', 'shr_pz_v', 'shrclusdir2', 'shr_energy_tot',\n",
    "                         'trk_theta_v', 'trk_phi_v','trk_dir_z_v', 'trk_energy', 'trk_energy_tot', 'trk_score_v',\n",
    "                         'pfnplanehits_U', 'pfnplanehits_V', 'pfnplanehits_Y', 'NeutrinoEnergy2']#, 'nu_flashmatch_score']\n",
    "\n",
    "ultimate_feature_list += ['shr_tkfit_dedx_max', 'topological_score']\n",
    "if Params[\"Run\"]==\"run3\":\n",
    "    ultimate_feature_list += ['nu_flashmatch_score']\n",
    "print(f\"Number of features is {len(ultimate_feature_list)}\")\n",
    "\n",
    "bdt_vars = ultimate_feature_list #This is using just the most important variables list\n",
    "\n",
    "new_value = -9999.0 #This tells XGB what number refers to missing data for all variables\n",
    "\n",
    "signal_train_dict = {}\n",
    "signal_test_dict = {}\n",
    "labels_dict = {} \n",
    "\n",
    "train_vs_test_fraction = 0.7 #This is the fraction used for training\n",
    "\n",
    "print(f\"Total length of overlay file is {len(Presel_overlay)}\")\n",
    "\n",
    "overlay_train = Presel_overlay[:int(len(Presel_overlay)*train_vs_test_fraction)]\n",
    "overlay_test = Presel_overlay[int(len(Presel_overlay)*train_vs_test_fraction):]\n",
    "\n",
    "print(f\"Total length of overlay TRAIN file is {len(overlay_train)}\")\n",
    "print(f\"Total length of overlay TEST file is {len(overlay_test)}\")\n",
    "\n",
    "# overlay_train = Presel_overlay[int(len(Presel_overlay)*train_vs_test_fraction):] #OLD WRONG WAY, i.e 70% test\n",
    "# overlay_test = Presel_overlay[:int(len(Presel_overlay)*train_vs_test_fraction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f2f4b4-2c9c-4987-9919-904f2d9d36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Params[\"Load_pi0_signal\"] == False: BDT_name = f\"ee{end_string}\"\n",
    "# if Params[\"Load_pi0_signal\"] == True: BDT_name = f\"pi0{end_string}\"\n",
    "if Params[\"Load_pi0_signal\"] == False: BDT_name = f\"{end_string}\"\n",
    "if Params[\"Load_pi0_signal\"] == True: BDT_name = f\"{end_string}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5bc6b1-715e-4b69-8dd2-f54a2876c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_samples_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0954e49e-bc46-4310-aa18-f63612b60563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_samples(Presel_overlay, signal_samples_dict, Presel_EXT, overlay_train_frac=0.7, signal_train_frac=0.7, EXT_train_frac=0.3):\n",
    "    \"\"\"\n",
    "    Input pkl files and train_vs_test fractions.\n",
    "    Return the split samples.\n",
    "    \"\"\"\n",
    "    overlay_train = Presel_overlay[:int(len(Presel_overlay)*overlay_train_frac)]\n",
    "    overlay_test = Presel_overlay[int(len(Presel_overlay)*overlay_train_frac):]\n",
    "    \n",
    "    EXT_train = Presel_EXT[:int(len(Presel_EXT)*EXT_train_frac)]\n",
    "    EXT_test = Presel_EXT[int(len(Presel_EXT)*EXT_train_frac):]\n",
    "    \n",
    "    signal_train_dict, signal_test_dict = {}, {}\n",
    "    \n",
    "    print(f\"Length overlay train {len(overlay_train)}\")\n",
    "    print(f\"Length overlay test {len(overlay_test)}\")\n",
    "    print(f\"Length EXT train {len(EXT_train)}\")\n",
    "    print(f\"Length EXT test {len(EXT_test)}\")\n",
    "    \n",
    "    for HNL_mass in signal_samples_dict:\n",
    "        signal_train_dict[HNL_mass] = signal_samples_dict[HNL_mass][:int(len(signal_samples_dict[HNL_mass])*signal_train_frac)]\n",
    "        signal_test_dict[HNL_mass] = signal_samples_dict[HNL_mass][int(len(signal_samples_dict[HNL_mass])*signal_train_frac):]\n",
    "        \n",
    "        print(f\"Length {HNL_mass} train {len(signal_train_dict[HNL_mass])}\")\n",
    "        print(f\"Length {HNL_mass} test {len(signal_test_dict[HNL_mass])}\")\n",
    "        \n",
    "    return overlay_train, overlay_test, EXT_train, EXT_test, signal_train_dict, signal_test_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9873ba6f-b41a-4eb2-9343-c0dfd7032455",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay_train, overlay_test, EXT_train, EXT_test, signal_train_dict, signal_test_dict = Split_samples(Presel_overlay, signal_samples_dict, Presel_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8255bed-472b-414c-be13-c9e3c35805a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_test_pkls(Params, loc_pkls, save_str, overlay_test, signal_test_dict, EXT_test = []):\n",
    "    \"\"\"\n",
    "    Input Params, save_str, overlay test df, signal_test_dict, and EXT test sample if using.\n",
    "    Saves the dataframes as .pkl files.\n",
    "    \"\"\"\n",
    "    start_str = loc_pkls\n",
    "    if Params[\"Load_pi0_signal\"] == False: start_str+=\"BDT_Test_dfs/\"\n",
    "    if Params[\"Load_pi0_signal\"] == True: start_str+=\"BDT_Test_dfs/pi0_selection/\"\n",
    "    \n",
    "    print(f\"Pickling overlay test sample\")\n",
    "    overlay_test.to_pickle(start_str+\"Test_overlay_\"+Params[\"Run\"]+f\"_flattened{save_str}.pkl\")\n",
    "    \n",
    "    for HNL_mass in signal_test_dict:\n",
    "        print(f\"Pickling {HNL_mass} test sample\")\n",
    "        signal_test_dict[HNL_mass].to_pickle(start_str+f\"Test_{HNL_mass}_\"+Params[\"Run\"]+f\"_flattened{save_str}.pkl\")\n",
    "        \n",
    "    if Params[\"EXT_in_training\"] == True: \n",
    "        print(f\"Pickling beamoff test sample\")\n",
    "        EXT_test.to_pickle(start_str+\"Test_beamoff_\"+Params[\"Run\"]+f\"_flattened{save_str}.pkl\")\n",
    "        \n",
    "    if Params[\"EXT_in_training\"] == False: print(\"Not saving beamoff test, as not using in training.\") \n",
    "        \n",
    "\n",
    "def Make_train_labels_and_dicts(Params, bdt_vars, overlay_train, EXT_train, signal_train_dict):\n",
    "    \"\"\"\n",
    "    Input Params, bdt variables, training samples.\n",
    "    Return dicts of labels indicating if the event is signal (1) or bkg (0).\n",
    "    \"\"\"\n",
    "    combined_dict, labels_dict = {}, {}\n",
    "    \n",
    "    for HNL_mass in signal_train_dict:\n",
    "        # labels_dict[HNL_mass] = [1]*len(signal_train_dict[HNL_mass][bdt_vars]) + [0]*len(overlay_train[bdt_vars])\n",
    "        combined_dict[HNL_mass] = pd.concat([signal_train_dict[HNL_mass][bdt_vars], overlay_train[bdt_vars]])\n",
    "        labels_dict[HNL_mass] = [1]*len(signal_train_dict[HNL_mass]) + [0]*len(overlay_train)\n",
    "        \n",
    "    bkg_train = overlay_train.copy()\n",
    "        \n",
    "    if Params[\"EXT_in_training\"] == True:\n",
    "        for HNL_mass in signal_train_dict:\n",
    "            # labels_dict[HNL_mass] = labels_dict[HNL_mass] + [0]*len(EXT_train[bdt_vars])\n",
    "            combined_dict[HNL_mass] = pd.concat([combined_dict[HNL_mass][bdt_vars], EXT_train[bdt_vars]])\n",
    "            labels_dict[HNL_mass] = labels_dict[HNL_mass] + [0]*len(EXT_train)\n",
    "        bkg_train=pd.concat([overlay_train, EXT_train])\n",
    "    \n",
    "    return combined_dict, labels_dict, bkg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c2490-bb46-49dc-a1be-a3f0853f7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Save_test_pkls(Params, loc_pkls, \"_full_Finished\", overlay_test, signal_test_dict, EXT_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f3f899-8eef-408e-b520-a2483ec89a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict, labels_dict, bkg_train = Make_train_labels_and_dicts(Params, bdt_vars, overlay_train, EXT_train, signal_train_dict)\n",
    "\n",
    "if Params[\"EXT_in_training\"] == True:\n",
    "    bkg_test = pd.concat([overlay_test,EXT_test])\n",
    "else: bkg_test = overlay_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fc7ce8-8cb0-469d-ab0e-815c25a9aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_for_xgb(Params, bdt_vars, combined_train_dict, signal_train_dict, bkg_train,  signal_test_dict, bkg_test,\n",
    "                    labels_train_dict, missing=-9999.0):\n",
    "    \"\"\"\n",
    "    Input training dict, test dict, labels and xgboost parameters.\n",
    "    Returns the DMatrix forms of the dataframes for training.\n",
    "    \"\"\"\n",
    "    xgb_train_bkg = xgboost.DMatrix(bkg_train[bdt_vars],label=[0]*len(bkg_train[bdt_vars]), missing=missing, feature_names=bdt_vars)\n",
    "    xgb_test_bkg = xgboost.DMatrix(bkg_test[bdt_vars], label=[0]*len(bkg_test[bdt_vars]), missing=missing, feature_names=bdt_vars)\n",
    "    \n",
    "    xgb_train_dict, xgb_sig_test_dict, xgb_sig_train_dict = {}, {}, {}\n",
    "    \n",
    "    for HNL_mass in signal_train_dict:\n",
    "        xgb_train_dict[HNL_mass] = xgboost.DMatrix(combined_train_dict[HNL_mass][bdt_vars], label=labels_dict[HNL_mass], \n",
    "                                                   missing=missing, feature_names=bdt_vars)\n",
    "        \n",
    "        xgb_sig_test_dict[HNL_mass] = xgboost.DMatrix(signal_test_dict[HNL_mass][bdt_vars], label=[1]*len(signal_test_dict[HNL_mass][bdt_vars]), \n",
    "                                                      missing=missing, feature_names=bdt_vars)\n",
    "        \n",
    "        xgb_sig_train_dict[HNL_mass] = xgboost.DMatrix(signal_train_dict[HNL_mass][bdt_vars],label=[1]*len(signal_train_dict[HNL_mass][bdt_vars]),\n",
    "                                                  missing=missing, feature_names=bdt_vars)\n",
    "        \n",
    "        \n",
    "    return xgb_train_dict, xgb_sig_train_dict, xgb_sig_test_dict, xgb_train_bkg, xgb_test_bkg\n",
    "        \n",
    "def Train_BDTs(Params, bdt_vars, BDT_name, xgb_train_dict, xgb_sig_test_dict, xgb_test_bkg, xgb_param, progress, num_round = 150, missing=-9999.0):\n",
    "    \"\"\"\n",
    "    Input training dict, test dict, labels and xgboost parameters.\n",
    "    Saves the BDT models as .json files.\n",
    "    \"\"\"\n",
    "    watchlist = {}\n",
    "    for HNL_mass in xgb_train_dict:\n",
    "        watchlist[HNL_mass] = [(xgb_train_dict[HNL_mass], 'train'), (xgb_sig_test_dict[HNL_mass], 'test_sig'), (xgb_test_bkg,'test_bkg')]\n",
    "        print(f\"Training {HNL_mass} BDT\" + \"\\n\")\n",
    "        bdt = xgboost.train(xgb_param, xgb_train_dict[HNL_mass], num_round, watchlist[HNL_mass], evals_result=progress, verbose_eval=False)\n",
    "\n",
    "        if Params[\"Load_pi0_signal\"] == False:\n",
    "            bdt.save_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}{BDT_name}.json\")\n",
    "        if Params[\"Load_pi0_signal\"] == True:\n",
    "            bdt.save_model(\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}{BDT_name}.json\")\n",
    "            \n",
    "    return watchlist\n",
    "        \n",
    "def Test_vs_train_plot():\n",
    "    results_sig = bdt.predict(xgb_sig_test_dict[HNL_mass])\n",
    "    results_bkg = bdt.predict(xgb_test_bkg)\n",
    "    \n",
    "    train_results_sig = bdt.predict(xgb_sig_train_dict[HNL_mass])\n",
    "    train_results_bkg = bdt.predict(xgb_bkg_train_dict[HNL_mass])\n",
    "    \n",
    "    test_results_sig_dict.update({HNL_mass:results_sig})\n",
    "    test_results_bkg_dict.update({HNL_mass:results_bkg})\n",
    "    \n",
    "    train_results_sig_dict.update({HNL_mass:train_results_sig})\n",
    "    train_results_bkg_dict.update({HNL_mass:train_results_bkg})\n",
    "    \n",
    "    print(\"write this\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159feff4-fafc-4770-b656-26e86f0c7d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_dict, xgb_sig_train_dict, xgb_sig_test_dict, xgb_train_bkg, xgb_test_bkg = Prepare_for_xgb(Params, bdt_vars, combined_dict, \n",
    "                                                                                                     signal_train_dict, bkg_train,  signal_test_dict, \n",
    "                                                                                                     bkg_test, labels_dict, missing=-9999.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689a4ae-9dcf-4d9c-9963-0501e234ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param = {'booster': 'dart',\n",
    "        'max_depth':6,\n",
    "        'eta':0.3,\n",
    "        'objective':'binary:logistic',\n",
    "#        'eval_metric':'auc', \n",
    "#        'subsample':0.5,\n",
    "        'tree_method':'hist',\n",
    "#        'scale_pos_weight': float(len(data_bkg_train))/float(len(data_sig_train)),\n",
    "        'rate_drop': 0.1,\n",
    "        'skip_drop': 0.5 }\n",
    "progress = dict()\n",
    "\n",
    "watchlist = Train_BDTs(Params, bdt_vars, BDT_name, xgb_train_dict, xgb_sig_test_dict, xgb_test_bkg, xgb_param, progress, num_round = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb63ea-beab-4141-9ec1-01331afd160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8dfcb7-34ab-4420-adb5-14410c9f717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_files = True\n",
    "\n",
    "save_str = \"_full_Finished\"\n",
    "\n",
    "# overlay_test.to_pickle(loc_pkls+\"BDT_Test_dfs/Test_overlay_\"+Params[\"Run\"]+f\"_flattened{end_string}.pkl\")\n",
    "if Params[\"Load_pi0_signal\"] == False:\n",
    "    overlay_test.to_pickle(loc_pkls+\"BDT_Test_dfs/Test_overlay_\"+Params[\"Run\"]+f\"_flattened{save_str}.pkl\")\n",
    "    # overlay_test.to_pickle(loc_pkls+\"BDT_Test_dfs/Test_overlay_\"+Params[\"Run\"]+\"_my_vars_flattened_ultimate.pkl\")\n",
    "    for HNL_mass in signal_samples_dict:\n",
    "        signal_train_dict[HNL_mass] = signal_samples_dict[HNL_mass][:int(len(signal_samples_dict[HNL_mass])*train_vs_test_fraction)]\n",
    "        signal_test_dict[HNL_mass] = signal_samples_dict[HNL_mass][int(len(signal_samples_dict[HNL_mass])*train_vs_test_fraction):]\n",
    "        if pickle_files == True:\n",
    "            print(f\"Pickling {HNL_mass} HNL test sample\")\n",
    "            # signal_test_dict[HNL_mass].to_pickle(loc_pkls+f\"BDT_Test_dfs/Test_signal_{HNL_mass}_\"+Params[\"Run\"]+f\"{end_string}.pkl\")\n",
    "            signal_test_dict[HNL_mass].to_pickle(loc_pkls+f\"BDT_Test_dfs/Test_{HNL_mass}_\"+Params[\"Run\"]+f\"_flattened{save_str}.pkl\")\n",
    "    \n",
    "        labels_dict[HNL_mass] = [1]*len(signal_train_dict[HNL_mass][bdt_vars]) + [0]*len(overlay_train[bdt_vars])\n",
    "        \n",
    "if Params[\"Load_pi0_signal\"] == True:\n",
    "    overlay_test.to_pickle(loc_pkls+\"BDT_Test_dfs/pi0_selection/Test_overlay_\"+Params[\"Run\"]+f\"_flattened{end_string}.pkl\")\n",
    "\n",
    "    for HNL_mass in signal_samples_dict:\n",
    "        signal_train_dict[HNL_mass] = signal_samples_dict[HNL_mass][:int(len(signal_samples_dict[HNL_mass])*train_vs_test_fraction)]\n",
    "        signal_test_dict[HNL_mass] = signal_samples_dict[HNL_mass][int(len(signal_samples_dict[HNL_mass])*train_vs_test_fraction):]\n",
    "        if pickle_files == True:\n",
    "            print(f\"Pickling {HNL_mass} HNL pi0 test sample\")\n",
    "            # signal_test_dict[HNL_mass].to_pickle(loc_pkls+f\"BDT_Test_dfs/pi0_selection/Test_{HNL_mass}_\"+\n",
    "            #                                      Params[\"Run\"]+f\"_flattened{end_string}.pkl\")\n",
    "            signal_test_dict[HNL_mass].to_pickle(loc_pkls+f\"BDT_Test_dfs/Test_{HNL_mass}_\"+\n",
    "                                                 Params[\"Run\"]+f\"_flattened{save_str}.pkl\")\n",
    "    \n",
    "        labels_dict[HNL_mass] = [1]*len(signal_train_dict[HNL_mass][bdt_vars]) + [0]*len(overlay_train[bdt_vars])\n",
    "    \n",
    "if Params[\"EXT_in_training\"] == True:\n",
    "    frac_EXT = 0.1\n",
    "    EXT_train = Presel_EXT[:int(len(Presel_EXT)*frac_EXT)]\n",
    "    EXT_test = Presel_EXT[int(len(Presel_EXT)*frac_EXT):]\n",
    "    print(\"Number of EXT to train: \" + str(len(EXT_train)))\n",
    "    overlay_plus_EXT = pd.concat([overlay_train[bdt_vars],EXT_train[bdt_vars]])\n",
    "    for HNL_mass in signal_samples_dict:\n",
    "        labels_dict[HNL_mass] = labels_dict[HNL_mass] + [0]*len(EXT_train[bdt_vars])\n",
    "    \n",
    "    if pickle_files == True:\n",
    "        EXT_test.to_pickle(loc_pkls+f\"BDT_Test_dfs/Test_beamoff_\"+Params[\"Run\"]+f\"_flattened{save_str}.pkl\")\n",
    "\n",
    "    print(f\"Total length of beamoff TRAIN file is {len(EXT_train)}\")\n",
    "    print(f\"Total length of beamoff TEST file is {len(EXT_test)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e387da2f-dfb1-4433-a85a-5b98d40611d3",
   "metadata": {},
   "source": [
    "## BDT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f0a87a-aad9-4ec4-98ba-517bfe7b7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_dict = {}\n",
    "xgb_test_dict = {}\n",
    "\n",
    "xgb_sig_train_dict = {}\n",
    "xgb_bkg_train_dict = {}\n",
    "\n",
    "xgb_test_bkg = xgboost.DMatrix(overlay_test[bdt_vars], label=[0]*len(overlay_test[bdt_vars]), missing=new_value, feature_names=bdt_vars)\n",
    "\n",
    "xgb_param = {'booster': 'dart',\n",
    "        'max_depth':6,\n",
    "        'eta':0.3,\n",
    "        'objective':'binary:logistic',\n",
    "#        'eval_metric':'auc', \n",
    "#        'subsample':0.5,\n",
    "        'tree_method':'hist',\n",
    "#        'scale_pos_weight': float(len(data_bkg_train))/float(len(data_sig_train)),\n",
    "        'rate_drop': 0.1,\n",
    "        'skip_drop': 0.5 }\n",
    "num_round = 150\n",
    "progress = dict()\n",
    "\n",
    "for HNL_mass in signal_train_dict:\n",
    "    if Params[\"EXT_in_training\"] == False:\n",
    "        xgb_train_dict[HNL_mass] = xgboost.DMatrix(pd.concat([signal_train_dict[HNL_mass][bdt_vars], #This is both signal and bkg combined into one\n",
    "                                                               overlay_train[bdt_vars]]), \n",
    "                                               label=labels_dict[HNL_mass], \n",
    "                                                    missing=new_value, feature_names=bdt_vars)\n",
    "    if Params[\"EXT_in_training\"] == True:\n",
    "        xgb_train_dict[HNL_mass] = xgboost.DMatrix(pd.concat([signal_train_dict[HNL_mass][bdt_vars], #This is both signal and bkg combined into one\n",
    "                                                               overlay_plus_EXT[bdt_vars]]), \n",
    "                                               label=labels_dict[HNL_mass], \n",
    "                                                    missing=new_value, feature_names=bdt_vars)\n",
    "    xgb_test_dict[HNL_mass] = xgboost.DMatrix(signal_test_dict[HNL_mass][bdt_vars], label=[1]*len(signal_test_dict[HNL_mass][bdt_vars]), #Just signal test\n",
    "                                              missing=new_value, feature_names=bdt_vars)\n",
    "    \n",
    "    xgb_sig_train_dict[HNL_mass] = xgboost.DMatrix(signal_train_dict[HNL_mass][bdt_vars],label=[1]*len(signal_train_dict[HNL_mass][bdt_vars]), #Signal training\n",
    "                                                  missing=new_value, feature_names=bdt_vars)\n",
    "    xgb_bkg_train_dict[HNL_mass] = xgboost.DMatrix(overlay_train[bdt_vars],label=[0]*len(overlay_train[bdt_vars]), #Just background training\n",
    "                                                  missing=new_value, feature_names=bdt_vars)\n",
    "\n",
    "    #watchlist so that you can monitor the performance of the training by iterations\n",
    "    watchlist = [(xgb_train_dict[HNL_mass], 'train'), (xgb_test_dict[HNL_mass], 'test_sig'), (xgb_test_bkg,'test_bkg')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8965e-42ff-466b-9971-24bb5e0cb0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Params[\"Load_pi0_signal\"] == False: BDT_name = \"ee_FINAL\"\n",
    "# if Params[\"Load_pi0_signal\"] == True: BDT_name = \"pi0_FINAL\"\n",
    "\n",
    "for HNL_mass in signal_train_dict:\n",
    "    print(f\"Training {HNL_mass} BDT\" + \"\\n\")\n",
    "    bdt = xgboost.train(xgb_param, xgb_train_dict[HNL_mass], num_round, watchlist, evals_result=progress, verbose_eval=False)\n",
    "    # doesnt like watchlist/eval_result if using AOC\n",
    "    # save model so you can load it later\n",
    "    if Params[\"Load_pi0_signal\"] == False:\n",
    "        bdt.save_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}{BDT_name}.json\")\n",
    "    if Params[\"Load_pi0_signal\"] == True:\n",
    "        bdt.save_model(\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}{BDT_name}.json\")\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ed366-ed68-4d56-b969-0887b09bc6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BDT_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa754b8-6edb-43bf-8ed4-5864a4c4a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = bdt_vars\n",
    "\n",
    "# BDT_name = \"New_20_variables_FIXED\"\n",
    "if Params[\"Load_pi0_signal\"] == False:\n",
    "    with open(f\"bdts/input_vars/{BDT_name}_\"+Params[\"Run\"], \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(var_list, fp)\n",
    "elif Params[\"Load_pi0_signal\"] == True:\n",
    "    with open(f\"bdts/pi0_selection/input_vars/{BDT_name}_\"+Params[\"Run\"], \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(var_list, fp)\n",
    "\n",
    "# with open(\"bdts/input_vars/\"+BDT_name, \"rb\") as fp:   # Unpickling\n",
    "#     b = pickle.load(fp)\n",
    "    \n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5a815-d856-4b76-86af-1abd2b38f6d7",
   "metadata": {},
   "source": [
    "# Finished Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293380da-31c0-4d08-b5f5-7ac71386a1ef",
   "metadata": {},
   "source": [
    "## Checking variable correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ce0d0-4404-4a8c-9acd-642687cf8462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bdt_vars = feature_names\n",
    "HNL_mass = \"150_pi0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad1892-8b6b-48f2-af3b-02622353d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from Luis' code\n",
    "# for HNL_mass in HNL_masses:\n",
    "method = 'kendall'\n",
    "correlations = signal_samples_dict[HNL_mass][bdt_vars].astype(np.float64).corr(method=method)\n",
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(correlations,vmin=-1,annot=False,square=True,cbar_kws={'label':method+' correlation'},cmap = 'RdBu_r')\n",
    "plt.title('Input Variable Correlations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd62bb6-b92f-448f-8514-9d5243a9aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just looking at most correlated \n",
    "corr=signal_samples_dict[HNL_mass][bdt_vars].corr()\n",
    "high_corr_var=np.where(corr>0.999)\n",
    "high_corr_var=[(corr.columns[x],corr.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\n",
    "#high_corr_var\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f50740e-dd5e-4cb7-ac69-396d483a0428",
   "metadata": {},
   "source": [
    "## Looking at feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608634a-1eb0-4ff2-a83a-cfdd282d7e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"150_pi0\"\n",
    "print(a.split(\"_\")[1])\n",
    "\n",
    "print(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351d3da-76cc-4079-9036-548307e99353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make an importances dict   \n",
    "def Top_N_vars(bdt_model, N_vars):\n",
    "    importance = bdt.get_score(importance_type=\"gain\")\n",
    "    # for key in importance.keys():\n",
    "    #     importance[key] = round(importance[key],1)\n",
    "    sorted_importance = dict(sorted(importance.items(), key=lambda item: item[1]))\n",
    "    sorted_importance_list = list(sorted_importance.values())\n",
    "    sorted_importance_keys= list(sorted_importance.keys())\n",
    "    top_N = sorted_importance_keys[-N_vars:]\n",
    "    \n",
    "    return top_N\n",
    "\n",
    "def Sorted_importance(bdt_model):\n",
    "    importance = bdt.get_score(importance_type=\"gain\")\n",
    "    return importance\n",
    "\n",
    "top_N_dict = {}\n",
    "importance_dict = {}\n",
    "list_of_lists = []\n",
    "# BDT_name = \"ee_Finished\"\n",
    "# BDT_name = \"pi0\"\n",
    "\n",
    "# if Params[\"Load_pi0_signal\"] == False: \n",
    "#     sample_names = Constants.HNL_ee_samples_names\n",
    "#     loc = \"bdts/\"\n",
    "# if Params[\"Load_pi0_signal\"] == True: \n",
    "#     sample_names = Constants.HNL_mass_pi0_samples_names #I should load BOTH\n",
    "#     loc = \"bdts/pi0_selection/\"\n",
    "sample_names = Constants.HNL_ee_samples_names + Constants.HNL_mass_pi0_samples_names\n",
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "for HNL_mass in sample_names:\n",
    "    bdt = xgboost.Booster()\n",
    "    if HNL_mass.split(\"_\")[1] == \"pi0\": loc = \"bdts/pi0_selection/\"\n",
    "    if HNL_mass.split(\"_\")[1] == \"ee\": loc = \"bdts/\"\n",
    "    # bdt.load_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_ultimate.json\")\n",
    "    # bdt.load_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_{BDT_name}.json\")\n",
    "    bdt.load_model(loc+Params[\"Run\"]+f\"_{HNL_mass}{BDT_name}.json\")\n",
    "    # print(\"Number of entries in top20 is \" + str(len(Top_N_vars(bdt, 20))))\n",
    "    # top_N_dict[HNL_mass] = Top_N_vars(bdt, 20)\n",
    "    top_N = Top_N_vars(bdt, 50)\n",
    "    top_N_dict[HNL_mass] = Top_N_vars(bdt, 50)\n",
    "    list_of_lists.append(Top_N_vars(bdt, 50))\n",
    "    importance_dict[HNL_mass] = Sorted_importance(bdt)\n",
    "    \n",
    "elements_in_all = list(set.intersection(*map(set, list_of_lists)))\n",
    "print(len(elements_in_all))\n",
    "print(elements_in_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be7c30-7f83-4734-b9d7-98bab2c7556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "highets_imps = ['n_showers', 'NeutrinoEnergy2', 'secondshower_Y_charge', 'trk_chipr_best', 'trk_energy_hits_tot', 'shr_energy_tot', \n",
    "                'contained_sps_ratio', 'pfnplanehits_Y', 'shrclusdir2', 'trk_dir_y_v', 'SliceCaloEnergy2', 'trk_theta_v', 'trk_start_x_v', \n",
    "                'pi0_radlen2', 'pfnplanehits_V', 'trk_energy', 'trk_score_v', 'shrclusdir0', 'trk_bragg_mip_v', 'shr_py_v', \n",
    "                'nu_flashmatch_score', 'n_pfps', 'CosmicIPAll3D', 'pi0_dir2_z']\n",
    "\n",
    "ultimate_feature_list = ['n_pfps', 'n_tracks', 'shr_theta_v', 'shr_phi_v', 'shr_pz_v', 'shrclusdir2', 'shr_energy_tot',\n",
    "                         'trk_theta_v', 'trk_phi_v','trk_dir_z_v', 'trk_energy', 'trk_energy_tot', 'trk_calo_energy_u_v', 'trk_score_v',\n",
    "                         'pfnplanehits_U', 'pfnplanehits_V', 'pfnplanehits_Y', 'NeutrinoEnergy2', 'nu_flashmatch_score']\n",
    "\n",
    "elements_in_all = list(set.intersection(*map(set, [highets_imps,ultimate_feature_list])))\n",
    "print(len(elements_in_all))\n",
    "print(elements_in_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd2dc0-fc9a-4d54-b256-3785ba5e6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(importance_dict[245].keys())\n",
    "print(len(importance_dict[245].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de628a-83ec-4505-8147-3a1e92e5b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for HNL_mass in Constants.HNL_mass_samples:\n",
    "savefig = True\n",
    "plt.figure(figsize=[20,8])\n",
    "if Params[\"Load_pi0_signal\"] == True: \n",
    "    colours = {\"150_pi0\":\"coral\", \"200_pi0\":\"cornflowerblue\", \"245_pi0\":\"olivedrab\"}\n",
    "    smaller_samples = [\"150_pi0\", \"200_pi0\", \"245_pi0\"]\n",
    "if Params[\"Load_pi0_signal\"] == False: \n",
    "    colours = {\"10_ee\":\"coral\", \"100_ee\":\"cornflowerblue\", \"150_ee\":\"olivedrab\"}\n",
    "    smaller_samples = [\"10_ee\", \"100_ee\", \"150_ee\"]\n",
    "    \n",
    "smaller_samples = [\"10_ee\", \"100_ee\", \"150_ee\",\"150_pi0\", \"200_pi0\", \"245_pi0\"]\n",
    "color_cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "for i, HNL_mass in enumerate(smaller_samples):\n",
    "    plt.bar(importance_dict[HNL_mass].keys(),importance_dict[HNL_mass].values(), label=f\"{HNL_mass} model\", \n",
    "            fill=False,linewidth=3, edgecolor=color_cycle[i])\n",
    "    # plt.bar(importance_dict[HNL_mass].keys(),importance_dict[HNL_mass].values(), label=f\"{HNL_mass}MeV model\", \n",
    "    #         fill=False,linewidth=3, edgecolor=colours[HNL_mass], color=colours[HNL_mass])\n",
    "# plt.bar(importance_dict[245].keys(),importance_dict[245].values())\n",
    "plt.xticks(np.array(range(0, len(importance_dict[HNL_mass].keys()))),importance_dict[HNL_mass].keys(),rotation=80)\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.legend(fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "if savefig==True:\n",
    "    plt.savefig(\"plots/BDT_output/variable_importance/\"+ Params[\"Run\"]+f\"_importances{BDT_name}.pdf\")\n",
    "    plt.savefig(\"plots/BDT_output/variable_importance/\"+ Params[\"Run\"]+f\"_importances{BDT_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979aa2f-7260-47da-a99c-766ceabfc690",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20,8])\n",
    "colours = {150:\"coral\", 200:\"cornflowerblue\", 245:\"olivedrab\"}\n",
    "for HNL_mass in [245]:\n",
    "    plt.bar(importance_dict[HNL_mass].keys(),importance_dict[HNL_mass].values(), label=f\"{HNL_mass}MeV model\", \n",
    "            fill=False,linewidth=3, edgecolor=colours[HNL_mass], color=colours[HNL_mass])\n",
    "# plt.bar(importance_dict[245].keys(),importance_dict[245].values())\n",
    "plt.xticks(np.array(range(0, len(importance_dict[HNL_mass].keys()))),importance_dict[HNL_mass].keys(),rotation=80)\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.legend(fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"plots/BDT_output/variable_importance/\"+ Params[\"Run\"]+f\"_importances_{BDT_name}.pdf\")\n",
    "# plt.savefig(\"plots/BDT_output/variable_importance/\"+ Params[\"Run\"]+f\"_importances_{BDT_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea472ee-95b7-425f-9eaa-845b4da3425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[20,20])\n",
    "ax1 = fig.add_subplot(projection='3d')\n",
    "\n",
    "mass_list = [\"2MeV\", \"100MeV\", \"245MeV\"]\n",
    "x = importance_dict[2].keys()\n",
    "y = mass_list\n",
    "data = np.array([list(importance_dict[2].values()),\n",
    "                 list(importance_dict[100].values()),\n",
    "                 list(importance_dict[245].values())])\n",
    "\n",
    "numOfCols = len(x)\n",
    "numOfRows = len(y)\n",
    "\n",
    "xpos = np.arange(0, numOfCols, 1)\n",
    "ypos = np.arange(0, numOfRows, 1)\n",
    "xpos, ypos = np.meshgrid(xpos + 0.5, ypos + 0.5)\n",
    "\n",
    "xpos = xpos.flatten()\n",
    "ypos = ypos.flatten()\n",
    "zpos = np.zeros(numOfCols * numOfRows)\n",
    "\n",
    "dx = np.ones(numOfRows * numOfCols) * 0.5\n",
    "dy = np.ones(numOfCols * numOfRows) * 0.5\n",
    "dz = data.flatten()\n",
    "\n",
    "mass_list_labels = [\" \",\" \", \"2MeV\",\" \", \"100MeV\",\" \", \"245MeV\"]\n",
    "\n",
    "ax1.bar3d(xpos, ypos, zpos, dx, dy, dz)\n",
    "ax1.set_xticklabels(list(importance_dict[2].keys()),rotation=45)\n",
    "# ax1.set_xticklabels(list(importance_dict[2].keys()))\n",
    "ax1.set_yticklabels(mass_list_labels)\n",
    "\n",
    "ax1.set_xlabel('Variable')\n",
    "ax1.set_ylabel('Model')\n",
    "ax1.set_zlabel('Importance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380877bb-f8ed-4648-88fd-3e4eec8c6c1d",
   "metadata": {},
   "source": [
    "# Finished code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28366e0-1cea-490c-840e-bcccdfd88413",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966bb616-3075-4722-bbad-57a1e9afb281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirt_matrix = xgboost.DMatrix(dirt_BDT[bdt_vars])\n",
    "# EXT_matrix = xgboost.DMatrix(EXT_BDT[bdt_vars])\n",
    "\n",
    "test_results_sig_dict = {}\n",
    "test_results_bkg_dict = {}\n",
    "\n",
    "train_results_sig_dict = {}\n",
    "train_results_bkg_dict = {}\n",
    "\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "\n",
    "    bdt = xgboost.Booster()\n",
    "    bdt.load_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}_MeV_New_20_variables_FIXED.json\")\n",
    "    #bdt.load_model(f'bdts/{Run}_{HNL_mass}_MeV_REDUCED_variables_flattened_highest_E_2.json')\n",
    "    \n",
    "    importance = bdt.get_score(importance_type=\"gain\")\n",
    "    \n",
    "    for key in importance.keys():\n",
    "        importance[key] = round(importance[key],1)\n",
    "        \n",
    "    #importance_dict[HNL_mass] = importance\n",
    "\n",
    "    results_sig = bdt.predict(xgb_test_dict[HNL_mass])\n",
    "    results_bkg = bdt.predict(xgb_test_bkg)\n",
    "    \n",
    "    train_results_sig = bdt.predict(xgb_sig_train_dict[HNL_mass])\n",
    "    train_results_bkg = bdt.predict(xgb_bkg_train_dict[HNL_mass])\n",
    "    \n",
    "    test_results_sig_dict.update({HNL_mass:results_sig})\n",
    "    test_results_bkg_dict.update({HNL_mass:results_bkg})\n",
    "    \n",
    "    train_results_sig_dict.update({HNL_mass:train_results_sig})\n",
    "    train_results_bkg_dict.update({HNL_mass:train_results_bkg})\n",
    "\n",
    "    # results_dirt = bdt.predict(dirt_matrix)\n",
    "    # results_EXT = bdt.predict(EXT_matrix)\n",
    "    \n",
    "    # dirt_BDT[f'BDT_output_{HNL_mass}MeV'] = results_dirt\n",
    "    # EXT_BDT[f'BDT_output_{HNL_mass}MeV'] = results_EXT\n",
    "\n",
    "    # overlay_test_BDT[f'BDT_output_{HNL_mass}MeV'] = results_bkg\n",
    "    # #Can add in a second loop over HNL_masses so that I predict each signal mass point with every other mass point bdt\n",
    "    # signal_test_BDT_dict[HNL_mass][f'BDT_output'] = results_sig\n",
    "    \n",
    "    #Plotting importances of variables\n",
    "    plt.figure(figsize=(12,12),facecolor='white')\n",
    "    print(f\"Plotting {HNL_mass}MeV importances:\")\n",
    "    a = xgboost.plot_importance(importance,max_num_features=10,importance_type='gain')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91c75b-3da4-4991-8cdb-aa6644bba871",
   "metadata": {},
   "source": [
    "## Test vs. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab0164d-636e-4e96-9b92-56e108ccae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_range=[0,1.0]\n",
    "n_bins=20\n",
    "\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.hist(train_results_sig_dict[HNL_mass],bins=n_bins, range=hist_range, density=True,alpha=0.4,color='red',label=f'Train {HNL_mass}MeV HNL' )\n",
    "    counts,bin_edges = np.histogram(test_results_sig_dict[HNL_mass],bins=n_bins,range=hist_range,density=True)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:])/2.\n",
    "    plt.plot(bin_centers,counts,marker='o',linestyle=\"None\",color='red',label=f'Test {HNL_mass}MeV HNL')\n",
    "\n",
    "    plt.hist(train_results_bkg_dict[HNL_mass], bins = n_bins, range = hist_range, density = True, alpha = 0.4, color = 'orange', label = r'Train overlay')\n",
    "    counts,bin_edges = np.histogram(test_results_bkg_dict[HNL_mass],bins = n_bins, range= hist_range,density = True)\n",
    "    bin_centers = (bin_edges[:-1] +  bin_edges[1:])/2.\n",
    "    plt.plot(bin_centers,counts,marker='o',linestyle =\"None\",color='orange',label = r'Test overlay')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f34d7-408f-40ce-9ca6-2d74dce1e6f1",
   "metadata": {},
   "source": [
    "## Checking variable correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a4680-42f1-4f9b-954d-1197dd26917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from Luis' code\n",
    "# for HNL_mass in HNL_masses:\n",
    "method = 'kendall'\n",
    "correlations = cleaned_signal_dict[100][bdt_vars].astype(np.float64).corr(method=method)\n",
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(correlations,vmin=-1,annot=False,square=True,cbar_kws={'label':method+' correlation'},cmap = 'RdBu_r')\n",
    "plt.title('Input Variable Correlations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a32c08-e4b9-4d6c-8037-4aa3bc074521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just looking at most correlated \n",
    "corr=cleaned_signal_dict[100][bdt_vars].corr()\n",
    "high_corr_var=np.where(corr>0.95)\n",
    "high_corr_var=[(corr.columns[x],corr.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\n",
    "high_corr_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7d7ce-cb79-4e2f-a35d-88717247cdf0",
   "metadata": {},
   "source": [
    "# End of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ebabd-3c77-4278-914d-0c65d0201329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
