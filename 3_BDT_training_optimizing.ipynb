{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab81f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.24/06\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "import os,sys,string, time\n",
    "import ROOT\n",
    "from math import *\n",
    "from ROOT import gPad, TTree, TObject, TFile, gDirectory, TH1D, TH2D, TH3D, TCanvas, gROOT, TGaxis, gStyle, TColor, TLegend, THStack, TChain, TLatex, TText, TCollection, kRed, kBlue\n",
    "from array import array\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "import pandas as pd\n",
    "from root_pandas import read_root\n",
    "from platform import python_version\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import uproot3\n",
    "import math\n",
    "from matplotlib.patches import Rectangle\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "\n",
    "import Utilities.Plotter as PT\n",
    "import Utilities.Constants as Constants\n",
    "import Utilities.Variables_list as Variables\n",
    "import Utilities.Functions as Functions\n",
    "from Utilities.ipython_exit import exit\n",
    "\n",
    "print ('Success')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a3504-392d-4b23-a225-0db439fcb0f5",
   "metadata": {},
   "source": [
    "## Reading in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e41222-cb45-4107-a206-3b3653d23ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = {\"Run\":\"run3\", #The run number, so far either \"run1\" or \"run3\"\n",
    "          \"Load_single_file\":False, #This will override everything else, put the desired file in the \"single_file\" line\n",
    "          \"single_file\":10,\n",
    "          \"Load_standard\":True,\n",
    "          \"Load_DetVars\":False,\n",
    "          \"Only_keep_common_DetVar_evs\":True,\n",
    "          \"Load_data\":False,\n",
    "          \"FLATTEN\":True, #Have one row per reconstructed object in the analysis dataframe\n",
    "          \"only_presel\":False, #Create small files containing only variables necessary for pre-selection, for making pre-selection plots\n",
    "          \"EXT_in_training\":False,\n",
    "          \"Load_pi0_signal\":False} \n",
    "\n",
    "feature_names = Variables.First_pass_vars_for_BDT #All variables\n",
    "feature_names_MC = feature_names + [\"weight\"]\n",
    "loc_pkls = \"pkl_files/\"+Params[\"Run\"]+\"/current_files/my_vars/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d786fc-af51-48e5-88d4-d0ee5bcd2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_samples_dict = {}\n",
    "\n",
    "if Params[\"Load_pi0_signal\"] == False:\n",
    "    Presel_overlay = pd.read_pickle(loc_pkls+\"Preselected_overlay_\"+Params[\"Run\"]+\"_my_vars_flattened_FINAL.pkl\")\n",
    "    if Params[\"Load_single_file\"] == True:\n",
    "        HNL_mass = Params[\"single_file\"]\n",
    "        Presel_signal = pd.read_pickle(loc_pkls+f\"Preselected_{HNL_mass}_\"+Params[\"Run\"]+\"_my_vars_flattened_FINAL.pkl\")\n",
    "        signal_samples_dict[HNL_mass] = Presel_signal\n",
    "    else:\n",
    "        for HNL_mass in Constants.HNL_mass_samples:\n",
    "            Presel_signal = pd.read_pickle(loc_pkls+f\"Preselected_{HNL_mass}_\"+Params[\"Run\"]+\"_my_vars_flattened_FINAL.pkl\")\n",
    "            signal_samples_dict[HNL_mass] = Presel_signal\n",
    "    \n",
    "if Params[\"Load_pi0_signal\"] == True:\n",
    "    Presel_overlay = pd.read_pickle(loc_pkls+\"pi0_selection/Preselected_\"+Params[\"Run\"]+\"_overlay_final.pkl\")\n",
    "    \n",
    "    for HNL_mass in Constants.HNL_mass_pi0_samples:\n",
    "        Presel_signal = pd.read_pickle(loc_pkls+f\"pi0_selection/Preselected_\"+Params[\"Run\"]+f\"_{HNL_mass}_pi0_final.pkl\")\n",
    "        signal_samples_dict[HNL_mass] = Presel_signal\n",
    "    \n",
    "if Params[\"EXT_in_training\"] == True:\n",
    "    Presel_EXT = pd.read_pickle(loc_pkls+\"Preselected_EXT_\"+Params[\"Run\"]+\"_my_vars_flattened_FINAL.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb76888-7925-407b-9a82-b6a997a12e5e",
   "metadata": {},
   "source": [
    "## Splitting into test and training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d1638c-9af1-4f64-aed1-16affdd7aec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features is 19\n",
      "Total length of overlay file is 29734\n",
      "Total length of overlay TRAIN file is 20813\n",
      "Total length of overlay TEST file is 8921\n"
     ]
    }
   ],
   "source": [
    "New_feature_list = ['shrclusdir2', 'n_tracks', 'trk_energy', 'shr_theta_v', 'contained_sps_ratio', 'shr_px_v',\n",
    "                    'trk_end_x_v', 'n_pfps', 'pfnplanehits_V', 'pfnplanehits_U', 'trk_calo_energy_u_v', 'trk_score_v',\n",
    "                    'NeutrinoEnergy2', 'shr_phi_v', 'pfnplanehits_Y', 'shr_pz_v', 'trk_theta_v', 'trk_phi_v',\n",
    "                    'trk_dir_z_v']\n",
    "print(f\"Number of features is {len(New_feature_list)}\")\n",
    "# bdt_vars = feature_names\n",
    "bdt_vars = New_feature_list #This is using just the most important variables list\n",
    "\n",
    "new_value = -9999 #This tells XGB what number refers to missing data for all variables\n",
    "\n",
    "signal_train_dict = {}\n",
    "signal_test_dict = {}\n",
    "labels_dict = {} \n",
    "\n",
    "train_vs_test_fraction = 0.7 #This is the fraction used for training\n",
    "\n",
    "print(f\"Total length of overlay file is {len(Presel_overlay)}\")\n",
    "\n",
    "overlay_train = Presel_overlay[:int(len(Presel_overlay)*train_vs_test_fraction)]\n",
    "overlay_test = Presel_overlay[int(len(Presel_overlay)*train_vs_test_fraction):]\n",
    "\n",
    "print(f\"Total length of overlay TRAIN file is {len(overlay_train)}\")\n",
    "print(f\"Total length of overlay TEST file is {len(overlay_test)}\")\n",
    "\n",
    "# overlay_train = Presel_overlay[int(len(Presel_overlay)*train_vs_test_fraction):] #OLD WRONG WAY, i.e 70% test\n",
    "# overlay_test = Presel_overlay[:int(len(Presel_overlay)*train_vs_test_fraction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb8dfcb7-34ab-4420-adb5-14410c9f717a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling 2MeV HNL test sample\n",
      "Pickling 10MeV HNL test sample\n",
      "Pickling 20MeV HNL test sample\n",
      "Pickling 50MeV HNL test sample\n",
      "Pickling 100MeV HNL test sample\n",
      "Pickling 150MeV HNL test sample\n",
      "Pickling 180MeV HNL test sample\n",
      "Pickling 200MeV HNL test sample\n",
      "Pickling 220MeV HNL test sample\n",
      "Pickling 240MeV HNL test sample\n",
      "Pickling 245MeV HNL test sample\n"
     ]
    }
   ],
   "source": [
    "pickle_files = True\n",
    "\n",
    "if Params[\"Load_pi0_signal\"] == False:\n",
    "    overlay_test.to_pickle(loc_pkls+\"BDT_Test_dfs/Test_overlay_\"+Params[\"Run\"]+\"_my_vars_flattened_FINAL.pkl\")\n",
    "    \n",
    "    for HNL_mass in signal_samples_dict:\n",
    "        signal_train_dict[HNL_mass] = signal_samples_dict[HNL_mass][:int(len(signal_samples_dict[HNL_mass])*train_vs_test_fraction)]\n",
    "        signal_test_dict[HNL_mass] = signal_samples_dict[HNL_mass][int(len(signal_samples_dict[HNL_mass])*train_vs_test_fraction):]\n",
    "        if pickle_files == True:\n",
    "            print(f\"Pickling {HNL_mass}MeV HNL test sample\")\n",
    "            signal_test_dict[HNL_mass].to_pickle(loc_pkls+f\"BDT_Test_dfs/Test_signal_{HNL_mass}_\"+Params[\"Run\"]+\"_my_vars_flattened_FINAL.pkl\")\n",
    "    \n",
    "        labels_dict[HNL_mass] = [1]*len(signal_train_dict[HNL_mass][bdt_vars]) + [0]*len(overlay_train[bdt_vars])\n",
    "if Params[\"Load_pi0_signal\"] == True:\n",
    "    overlay_test.to_pickle(loc_pkls+\"BDT_Test_dfs/pi0_selection/Test_overlay_\"+Params[\"Run\"]+\"_FIXED.pkl\")\n",
    "\n",
    "    for HNL_mass in signal_samples_dict:\n",
    "        signal_train_dict[HNL_mass] = signal_samples_dict[HNL_mass][:int(len(signal_samples_dict[HNL_mass])*train_vs_test_fraction)]\n",
    "        signal_test_dict[HNL_mass] = signal_samples_dict[HNL_mass][int(len(signal_samples_dict[HNL_mass])*train_vs_test_fraction):]\n",
    "        if pickle_files == True:\n",
    "            print(f\"Pickling {HNL_mass}MeV HNL pi0 test sample\")\n",
    "            signal_test_dict[HNL_mass].to_pickle(loc_pkls+f\"BDT_Test_dfs/pi0_selection/Test_signal_{HNL_mass}_pi0_\"+\n",
    "                                                 Params[\"Run\"]+\"_FIXED.pkl\")\n",
    "    \n",
    "        labels_dict[HNL_mass] = [1]*len(signal_train_dict[HNL_mass][bdt_vars]) + [0]*len(overlay_train[bdt_vars])\n",
    "    \n",
    "if Params[\"EXT_in_training\"] == True:\n",
    "    frac_EXT = 0.1\n",
    "    EXT_train = Presel_EXT[:int(len(Presel_EXT)*frac_EXT)]\n",
    "    print(\"Number of EXT to train: \" + str(len(EXT_train)))\n",
    "    overlay_plus_EXT = pd.concat([overlay_train[bdt_vars],EXT_train[bdt_vars]])\n",
    "    for HNL_mass in Constants.HNL_mass_samples:\n",
    "        labels_dict[HNL_mass] = labels_dict[HNL_mass] + [0]*len(EXT_train[bdt_vars])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e387da2f-dfb1-4433-a85a-5b98d40611d3",
   "metadata": {},
   "source": [
    "## BDT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82f0a87a-aad9-4ec4-98ba-517bfe7b7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_dict = {}\n",
    "xgb_test_dict = {}\n",
    "\n",
    "xgb_sig_train_dict = {}\n",
    "xgb_bkg_train_dict = {}\n",
    "\n",
    "xgb_test_bkg = xgboost.DMatrix(overlay_test[bdt_vars], label=[0]*len(overlay_test[bdt_vars]), missing=new_value, feature_names=bdt_vars)\n",
    "\n",
    "xgb_param = {'booster': 'dart',\n",
    "        'max_depth':6,\n",
    "        'eta':0.3,\n",
    "        'objective':'binary:logistic',\n",
    "#        'eval_metric':'auc', \n",
    "#        'subsample':0.5,\n",
    "        'tree_method':'hist',\n",
    "#        'scale_pos_weight': float(len(data_bkg_train))/float(len(data_sig_train)),\n",
    "        'rate_drop': 0.1,\n",
    "        'skip_drop': 0.5 }\n",
    "num_round = 50\n",
    "progress = dict()\n",
    "\n",
    "for HNL_mass in signal_train_dict:\n",
    "    if Params[\"EXT_in_training\"] == False:\n",
    "        xgb_train_dict[HNL_mass] = xgboost.DMatrix(pd.concat([signal_train_dict[HNL_mass][bdt_vars], #This is both signal and bkg combined into one\n",
    "                                                               overlay_train[bdt_vars]]), \n",
    "                                               label=labels_dict[HNL_mass], \n",
    "                                                    missing=new_value, feature_names=bdt_vars)\n",
    "    if Params[\"EXT_in_training\"] == True:\n",
    "        xgb_train_dict[HNL_mass] = xgboost.DMatrix(pd.concat([signal_train_dict[HNL_mass][bdt_vars], #This is both signal and bkg combined into one\n",
    "                                                               overlay_plus_EXT[bdt_vars]]), \n",
    "                                               label=labels_dict[HNL_mass], \n",
    "                                                    missing=new_value, feature_names=bdt_vars)\n",
    "    xgb_test_dict[HNL_mass] = xgboost.DMatrix(signal_test_dict[HNL_mass][bdt_vars], label=[1]*len(signal_test_dict[HNL_mass][bdt_vars]), #Just signal test\n",
    "                                              missing=new_value, feature_names=bdt_vars)\n",
    "    \n",
    "    xgb_sig_train_dict[HNL_mass] = xgboost.DMatrix(signal_train_dict[HNL_mass][bdt_vars],label=[1]*len(signal_train_dict[HNL_mass][bdt_vars]), #Signal training\n",
    "                                                  missing=new_value, feature_names=bdt_vars)\n",
    "    xgb_bkg_train_dict[HNL_mass] = xgboost.DMatrix(overlay_train[bdt_vars],label=[0]*len(overlay_train[bdt_vars]), #Just background training\n",
    "                                                  missing=new_value, feature_names=bdt_vars)\n",
    "\n",
    "    #watchlist so that you can monitor the performance of the training by iterations\n",
    "    watchlist = [(xgb_train_dict[HNL_mass], 'train'), (xgb_test_dict[HNL_mass], 'test_sig'), (xgb_test_bkg,'test_bkg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abd8965e-42ff-466b-9971-24bb5e0cb0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 2MeV BDT\n",
      "\n",
      "[14:05:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 10MeV BDT\n",
      "\n",
      "[14:05:12] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 20MeV BDT\n",
      "\n",
      "[14:05:14] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 50MeV BDT\n",
      "\n",
      "[14:05:17] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 100MeV BDT\n",
      "\n",
      "[14:05:19] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 150MeV BDT\n",
      "\n",
      "[14:05:21] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 180MeV BDT\n",
      "\n",
      "[14:05:24] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 200MeV BDT\n",
      "\n",
      "[14:05:26] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 220MeV BDT\n",
      "\n",
      "[14:05:29] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 240MeV BDT\n",
      "\n",
      "[14:05:32] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Training 245MeV BDT\n",
      "\n",
      "[14:05:35] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1637426272325/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "for HNL_mass in signal_train_dict:\n",
    "    print(f\"Training {HNL_mass}MeV BDT\" + \"\\n\")\n",
    "    bdt = xgboost.train(xgb_param, xgb_train_dict[HNL_mass], num_round, watchlist, evals_result=progress, verbose_eval=False)\n",
    "    # doesnt like watchlist/eval_result if using AOC\n",
    "    # save model so you can load it later\n",
    "    if Params[\"Load_pi0_signal\"] == False:\n",
    "        bdt.save_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_FINAL.json\")\n",
    "    if Params[\"Load_pi0_signal\"] == True:\n",
    "        bdt.save_model(\"bdts/pi0_selection/\"+Params[\"Run\"]+f\"_{HNL_mass}MeV_pi0_FINAL.json\")\n",
    "    #bdt.save_model(f'bdts/{Run}_{HNL_mass}_MeV_REDUCED_variables_flattened_highest_E_3.json')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5a815-d856-4b76-86af-1abd2b38f6d7",
   "metadata": {},
   "source": [
    "# Finished Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293380da-31c0-4d08-b5f5-7ac71386a1ef",
   "metadata": {},
   "source": [
    "## Checking variable correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ce0d0-4404-4a8c-9acd-642687cf8462",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_vars = feature_names\n",
    "HNL_mass = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad1892-8b6b-48f2-af3b-02622353d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from Luis' code\n",
    "# for HNL_mass in HNL_masses:\n",
    "method = 'kendall'\n",
    "correlations = signal_samples_dict[HNL_mass][bdt_vars].astype(np.float64).corr(method=method)\n",
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(correlations,vmin=-1,annot=False,square=True,cbar_kws={'label':method+' correlation'},cmap = 'RdBu_r')\n",
    "plt.title('Input Variable Correlations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd62bb6-b92f-448f-8514-9d5243a9aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just looking at most correlated \n",
    "corr=signal_samples_dict[HNL_mass][bdt_vars].corr()\n",
    "high_corr_var=np.where(corr>0.999)\n",
    "high_corr_var=[(corr.columns[x],corr.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\n",
    "#high_corr_var\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f50740e-dd5e-4cb7-ac69-396d483a0428",
   "metadata": {},
   "source": [
    "## Looking at feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351d3da-76cc-4079-9036-548307e99353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make an importances dict   \n",
    "def Top_N_vars(bdt_model, N_vars):\n",
    "    importance = bdt.get_score(importance_type=\"gain\")\n",
    "    # for key in importance.keys():\n",
    "    #     importance[key] = round(importance[key],1)\n",
    "    sorted_importance = dict(sorted(importance.items(), key=lambda item: item[1]))\n",
    "    sorted_importance_list = list(sorted_importance.values())\n",
    "    sorted_importance_keys= list(sorted_importance.keys())\n",
    "    top_N = sorted_importance_keys[-N_vars:]\n",
    "    \n",
    "    return top_N\n",
    "\n",
    "top_N_dict = {}\n",
    "list_of_lists = []\n",
    "\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    bdt = xgboost.Booster()\n",
    "    bdt.load_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}_MeV_New_20_variables_FIXED.json\")\n",
    "    # print(\"Number of entries in top20 is \" + str(len(Top_N_vars(bdt, 20))))\n",
    "    # top_N_dict[HNL_mass] = Top_N_vars(bdt, 20)\n",
    "    top_N = Top_N_vars(bdt, 50)\n",
    "    print(len(top_N))\n",
    "    list_of_lists.append(Top_N_vars(bdt, 50))\n",
    "    \n",
    "elements_in_all = list(set.intersection(*map(set, list_of_lists)))\n",
    "print(len(elements_in_all))\n",
    "print(elements_in_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380877bb-f8ed-4648-88fd-3e4eec8c6c1d",
   "metadata": {},
   "source": [
    "# Finished code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28366e0-1cea-490c-840e-bcccdfd88413",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655c588-34cf-4b24-aac7-73821673945b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966bb616-3075-4722-bbad-57a1e9afb281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirt_matrix = xgboost.DMatrix(dirt_BDT[bdt_vars])\n",
    "# EXT_matrix = xgboost.DMatrix(EXT_BDT[bdt_vars])\n",
    "\n",
    "test_results_sig_dict = {}\n",
    "test_results_bkg_dict = {}\n",
    "\n",
    "train_results_sig_dict = {}\n",
    "train_results_bkg_dict = {}\n",
    "\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "\n",
    "    bdt = xgboost.Booster()\n",
    "    bdt.load_model(\"bdts/\"+Params[\"Run\"]+f\"_{HNL_mass}_MeV_New_20_variables_FIXED.json\")\n",
    "    #bdt.load_model(f'bdts/{Run}_{HNL_mass}_MeV_REDUCED_variables_flattened_highest_E_2.json')\n",
    "    \n",
    "    importance = bdt.get_score(importance_type=\"gain\")\n",
    "    \n",
    "    for key in importance.keys():\n",
    "        importance[key] = round(importance[key],1)\n",
    "        \n",
    "    #importance_dict[HNL_mass] = importance\n",
    "\n",
    "    results_sig = bdt.predict(xgb_test_dict[HNL_mass])\n",
    "    results_bkg = bdt.predict(xgb_test_bkg)\n",
    "    \n",
    "    train_results_sig = bdt.predict(xgb_sig_train_dict[HNL_mass])\n",
    "    train_results_bkg = bdt.predict(xgb_bkg_train_dict[HNL_mass])\n",
    "    \n",
    "    test_results_sig_dict.update({HNL_mass:results_sig})\n",
    "    test_results_bkg_dict.update({HNL_mass:results_bkg})\n",
    "    \n",
    "    train_results_sig_dict.update({HNL_mass:train_results_sig})\n",
    "    train_results_bkg_dict.update({HNL_mass:train_results_bkg})\n",
    "\n",
    "    # results_dirt = bdt.predict(dirt_matrix)\n",
    "    # results_EXT = bdt.predict(EXT_matrix)\n",
    "    \n",
    "    # dirt_BDT[f'BDT_output_{HNL_mass}MeV'] = results_dirt\n",
    "    # EXT_BDT[f'BDT_output_{HNL_mass}MeV'] = results_EXT\n",
    "\n",
    "    # overlay_test_BDT[f'BDT_output_{HNL_mass}MeV'] = results_bkg\n",
    "    # #Can add in a second loop over HNL_masses so that I predict each signal mass point with every other mass point bdt\n",
    "    # signal_test_BDT_dict[HNL_mass][f'BDT_output'] = results_sig\n",
    "    \n",
    "    #Plotting importances of variables\n",
    "    plt.figure(figsize=(12,12),facecolor='white')\n",
    "    print(f\"Plotting {HNL_mass}MeV importances:\")\n",
    "    a = xgboost.plot_importance(importance,max_num_features=10,importance_type='gain')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91c75b-3da4-4991-8cdb-aa6644bba871",
   "metadata": {},
   "source": [
    "## Test vs. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb41dc5-04b9-4043-bfa1-b0458f7697a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_sig_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab0164d-636e-4e96-9b92-56e108ccae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_range=[0,1.0]\n",
    "n_bins=20\n",
    "\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.hist(train_results_sig_dict[HNL_mass],bins=n_bins, range=hist_range, density=True,alpha=0.4,color='red',label=f'Train {HNL_mass}MeV HNL' )\n",
    "    counts,bin_edges = np.histogram(test_results_sig_dict[HNL_mass],bins=n_bins,range=hist_range,density=True)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:])/2.\n",
    "    plt.plot(bin_centers,counts,marker='o',linestyle=\"None\",color='red',label=f'Test {HNL_mass}MeV HNL')\n",
    "\n",
    "    plt.hist(train_results_bkg_dict[HNL_mass], bins = n_bins, range = hist_range, density = True, alpha = 0.4, color = 'orange', label = r'Train overlay')\n",
    "    counts,bin_edges = np.histogram(test_results_bkg_dict[HNL_mass],bins = n_bins, range= hist_range,density = True)\n",
    "    bin_centers = (bin_edges[:-1] +  bin_edges[1:])/2.\n",
    "    plt.plot(bin_centers,counts,marker='o',linestyle =\"None\",color='orange',label = r'Test overlay')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d368690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names = ['shrclusdir0','shrclusdir1','shrclusdir2', 'shr_pca_1', 'shr_hits_max',\n",
    "#                  'trk_chipr_best', 'trk_energy_hits_tot', 'trk_dir_z_v', 'trk_theta_v', 'trk_phi_v', 'trk_bragg_mip_v',\n",
    "#                  'NeutrinoEnergy2', 'merge_bestdist', 'n_pfps', 'pfnplanehits_Y']\n",
    "feature_names = Variables.First_pass_vars_for_BDT #All variables\n",
    "feature_names_MC = feature_names + [\"weight\"]\n",
    "\n",
    "HNL_masses = Constants.HNL_mass_samples #in MeV\n",
    "\n",
    "Run = \"run1\" #Either \"run1\" or \"run3\" at the moment\n",
    "\n",
    "loc_pkls = f\"pkl_files/{Run}/current_files/my_vars/\"\n",
    "\n",
    "variables_string = \"my_vars\"\n",
    "Flat_state = \"flattened\"\n",
    "\n",
    "Load_standard = False\n",
    "Load_DetVars = True\n",
    "\n",
    "if Load_standard == True:\n",
    "    print(f\"Loading {Run} .pkls\" + \"\\n\")\n",
    "\n",
    "    Presel_overlay = pd.read_pickle(loc_pkls+f\"Preselected_overlay_{Run}_my_vars_flattened.pkl\")\n",
    "    Presel_dirt = pd.read_pickle(loc_pkls+f\"Preselected_dirt_{Run}_my_vars_flattened.pkl\")\n",
    "    Presel_EXT = pd.read_pickle(loc_pkls+f\"Preselected_EXT_{Run}_my_vars_flattened.pkl\")\n",
    "\n",
    "    signal_samples_dict = {}\n",
    "\n",
    "    for HNL_mass in HNL_masses:\n",
    "        Presel_signal = pd.read_pickle(loc_pkls+f\"Preselected_signal_{HNL_mass}MeV_{Run}_my_vars_flattened.pkl\")\n",
    "        signal_samples_dict[HNL_mass] = Presel_signal\n",
    "\n",
    "    print(\"Overlay .pkl is \"+str(len(Presel_overlay))+\" entries long.\")\n",
    "    print(\"Dirt .pkl is \"+str(len(Presel_dirt))+\" entries long.\")\n",
    "    print(\"EXT .pkl is \"+str(len(Presel_EXT))+\" entries long.\")\n",
    "    for HNL_mass in HNL_masses:\n",
    "        print(f\"{HNL_mass} Signal .pkl is \"+str(len(signal_samples_dict[HNL_mass]))+\" entries long.\")\n",
    "\n",
    "    print()\n",
    "    print(\"Variables in overlay is \" + str(len(Presel_overlay.keys())))\n",
    "    print(\"Variables in dirt is \" + str(len(Presel_dirt.keys())))\n",
    "    print(\"Variables in EXT is \" + str(len(Presel_EXT.keys())))\n",
    "    for HNL_mass in HNL_masses:\n",
    "        print(f\"Variables in {HNL_mass}MeV Signal is \" + str(len(signal_samples_dict[HNL_mass].keys())))\n",
    "\n",
    "if Load_DetVars == True:\n",
    "    DetVars = Constants.Detector_variations\n",
    "    DetVar_samples = {}\n",
    "    for DetVar in DetVars:\n",
    "        placeholder = pd.read_pickle(loc_pkls+\"DetVars/\"+f\"Preselected_overlay_{Run}_{variables_string}_{DetVar}_{Flat_state}.pkl\")\n",
    "        placeholder_dict = {DetVar:placeholder}\n",
    "        DetVar_samples.update(placeholder_dict)\n",
    "    print(DetVar_samples.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae7f513-9717-4ed9-a850-a4314ab711cf",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05be5b8-7091-4be8-b3c3-4a81d5eaa7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_dfs_for_xgb(df, variable_list, edited_vars): #The default value for missing data in XGB is 0. So this changes those very large negative values to -9999.\n",
    "    value = -1e15\n",
    "    new_value = -9999\n",
    "    for variable in df.keys():\n",
    "        if(len(df.loc[df[variable] < value]) > 0):\n",
    "            edited_vars.append(variable)\n",
    "            df.loc[(df[variable] < value), variable] = new_value #Sets the new value\n",
    "        if(len(df.loc[df[variable] == -1.0]) > 0):\n",
    "            edited_vars.append(variable)\n",
    "            df.loc[(df[variable] == -1.0), variable] = new_value #Sets the new value\n",
    "        if(len(df.loc[df[variable] == np.nan]) > 0):\n",
    "            edited_vars.append(variable)\n",
    "            df.loc[(df[variable] == np.nan), variable] = new_value #Sets the new value\n",
    "        if(len(df.loc[df[variable] == np.inf]) > 0):\n",
    "            edited_vars.append(variable)\n",
    "            df.loc[(df[variable] == np.inf), variable] = new_value #Sets the new value\n",
    "            \n",
    "    df_edited = df[variable_list].copy() #Ensures the variable list is just those given, i.e removing any others\n",
    "    return df_edited\n",
    "\n",
    "def only_keep_highest_E(df):\n",
    "    df[\"highest_E\"]=df['pfnplanehits_Y'].groupby(\"entry\").transform(max) == df['pfnplanehits_Y']\n",
    "    df_new = df.query(\"highest_E\").copy()\n",
    "    return df_new\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669c303-5cc3-4db6-a477-2959d4598d00",
   "metadata": {},
   "source": [
    "## Only keeping highest energy object per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551e733-e126-4368-bff0-2685b8a5cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Load_standard == True:\n",
    "    overlay_highest_E = only_keep_highest_E(Presel_overlay)\n",
    "    dirt_highest_E = only_keep_highest_E(Presel_dirt)\n",
    "    EXT_highest_E = only_keep_highest_E(Presel_EXT)\n",
    "\n",
    "    signal_dict_highest_E = {}\n",
    "    for HNL_mass in HNL_masses:\n",
    "        signal_dict_highest_E[HNL_mass] = only_keep_highest_E(signal_samples_dict[HNL_mass])\n",
    "\n",
    "if Load_DetVars == True:\n",
    "    DetVar_dict_highest_E = {}\n",
    "    for DetVar in DetVars:\n",
    "        DetVar_dict_highest_E[DetVar] = only_keep_highest_E(DetVar_samples[DetVar])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b91d7-68db-4bb9-b3af-415eee49105e",
   "metadata": {},
   "source": [
    "## Splitting samples into test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df421a98-76df-46a7-ba4f-19e1f9a8dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_vars = feature_names\n",
    "\n",
    "new_value = -9999 #This tells XGB what number refers to missing data for all variables\n",
    "\n",
    "signal_train_dict = {}\n",
    "signal_test_dict = {}\n",
    "labels_dict = {} #I should try and understand what this is doing better\n",
    "\n",
    "train_vs_test_fraction = 0.7\n",
    "\n",
    "#data_bkg = Cleaned_Presel_overlay_highest_E\n",
    "overlay = overlay_highest_E\n",
    "\n",
    "overlay_train = overlay[int(len(overlay)*train_vs_test_fraction):]\n",
    "overlay_test = overlay[:int(len(overlay)*train_vs_test_fraction)]\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    signal_train_dict[HNL_mass] = signal_dict_highest_E[HNL_mass][int(len(signal_dict_highest_E[HNL_mass])*train_vs_test_fraction):]\n",
    "    signal_test_dict[HNL_mass] = signal_dict_highest_E[HNL_mass][:int(len(signal_dict_highest_E[HNL_mass])*train_vs_test_fraction)]\n",
    "    \n",
    "    labels_dict[HNL_mass] = [1]*len(signal_train_dict[HNL_mass][bdt_vars]) + [0]*len(overlay_train[bdt_vars])\n",
    "    print(len(signal_train_dict[HNL_mass])+len(overlay_train))\n",
    "    print(len(labels_dict[HNL_mass]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eeade5-fba6-484b-b10c-63f22888702a",
   "metadata": {},
   "source": [
    "## Deleting columns which shouldn't be trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c2c20d-7b7a-4285-995c-06a0431dbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_save = Variables.event_vars + Variables.weight_related + ['weight']\n",
    "\n",
    "overlay_to_save = overlay_test[vars_to_save].copy()\n",
    "dirt_to_save = dirt_highest_E[vars_to_save].copy()\n",
    "EXT_to_save = EXT_highest_E[Variables.event_vars].copy()\n",
    "\n",
    "signal_to_save_dict = {}\n",
    "for HNL_mass in HNL_masses:\n",
    "    signal_to_save_dict[HNL_mass] = signal_test_dict[HNL_mass][Variables.event_vars].copy()\n",
    "    \n",
    "\n",
    "edited_vars_overlay, edited_vars_dirt, edited_vars_EXT, edited_vars_signal = [], [], [], [] #Should probably get rid once I'm happy with what happens\n",
    "\n",
    "overlay_train_BDT = Prepare_dfs_for_xgb(overlay_train, feature_names_MC, edited_vars_overlay)\n",
    "overlay_test_BDT = Prepare_dfs_for_xgb(overlay_test, feature_names_MC, edited_vars_overlay)\n",
    "\n",
    "dirt_BDT = Prepare_dfs_for_xgb(dirt_highest_E, feature_names_MC, edited_vars_dirt)\n",
    "EXT_BDT = Prepare_dfs_for_xgb(EXT_highest_E, feature_names, edited_vars_EXT)\n",
    "\n",
    "signal_train_BDT_dict = {}\n",
    "signal_test_BDT_dict = {}\n",
    "for HNL_mass in HNL_masses:\n",
    "    signal_train_BDT_dict[HNL_mass] = Prepare_dfs_for_xgb(signal_train_dict[HNL_mass], feature_names, edited_vars_signal)\n",
    "    signal_test_BDT_dict[HNL_mass] = Prepare_dfs_for_xgb(signal_test_dict[HNL_mass], feature_names, edited_vars_signal)\n",
    "\n",
    "print(\"Variables in overlay is \" + str(len(overlay_train_BDT.keys())))\n",
    "print(\"Variables in dirt is \" + str(len(dirt_BDT.keys())))\n",
    "print(\"Variables in EXT is \" + str(len(EXT_BDT.keys())))\n",
    "for HNL_mass in HNL_masses:\n",
    "    print(f\"Variables in {HNL_mass}MeV Signal is \" + str(len(signal_train_BDT_dict[HNL_mass].keys())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad466c9-b8a7-4ea7-afda-df5591afb5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_vars_detvar = []\n",
    "if Load_DetVars == True:\n",
    "    DetVar_BDT_dict = {}\n",
    "    for DetVar in Constants.Detector_variations:\n",
    "        DetVar_BDT_dict[DetVar] = Prepare_dfs_for_xgb(DetVar_dict_highest_E[DetVar], feature_names_MC, edited_vars_detvar)\n",
    "        print(DetVar)\n",
    "        print(len(DetVar_BDT_dict[DetVar]), len(DetVar_dict_highest_E[DetVar]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90deba05-94e5-46fe-af19-4bfe84435f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DetVar_BDT_dict.keys())\n",
    "print(len(DetVar_BDT_dict['LYDown']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d995b4-5080-46f7-873d-902c6848317d",
   "metadata": {},
   "source": [
    "# BDT test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247b98dc-e5c1-4a0e-beb9-ca4cf1740457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making train and test samples\n",
    "#Do I need to create new \"random\" training samples for each signal sample to train against? - so far have not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb614370-bf97-40a1-b863-4158aad9ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_dict = {}\n",
    "xgb_test_dict = {}\n",
    "\n",
    "xgb_sig_train_dict = {}\n",
    "xgb_bkg_train_dict = {}\n",
    "\n",
    "xgb_test_bkg = xgboost.DMatrix(overlay_test_BDT[bdt_vars], label=[0]*len(overlay_test_BDT[bdt_vars]), missing=new_value, feature_names=bdt_vars)\n",
    "\n",
    "param = {'booster': 'dart',\n",
    "        'max_depth':6,\n",
    "        'eta':0.3,\n",
    "        'objective':'binary:logistic',\n",
    "#         'eval_metric':'auc', \n",
    "#        'subsample':0.5,\n",
    "        'tree_method':'hist',\n",
    "#         'scale_pos_weight': float(len(data_bkg_train))/float(len(data_sig_train)),\n",
    "        'rate_drop': 0.1,\n",
    "        'skip_drop': 0.5 }\n",
    "num_round = 50\n",
    "progress = dict()\n",
    "\n",
    "for HNL_mass in Constants.HNL_mass_samples:\n",
    "    xgb_train_dict[HNL_mass] = xgboost.DMatrix(pd.concat([signal_train_BDT_dict[HNL_mass][bdt_vars], \n",
    "                                                               overlay_train_BDT[bdt_vars]]), \n",
    "                                               label=labels_dict[HNL_mass], \n",
    "                                                    missing=new_value, feature_names=bdt_vars)\n",
    "    xgb_test_dict[HNL_mass] = xgboost.DMatrix(signal_test_BDT_dict[HNL_mass][bdt_vars], label=[1]*len(signal_test_BDT_dict[HNL_mass][bdt_vars]),\n",
    "                                              missing=new_value, feature_names=bdt_vars)\n",
    "    \n",
    "    xgb_sig_train_dict[HNL_mass] = xgboost.DMatrix(signal_train_BDT_dict[HNL_mass][bdt_vars],label=[1]*len(signal_train_BDT_dict[HNL_mass][bdt_vars]),\n",
    "                                                  missing=new_value, feature_names=bdt_vars)\n",
    "    xgb_bkg_train_dict[HNL_mass] = xgboost.DMatrix(overlay_train_BDT[bdt_vars],label=[0]*len(overlay_train_BDT[bdt_vars]),\n",
    "                                                  missing=new_value, feature_names=bdt_vars)\n",
    "\n",
    "    #watchlist so that you can monitor the performance of the training by iterations\n",
    "    watchlist = [(xgb_train_dict[HNL_mass], 'train'), (xgb_test_dict[HNL_mass], 'test_sig'),(xgb_test_bkg,'test_bkg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e1d1f1-bd7a-4c4f-810f-1b66abf67eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_vals = input(\"Do you want to retrain the BDTs? y/n \")\n",
    "if print_vals == \"y\":\n",
    "    for HNL_mass in HNL_masses:\n",
    "        print(f\"Training {HNL_mass} MeV BDT\" + \"\\n\")\n",
    "        bdt = xgboost.train(param, xgb_train_dict[HNL_mass], num_round, watchlist,evals_result=progress, verbose_eval=False)\n",
    "        # doesnt like watchlist/eval_result if using AOC\n",
    "        # save model so you can load it later\n",
    "        bdt.save_model(f'bdts/{Run}_{HNL_mass}_MeV_My_variables_flattened_highest_E_3.json')\n",
    "        #bdt.save_model(f'bdts/{Run}_{HNL_mass}_MeV_REDUCED_variables_flattened_highest_E_3.json')\n",
    "    \n",
    "else: \n",
    "    print(\"Not retraining BDTs, will load previous models in next cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0ef5d-8c83-498c-ba6d-4fe3c3377259",
   "metadata": {},
   "outputs": [],
   "source": [
    "bdt_vars = feature_names\n",
    "if Load_DetVars == True:\n",
    "    DetVars_matrices = {}\n",
    "    for DetVar in Constants.Detector_variations:\n",
    "        DetVars_matrices[DetVar] = xgboost.DMatrix(DetVar_BDT_dict[DetVar][bdt_vars])\n",
    "\n",
    "    for HNL_mass in HNL_masses:\n",
    "\n",
    "        bdt = xgboost.Booster()\n",
    "        bdt.load_model(f'bdts/{Run}_{HNL_mass}_MeV_My_variables_flattened_highest_E_3.json')\n",
    "        for DetVar in Constants.Detector_variations:\n",
    "            results = bdt.predict(DetVars_matrices[DetVar])\n",
    "            # print(DetVar)\n",
    "            # print(len(results))\n",
    "            DetVar_BDT_dict[DetVar][f'BDT_output_{HNL_mass}MeV'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262726f6-3c6c-42d4-9511-66c0bab7ceda",
   "metadata": {},
   "source": [
    "## Saving .pkl files of the selected dataframes with BDT score branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314b67d1-4394-4b14-9f28-0af44a1d22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_results_loc = f'pkl_files/{Run}/current_files/Results/'\n",
    "\n",
    "print_vals = input(\"Do you want to save the new BDT scores to .pkl files? y/n \")\n",
    "if print_vals == \"y\":\n",
    "    \n",
    "    labels = Variables.event_vars + Variables.weight_related + [\"weight\"]\n",
    "    \n",
    "    for HNL_mass in HNL_masses:\n",
    "        labels.append(f'BDT_output_{HNL_mass}MeV')\n",
    "    print(labels)\n",
    "    \n",
    "    overlay_merged = pd.concat([overlay_test_BDT,overlay_to_save], axis=1)\n",
    "\n",
    "    print(overlay_merged.head())\n",
    "    \n",
    "    overlay_merged_to_save = overlay_merged[labels] #Have duplicated \"weight\" column now, need to fix\n",
    "    df2 = overlay_merged_to_save.loc[:,~overlay_merged_to_save.columns.duplicated()] #Getting rid of duplicate \"weight\" column\n",
    "    df2.to_pickle(pkl_results_loc + \"overlay_test2.pkl\")\n",
    "    \n",
    "    print(df2.keys())\n",
    "    \n",
    "#     overlay_to_save = data_test_bkg[labels]\n",
    "#     overlay_to_save.head()\n",
    "    \n",
    "#     data_test_bkg\n",
    "#     signal_test_dict[HNL_mass]\n",
    "    \n",
    "#     Cleaned_Presel_dirt_highest_E\n",
    "#     Cleaned_Presel_EXT_highest_E\n",
    "#     new_overlay.to_pickle(loc_pkls+f\"overlay_{Run}_{variables_string}_\"+Flat_state+\".pkl\")\n",
    "\n",
    "else: \n",
    "    print(\"Not saving .pkl files with BDT scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5106d9-d72f-432e-a947-3c022798f773",
   "metadata": {},
   "source": [
    "## Plotting BDT Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafc2434-7e9f-4318-97ab-54f9c4a1f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting BDT output after only keeping highest bdt score per event\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    # plt.figure(figsize=(12,8),facecolor='white')\n",
    "    numdecays_signal = len(cleaned_signal_dict_highest_E[HNL_mass])\n",
    "    numdecays_bkg = len(Cleaned_Presel_overlay_highest_E)\n",
    "    SF_sig = numdecays_signal/len(signal_test_dict[HNL_mass][f'BDT_output'])\n",
    "    SF_overlay = (numdecays_bkg)/len(data_test_bkg[f'BDT_output_{HNL_mass}MeV'])\n",
    "    \n",
    "    # print(\"Scale factor for signal is \" + str(SF_sig)) #Should have a dict of SF_sig and SF_overlay\n",
    "    # print(\"Scale factor for overlay is \" + str(SF_overlay))\n",
    "    \n",
    "    if Run == \"run1\":\n",
    "        SF_overlay_run1_corrected = Constants.SF_overlay_run1*SF_overlay #Necessary to account for the events used for training\n",
    "        SF_signal_run1_corrected = SF_sig #Necessary to account for the events used for training\n",
    "        EXT_scale_list = np.ones(len(Cleaned_Presel_EXT_highest_E[f'BDT_output_{HNL_mass}MeV']))*Constants.SF_EXT_run1\n",
    "        EXT_scale = Constants.SF_EXT_run1\n",
    "        dirt_scale = Constants.SF_dirt_run1\n",
    "        signal_scale_list = np.ones(len(signal_test_dict[HNL_mass][f'BDT_output']))*SF_signal_run1_corrected\n",
    "        overlay_scale = SF_overlay_run1_corrected\n",
    "        \n",
    "    elif Run == \"run3\":\n",
    "        SF_overlay_run3_corrected = Constants.SF_overlay_run3*SF_overlay #Necessary to account for the events used for training\n",
    "        SF_signal_run3_corrected = SF_sig #Necessary to account for the events used for training\n",
    "        EXT_scale_list = np.ones(len(Cleaned_Presel_EXT_highest_E[f'BDT_output_{HNL_mass}MeV']))*Constants.SF_EXT_run3\n",
    "        EXT_scale = Constants.SF_EXT_run3\n",
    "        dirt_scale = Constants.SF_dirt_run3\n",
    "        signal_scale_list = np.ones(len(signal_test_dict[HNL_mass][f'BDT_output']))*SF_signal_run3_corrected\n",
    "        overlay_scale = SF_overlay_run3_corrected\n",
    "\n",
    "print(\"Creating the sample and normalisation dictionaries\")    \n",
    "samples={'overlay_test':data_test_bkg,\n",
    "         'dirtoverlay':Cleaned_Presel_dirt_highest_E,\n",
    "         'beamoff':Cleaned_Presel_EXT_highest_E}\n",
    "\n",
    "sample_norms={'overlay_test':np.array(data_test_bkg[\"weight\"]*overlay_scale),\n",
    "         'dirtoverlay':np.array(Cleaned_Presel_dirt_highest_E[\"weight\"]*dirt_scale),\n",
    "         'beamoff':EXT_scale_list}\n",
    "\n",
    "print(\"Adding signal samples to sample dictionary\")\n",
    "for HNL_mass in HNL_masses:\n",
    "    signal_scale_list = np.ones(len(signal_test_dict[HNL_mass][f'BDT_output']))*SF_sig\n",
    "    \n",
    "    sample_placeholder = {HNL_mass:signal_test_dict[HNL_mass]}\n",
    "    norm_placeholder = {HNL_mass:signal_scale_list}\n",
    "    samples.update(sample_placeholder)\n",
    "    sample_norms.update(norm_placeholder)\n",
    "    print(len(sample_norms[HNL_mass]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab964df9-eed6-4151-86a1-251ae059b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "PT.Plot_BDT_output(HNL_masses=HNL_masses, samples=samples, sample_norms=sample_norms, colours={}, xlims=[0,1.0],\n",
    "                bins=20,figsize=[12,8], MergeBins=False, density=False, legloc=\"upper center\",logy=True, savefig=False, Run=Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fe26e-28b5-4cd3-9f18-200d99f0e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the histograms for detector variations\n",
    "HNL_mass = 20\n",
    "if Load_DetVars == True:\n",
    "    for DetVar in DetVar_BDT_dict:\n",
    "        linewidth = 1\n",
    "        if DetVar == \"CV\":\n",
    "            linewidth = 3\n",
    "        plt.hist(DetVar_BDT_dict[DetVar][f'BDT_output_{HNL_mass}MeV'], weights=DetVar_BDT_dict[DetVar][\"weight\"], bins=20,range=[0,1.0],label=f'{DetVar}',\n",
    "                 lw=linewidth,histtype=\"step\")\n",
    "    plt.xlabel(f'BDT_output_{HNL_mass}MeV')\n",
    "    plt.legend()\n",
    "    plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca8068-554c-43c3-82aa-6b93f3e06fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DetVar_percentages_dict = {}\n",
    "if Load_DetVars == True:\n",
    "    cv_hist, cv_bins = np.histogram(\n",
    "        DetVar_BDT_dict[\"CV\"][f'BDT_output_{HNL_mass}MeV'],\n",
    "        range=[0,1.0],\n",
    "        bins=20,\n",
    "        weights=DetVar_BDT_dict[\"CV\"][\"weight\"])\n",
    "    bins_cent=(cv_bins[:-1]+cv_bins[1:])/2\n",
    "    for DetVar in DetVar_BDT_dict:\n",
    "        if DetVar == \"CV\":\n",
    "            continue\n",
    "        perc_list = []\n",
    "        detvar_hist, bins = np.histogram(\n",
    "        DetVar_BDT_dict[DetVar][f'BDT_output_{HNL_mass}MeV'],\n",
    "        range=[0,1.0],\n",
    "        bins=20,\n",
    "        weights=DetVar_BDT_dict[DetVar][\"weight\"])\n",
    "        for i in range(len(detvar_hist)):\n",
    "            frac = detvar_hist[i]/cv_hist[i]\n",
    "            frac_diff = frac - 1.0\n",
    "            perc_list.append(frac_diff*100)\n",
    "        DetVar_percentages_dict[DetVar] = perc_list\n",
    "        plt.hist(bins_cent,weights=DetVar_percentages_dict[DetVar], bins=20,range=[0,1.0],label=f'{DetVar}',\n",
    "                lw=linewidth,histtype=\"step\") #just 1 entry for each bin, then \"weight\" becomes what the percentage is (hacky way, could do something nicer)\n",
    "    plt.legend(loc='upper left',frameon=True)\n",
    "    plt.ylim([-150,150])\n",
    "    plt.xlabel(f'BDT Score {HNL_mass} MeV', fontsize=30)\n",
    "    plt.ylabel('% Difference', fontsize=30)\n",
    "    #plt.yscale()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eeca89-2a1b-41f3-a653-16e6a329bfed",
   "metadata": {},
   "source": [
    "# Function for saving histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f5359-b9c0-4a3f-9d2e-c1e2f1fae515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveToRoot(nbins,xlims,bkg_overlay,bkg_dirt,bkg_EXT,sig,data,fileName='test.root'):\n",
    "    nBins = nbins\n",
    "    binLimits = xlims\n",
    "  ### Save files \n",
    "    rFile = ROOT.TFile(f'bdt_output/{fileName}','RECREATE')\n",
    "    tData1 = ROOT.TH1F(\"Signal\",\"Signal\",nBins,binLimits[0],binLimits[1])\n",
    "    for i in range(nBins):\n",
    "        tData1.SetBinContent(i+1,sig['hist'][i])\n",
    "        tData1.SetBinError(i+1,sig['err'][i])\n",
    "    tData2 = ROOT.TH1F(\"bkg_overlay\",\"bkg_overlay\",nBins,binLimits[0],binLimits[1])\n",
    "    for i in range(nBins):\n",
    "        tData2.SetBinContent(i+1,bkg_overlay['hist'][i])\n",
    "        tData2.SetBinError(i+1,bkg_overlay['err'][i])\n",
    "    tData3 = ROOT.TH1F(\"bkg_dirt\",\"bkg_dirt\",nBins,binLimits[0],binLimits[1])\n",
    "    for i in range(nBins):\n",
    "        tData3.SetBinContent(i+1,bkg_dirt['hist'][i])\n",
    "        tData3.SetBinError(i+1,bkg_dirt['err'][i])\n",
    "    tData4 = ROOT.TH1F(\"bkg_EXT\",\"bkg_EXT\",nBins,binLimits[0],binLimits[1])\n",
    "    for i in range(nBins):\n",
    "        tData4.SetBinContent(i+1,bkg_EXT['hist'][i])\n",
    "        tData4.SetBinError(i+1,bkg_EXT['err'][i])\n",
    "    tData5 = ROOT.TH1F(\"Data\",\"Data\",nBins,binLimits[0],binLimits[1])\n",
    "    for i in range(nBins):\n",
    "        tData5.SetBinContent(i+1,data['hist'][i])\n",
    "        tData5.SetBinError(i+1,data['err'][i])\n",
    "    rFile.Write()\n",
    "    rFile.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c4f5b-983f-4f12-8cf9-6bf160fbef8c",
   "metadata": {},
   "source": [
    "# Saving output to .root files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd25a7-1e67-4072-8a28-edb4f850b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_theta = Constants.theta_mu_4\n",
    "\n",
    "SCALE_UP_dict = {20:200, #Doing this because the scale factor used in pyhf is bounded, should be able to change this within pyhf\n",
    "                50:50,\n",
    "                100:10,\n",
    "                150:5,\n",
    "                180:5,\n",
    "                200:2}\n",
    "\n",
    "SCALE_UP_dict_run3 = {20:200, #Doing this because the scale factor used in pyhf is bounded, should be able to change this within pyhf\n",
    "                50:50,\n",
    "                100:10,\n",
    "                150:5,\n",
    "                180:5,\n",
    "                200:2}\n",
    "\n",
    "new_theta_dict = {}\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "\n",
    "    numdecays_signal = len(cleaned_signal_dict_highest_E[HNL_mass])\n",
    "    numdecays_bkg = len(Cleaned_Presel_overlay_highest_E)\n",
    "    \n",
    "    SF_sig = numdecays_signal/len(signal_test_dict[HNL_mass][f'BDT_output'])\n",
    "    SF_overlay = (numdecays_bkg)/len(data_test_bkg[f'BDT_output_{HNL_mass}MeV'])\n",
    "    print(\"Scale factor for signal is \" + str(SF_sig)) #Should have a dict of SF_sig and SF_overlay\n",
    "    print(\"Scale factor for overlay is \" + str(SF_overlay))\n",
    "    \n",
    "    if Run == \"run1\":\n",
    "        new_theta_dict[HNL_mass] = original_theta*SCALE_UP_dict[HNL_mass]\n",
    "    elif Run == \"run3\":\n",
    "        new_theta_dict[HNL_mass] = original_theta*SCALE_UP_dict_run3[HNL_mass]\n",
    "    SCALE_EVENTS = SCALE_UP_dict[HNL_mass]**4 #The Number of events is proportional to theta^4\n",
    "    print(str(SCALE_EVENTS)+\"\\n\")\n",
    "\n",
    "    if Run == \"run1\":\n",
    "        SF_overlay_run1_corrected = Constants.SF_overlay_run1*SF_overlay #Necessary to account for the events used for training\n",
    "        SF_signal_run1_corrected = Constants.run1_POT_scaling_dict[HNL_mass]*SF_sig*SCALE_EVENTS #Necessary to account for the events used for training\n",
    "        EXT_scale_list = np.ones(len(Cleaned_Presel_EXT_highest_E[f'BDT_output_{HNL_mass}MeV']))*Constants.SF_EXT_run1\n",
    "        EXT_scale = Constants.SF_EXT_run3\n",
    "        dirt_scale = Constants.SF_dirt_run1\n",
    "        \n",
    "    elif Run == \"run3\":\n",
    "        SF_overlay_run1_corrected = Constants.SF_overlay_run3*SF_overlay #Necessary to account for the events used for training\n",
    "        SF_signal_run1_corrected = Constants.run3_POT_scaling_dict[HNL_mass]*SF_sig*SCALE_EVENTS #Necessary to account for the events used for training\n",
    "        EXT_scale_list = np.ones(len(Cleaned_Presel_EXT_highest_E[f'BDT_output_{HNL_mass}MeV']))*Constants.SF_EXT_run3\n",
    "        EXT_scale = Constants.SF_EXT_run3\n",
    "        dirt_scale = Constants.SF_dirt_run3\n",
    "    \n",
    "    signal_scale_list = np.ones(len(signal_test_dict[HNL_mass][f'BDT_output']))*SF_signal_run1_corrected\n",
    "\n",
    "    #Create and scale the histograms\n",
    "\n",
    "    signal_bdt_hist = np.histogram(signal_test_dict[HNL_mass][f'BDT_output'], bins=20, weights=signal_scale_list, range = (0.0,1.0))\n",
    "    overlay_bdt_hist = np.histogram(data_test_bkg[f'BDT_output_{HNL_mass}MeV'], bins=20, weights=data_test_bkg[\"weight\"]*SF_overlay_run1_corrected, range = (0.0,1.0))\n",
    "    dirt_bdt_hist = np.histogram(Cleaned_Presel_dirt_highest_E[f'BDT_output_{HNL_mass}MeV'], bins=20, weights=Cleaned_Presel_dirt_highest_E[\"weight\"]*dirt_scale, range = (0.0,1.0))\n",
    "    EXT_bdt_hist = np.histogram(Cleaned_Presel_EXT_highest_E[f'BDT_output_{HNL_mass}MeV'], bins=20, weights=EXT_scale_list, range = (0.0,1.0))\n",
    "    \n",
    "    #Error on any bin is sqrt(N)*scaling\n",
    "    signal_err = []\n",
    "    bkg_err = []\n",
    "    dirt_err = []\n",
    "    EXT_err = []\n",
    "    bdt_sig_arr = [] #Do I need this for anything? remove if not\n",
    "    bdt_bkg_arr = []\n",
    "\n",
    "\n",
    "    #Need to make sure the histograms have weighted but not scaled events\n",
    "    for i in range(0,len(signal_bdt_hist[0])):\n",
    "        error = np.sqrt(signal_bdt_hist[0][i])*np.sqrt(SF_signal_run1_corrected)\n",
    "        signal_err.append(error)\n",
    "    \n",
    "    for i in range(0,len(overlay_bdt_hist[0])):\n",
    "        error = np.sqrt(overlay_bdt_hist[0][i])*np.sqrt(SF_overlay_run1_corrected)\n",
    "        bkg_err.append(error)\n",
    "    \n",
    "    for i in range(0,len(dirt_bdt_hist[0])):\n",
    "        error = np.sqrt(dirt_bdt_hist[0][i])*np.sqrt(dirt_scale)\n",
    "        dirt_err.append(error)\n",
    "    \n",
    "    for i in range(0,len(EXT_bdt_hist[0])):\n",
    "        error = np.sqrt(EXT_bdt_hist[0][i])*np.sqrt(EXT_scale)\n",
    "        EXT_err.append(error)\n",
    "    \n",
    "    bb = overlay_bdt_hist[1]\n",
    "    hb = overlay_bdt_hist[0]\n",
    "    eb = bkg_err\n",
    "    bs = signal_bdt_hist[1]\n",
    "    hs = signal_bdt_hist[0]\n",
    "    es = signal_err\n",
    "    bd = dirt_bdt_hist[1]\n",
    "    hd = dirt_bdt_hist[0]\n",
    "    ed = dirt_err\n",
    "    be = EXT_bdt_hist[1]\n",
    "    he = EXT_bdt_hist[0]\n",
    "    ee = EXT_err\n",
    "    \n",
    "    data_b = overlay_bdt_hist[1]\n",
    "    data_h = overlay_bdt_hist[0]+dirt_bdt_hist[0]+EXT_bdt_hist[0]\n",
    "    data_e = bkg_err\n",
    "    \n",
    "    #Create output products\n",
    "    bkg_overlay = {'bins': np.array(bb), 'hist': np.array(hb), 'err': np.array(eb)}\n",
    "    bkg_dirt = {'bins': np.array(bd), 'hist': np.array(hd), 'err': np.array(ed)}\n",
    "    bkg_EXT = {'bins': np.array(be), 'hist': np.array(he), 'err': np.array(ee)}\n",
    "    sig = {'bins': np.array(bs), 'hist': np.array(hs), 'err': np.array(es)}\n",
    "    data = {'bins': np.array(data_b), 'hist': np.array(data_h), 'err': np.array(data_e)} #Should be added bkgs, with errors in quadrature\n",
    "\n",
    "\n",
    "    SaveToRoot(20,[0,1.0],bkg_overlay,bkg_dirt,bkg_EXT,sig,data,fileName=f'{Run}_{HNL_mass}MeV_test2.root')\n",
    "    \n",
    "print(\"New (scaled) thetas are:\")\n",
    "print(new_theta_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915f34d7-408f-40ce-9ca6-2d74dce1e6f1",
   "metadata": {},
   "source": [
    "## Checking importance correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a4680-42f1-4f9b-954d-1197dd26917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from Luis' code\n",
    "# for HNL_mass in HNL_masses:\n",
    "method = 'kendall'\n",
    "correlations = cleaned_signal_dict[100][bdt_vars].astype(np.float64).corr(method=method)\n",
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(correlations,vmin=-1,annot=False,square=True,cbar_kws={'label':method+' correlation'},cmap = 'RdBu_r')\n",
    "plt.title('Input Variable Correlations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a32c08-e4b9-4d6c-8037-4aa3bc074521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just looking at most correlated \n",
    "corr=cleaned_signal_dict[100][bdt_vars].corr()\n",
    "high_corr_var=np.where(corr>0.95)\n",
    "high_corr_var=[(corr.columns[x],corr.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\n",
    "high_corr_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7d7ce-cb79-4e2f-a35d-88717247cdf0",
   "metadata": {},
   "source": [
    "## Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057e26b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HNL_mass = 200\n",
    "\n",
    "first_model = xgboost.Booster()\n",
    "first_model.load_model(f'bdts/{Run}_{HNL_mass}_MeV_My_variables_flattened_highest_E_1.json')\n",
    "\n",
    "plt.figure(figsize=(12,12),facecolor='white')\n",
    "\n",
    "a = xgboost.plot_importance(first_model,max_num_features=15,importance_type='gain')\n",
    "\n",
    "print(type(a))  \n",
    "\n",
    "#Owen's dropdupes from the plotting function\n",
    "# overlay_post_bdt=data_test_bkg.sort_values('BDT_output',ascending=False).drop_duplicates(subset=[\"run\",\"evt\",\"sub\"])\n",
    "# dirt_post_bdt=data_test_bkg.sort_values('BDT_output',ascending=False).drop_duplicates(subset=[\"run\",\"evt\",\"sub\"])\n",
    "# EXT_post_bdt=data_test_bkg.sort_values('BDT_output',ascending=False).drop_duplicates(subset=[\"run\",\"evt\",\"sub\"])\n",
    "# signal_post_bdt=data_test_bkg.sort_values('BDT_output',ascending=False).drop_duplicates(subset=[\"run\",\"evt\",\"sub\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039f659-41d1-45f9-8233-6f395c25012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_dict = first_model.get_score(importance_type='gain')\n",
    "\n",
    "value = 15\n",
    "highest_imp = []\n",
    "\n",
    "for k in imp_dict:\n",
    "    if imp_dict[k] > value:\n",
    "        highest_imp.append(k)\n",
    "    \n",
    "print(highest_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77f5798-7c92-4033-b797-5a1e667db56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting unweighted\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    \n",
    "    numdecays_signal = len(signal_samples_dict[HNL_mass])\n",
    "    #numdecays_bkg = len(Presel_overlay)\n",
    "    numdecays_bkg = len(Cleaned_Presel_overlay_highest_E)\n",
    "    plt.figure(figsize=(12,8),facecolor='white')\n",
    "    SF_sig = numdecays_signal/len(signal_test_dict[HNL_mass][f'BDT_output'])\n",
    "    SF_overlay = (numdecays_bkg)/len(data_test_bkg[f'BDT_output_{HNL_mass}MeV'])\n",
    "    print(len(signal_test_dict[HNL_mass][f'BDT_output'])*SF_sig)\n",
    "    print(len(data_test_bkg[f'BDT_output_{HNL_mass}MeV'])*SF_overlay)\n",
    "\n",
    "    plt.hist(data_test_bkg[f'BDT_output_{HNL_mass}MeV'],bins=20,range = [0.0,1.0], color='blue', label='Neutrino bkg', alpha=0.8)\n",
    "    plt.hist(signal_test_dict[HNL_mass][f'BDT_output'],bins=20,range = [0.0,1.0], color='red', label=f'HNL {HNL_mass} MeV', alpha=0.8)\n",
    "    plt.hist(Cleaned_Presel_dirt_highest_E[f'BDT_output_{HNL_mass}MeV'],bins=20,range = [0.0,1.0], color='orange', label='Dirt bkg', alpha=0.8)\n",
    "    plt.hist(Cleaned_Presel_EXT_highest_E[f'BDT_output_{HNL_mass}MeV'],bins=20,range = [0.0,1.0], color='green', label=f'EXT bkg', alpha=0.8)\n",
    "\n",
    "    plt.xlabel('BDT score', fontsize=30)\n",
    "    plt.ylabel('Entries', fontsize=30)\n",
    "    plt.rcParams.update({'font.size': 30})\n",
    "    plt.legend()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103977c-5672-4978-85f0-1ef42549cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to loop over  HNL_mass points and save new dataframes only keeping highest BDT rows for each \n",
    "\n",
    "print(\"Number of entries in the overlay: \" + str(len(data_test_bkg)))\n",
    "print(\"Number of entries in the dirt: \" + str(len(Cleaned_Presel_dirt)))\n",
    "print(\"Number of entries in the EXT: \" + str(len(Cleaned_Presel_EXT)))\n",
    "\n",
    "overlay_post_bdt_dict = {}\n",
    "dirt_post_bdt_dict = {}\n",
    "EXT_post_bdt_dict = {}\n",
    "signal_post_bdt_dict = {}\n",
    "\n",
    "for HNL_mass in HNL_masses:\n",
    "    print(f\"Number of entries in the {HNL_mass}MeV signal: \" + str(len(signal_test_dict[HNL_mass])))\n",
    "\n",
    "    data_test_bkg[f\"highest_bdt_score_{HNL_mass}\"]=data_test_bkg[f'BDT_output_{HNL_mass}MeV'].groupby(\"entry\").transform(max) == data_test_bkg[f'BDT_output_{HNL_mass}MeV']\n",
    "    Cleaned_Presel_dirt[f\"highest_bdt_score_{HNL_mass}\"]=Cleaned_Presel_dirt[f'BDT_output_{HNL_mass}MeV'].groupby(\"entry\").transform(max) == Cleaned_Presel_dirt[f'BDT_output_{HNL_mass}MeV']\n",
    "    Cleaned_Presel_EXT[f\"highest_bdt_score_{HNL_mass}\"]=Cleaned_Presel_EXT[f'BDT_output_{HNL_mass}MeV'].groupby(\"entry\").transform(max) == Cleaned_Presel_EXT[f'BDT_output_{HNL_mass}MeV']\n",
    "    signal_test_dict[HNL_mass][f\"highest_bdt_score\"]=signal_test_dict[HNL_mass]['BDT_output'].groupby(\"entry\").transform(max) == signal_test_dict[HNL_mass]['BDT_output']\n",
    "\n",
    "    overlay_post_bdt_dict[HNL_mass] = data_test_bkg.query(f\"highest_bdt_score_{HNL_mass}\")\n",
    "    dirt_post_bdt_dict[HNL_mass] = Cleaned_Presel_dirt.query(f\"highest_bdt_score_{HNL_mass}\")\n",
    "    EXT_post_bdt_dict[HNL_mass] = Cleaned_Presel_EXT.query(f\"highest_bdt_score_{HNL_mass}\")\n",
    "    signal_post_bdt_dict[HNL_mass] = signal_test_dict[HNL_mass].query(\"highest_bdt_score\")\n",
    "\n",
    "print()\n",
    "print(\"-----Post dropping entries-----\")\n",
    "print(\"Number of entries in the overlay: \" + str(len(overlay_post_bdt_dict[200])))\n",
    "print(\"Number of entries in the dirt: \" + str(len(dirt_post_bdt_dict[200])))\n",
    "print(\"Number of entries in the EXT: \" + str(len(EXT_post_bdt_dict[200])))\n",
    "print(\"Number of entries in the signal: \" + str(len(signal_post_bdt_dict[200])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb11a1-1cf9-47bd-a980-b3dddd6f151f",
   "metadata": {},
   "source": [
    "##  Testing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d63aa5-a581-41ae-b753-d17033b5b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAT_Presel_overlay = Presel_overlay.columns.to_flat_index()\n",
    "#FLAT_Presel_overlay = Presel_overlay.reset_index()\n",
    "#FLAT_Presel_overlay = Presel_overlay(feature_names, flatten=False)\n",
    "\n",
    "for var in Presel_overlay.keys():\n",
    "    #if type(Presel_overlay.keys()[i]) == \"numpy.ndarray\":\n",
    "    if isinstance(Presel_overlay[var][0],np.ndarray):\n",
    "        if type(Presel_overlay[var][0]) == np.ndarray:\n",
    "            print(var)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756d5d9-46bb-49f7-a637-db6b99f5bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = Variables.First_pass_vars\n",
    "\n",
    "value = -1e15\n",
    "new_value = -9999\n",
    "\n",
    "for var in feature_names:\n",
    "    print(str(var)+' '+str(Presel_overlay[var][16]))\n",
    "    \n",
    "print()\n",
    "\n",
    "# for var in feature_names:\n",
    "#     print(str(var)+' '+str(FLAT_Presel_overlay[var][16]))\n",
    "\n",
    "if type(Presel_overlay['shr_dedx_v_v'][0])==np.ndarray:\n",
    "    print(\"yes it is\")\n",
    "    for i in (Presel_overlay['shr_dedx_v_v']):\n",
    "        Num = Presel_overlay['shr_dedx_v_v'][i].size\n",
    "        if Num == 1:\n",
    "            Presel_overlay['shr_dedx_v_v'][i][0] = new_value\n",
    "        else:\n",
    "            for j in range(Num):\n",
    "                if (Presel_overlay['shr_dedx_v_v'][i][j] < value):\n",
    "                    Presel_overlay['shr_dedx_v_v'][i][j] = new_value\n",
    "        \n",
    "else:\n",
    "    print(\"no it isnt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
